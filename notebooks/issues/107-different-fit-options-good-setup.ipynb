{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from importlib import reload\n",
    "import xarray as xr\n",
    "from typing import Union, Tuple, List, Dict\n",
    "\n",
    "from sdm_eurec4a.visulization import set_custom_rcParams, adjust_lightness_array, ncols_nrows_from_N\n",
    "import sdm_eurec4a.input_processing.models as smodels\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "from scipy.optimize import Bounds\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "from sdm_eurec4a import RepositoryPath\n",
    "from pathlib import Path\n",
    "\n",
    "from sdm_eurec4a.reductions import mean_and_stderror_of_mean\n",
    "\n",
    "from sdm_eurec4a.visulization import set_custom_rcParams, adjust_lightness_array, label_from_attrs\n",
    "from sdm_eurec4a.identifications import match_clouds_and_cloudcomposite\n",
    "from sdm_eurec4a.conversions import msd_from_psd_dataarray, psd_from_msd_dataarray\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "default_colors = set_custom_rcParams()\n",
    "dark_colors = adjust_lightness_array(default_colors, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sdm_eurec4a.input_processing.models' from '/home/m/m301096/repositories/sdm-eurec4a/src/sdm_eurec4a/input_processing/models.py'>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(smodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LnParameters:\n",
    "\n",
    "    def __init__(self, mu, sigma, scale):\n",
    "        self.parameters = dict(\n",
    "            mu=mu,\n",
    "            sigma=sigma,\n",
    "            scale=scale,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def custom_parameters(self):\n",
    "        \"\"\"Return the parameters in a dictionary with custom keys.\n",
    "        Keys are:\n",
    "        - mu1\n",
    "        - sigma1\n",
    "        - scale_factor1\n",
    "        \"\"\"\n",
    "        result = dict(\n",
    "            mu1=self.parameters[\"mu\"],\n",
    "            sigma1=self.parameters[\"sigma\"],\n",
    "            scale_factor1=self.parameters[\"scale\"],\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def scipy_parameters(self):\n",
    "        result = dict(\n",
    "            s=self.parameters[\"sigma\"],\n",
    "            loc=self.parameters[\"mu\"],\n",
    "            scale=self.parameters[\"scale\"],\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def geom_parameters(self):\n",
    "        result = dict(\n",
    "            geomean=self.parameters[\"mu\"],\n",
    "            geosig=self.parameters[\"sigma\"],\n",
    "            scalefac=self.parameters[\"scale\"],\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def normal_parameters(self):\n",
    "        result = dict(\n",
    "            mu=self.parameters[\"mu\"],\n",
    "            sigma=self.parameters[\"sigma\"],\n",
    "            scale=self.parameters[\"scale\"],\n",
    "        )\n",
    "        return result\n",
    "\n",
    "\n",
    "def mass_from_number(t: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    return 4 / 3 * np.pi * t**3 * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = 1e-2\n",
    "upper = 100\n",
    "N = 1000\n",
    "x = np.linspace(lower, upper, N)\n",
    "x1 = np.linspace(lower, upper, N * 2)\n",
    "x2 = np.logspace(np.log10(lower), np.log10(upper), N)\n",
    "x3 = np.logspace(np.log(lower), np.log(upper), N, base=np.exp(1))\n",
    "\n",
    "params = LnParameters(3, 2, 3)\n",
    "\n",
    "results = dict()\n",
    "for ps in [\"direct\", \"geometric\", \"exact\"]:\n",
    "    results[ps] = dict()\n",
    "    for space in [\"linear\", \"ln\", \"cleo\"]:\n",
    "        results[ps][space] = smodels.log_normal_distribution_all(\n",
    "            x=x,\n",
    "            parameter_space=ps,\n",
    "            space=space,\n",
    "            mu=params.parameters[\"mu\"],\n",
    "            sigma=params.parameters[\"sigma\"],\n",
    "            scale=params.parameters[\"scale\"],\n",
    "            density_scaled=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "ax1 = axs[0]\n",
    "ax2 = axs[1]\n",
    "\n",
    "i = 0\n",
    "for ps in [\"geometric\", \"exact\"]:\n",
    "    color = dark_colors[i]\n",
    "    for _ax in axs:\n",
    "        _ax.plot(x, results[ps][\"linear\"], label=f\"{ps} linear\", color=color, linestyle=\"-\")\n",
    "        # _ax.plot(x, results[ps]['ln'], label=f'{ps} ln', color = color, linestyle='--')\n",
    "        _ax.plot(x, results[ps][\"cleo\"], label=f\"{ps} cleo\", color=color, linestyle=\":\")\n",
    "    i += 1\n",
    "\n",
    "for _ax in axs:\n",
    "    # _ax.axvline(params.parameters['mu'], color = 'red', linestyle='--')\n",
    "    # _ax.axhline(params.parameters['scale'], color = 'red', linestyle='--')\n",
    "    # _ax.axvline(np.log(params.parameters['mu']), color = 'blue', linestyle='--')\n",
    "    _ax.legend()\n",
    "# ax2.axvline(10, color = 'k', linestyle='--')\n",
    "# plt.xlim(xlim)\n",
    "ax1.set_xscale(\"log\")\n",
    "\n",
    "# ax1.set_ylim(0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "ax1 = axs[0]\n",
    "ax2 = axs[1]\n",
    "\n",
    "i = 0\n",
    "for ps in [\"geometric\", \"exact\"]:\n",
    "    color = dark_colors[i]\n",
    "    for _ax in axs:\n",
    "        _ax.plot(x, results[ps][\"linear\"], label=f\"{ps} linear\", color=color, linestyle=\"-\")\n",
    "        # _ax.plot(x, results[ps]['ln'], label=f'{ps} ln', color = color, linestyle='--')\n",
    "        _ax.plot(x, results[ps][\"cleo\"], label=f\"{ps} cleo\", color=color, linestyle=\":\")\n",
    "    i += 1\n",
    "\n",
    "for _ax in axs:\n",
    "    _ax.axvline(params.parameters[\"mu\"], color=\"red\", linestyle=\"--\")\n",
    "    _ax.axhline(params.parameters[\"scale\"], color=\"red\", linestyle=\"--\")\n",
    "    # _ax.axvline(np.log(params.parameters['mu']), color = 'blue', linestyle='--')\n",
    "    _ax.legend()\n",
    "# ax2.axvline(10, color = 'k', linestyle='--')\n",
    "# plt.xlim(xlim)\n",
    "ax1.set_xscale(\"log\")\n",
    "\n",
    "# ax1.set_ylim(0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = RepositoryPath(\"levante\").get_data_dir()\n",
    "\n",
    "cloud_composite = xr.open_dataset(\n",
    "    data_dir / Path(\"observation/cloud_composite/processed/cloud_composite_SI_units_20241025.nc\"),\n",
    ")\n",
    "identified_clusters = xr.open_dataset(\n",
    "    data_dir\n",
    "    / Path(\n",
    "        \"observation/cloud_composite/processed/identified_clusters/identified_clusters_rain_mask_5.nc\"\n",
    "    )\n",
    ")\n",
    "identified_clusters = identified_clusters.swap_dims({\"time\": \"cloud_id\"})\n",
    "\n",
    "attrs = cloud_composite[\"radius\"].attrs.copy()\n",
    "# attrs.update({\"units\": \"µm\"})\n",
    "cloud_composite[\"radius\"] = cloud_composite[\"radius\"]\n",
    "# cloud_composite[\"radius_micro\"] = 1e6 * cloud_composite[\"radius\"]\n",
    "cloud_composite[\"radius\"].attrs = attrs\n",
    "\n",
    "cloud_composite[\"radius2D\"] = cloud_composite[\"radius\"].expand_dims(time=cloud_composite[\"time\"])\n",
    "cloud_composite = cloud_composite.transpose(\"radius\", ...)\n",
    "\n",
    "\n",
    "# cloud_composite = cloud_composite.sel(radius = slice(10, None))\n",
    "\n",
    "identified_clusters = identified_clusters.where(\n",
    "    (\n",
    "        (identified_clusters.duration.dt.seconds >= 3)\n",
    "        & (identified_clusters.altitude < 1200)\n",
    "        & (identified_clusters.altitude > 500)\n",
    "    ),\n",
    "    drop=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to coarsen the results, we need to make sure to apply the coarsening on the **NON** normalized data.\n",
    "Then we can normalized afterwards again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius_split = 95e-6  # 50 µm\n",
    "coarsen_factor = 3\n",
    "\n",
    "\n",
    "coarse_composite = cloud_composite.sel(radius=slice(radius_split, None)).copy()\n",
    "\n",
    "# make sure to have non normalized data to be coarsened\n",
    "# otherwise, the sum will not be conserved\n",
    "coarse_composite[\"particle_size_distribution\"] = (\n",
    "    coarse_composite[\"particle_size_distribution\"] * coarse_composite[\"bin_width\"]\n",
    ")\n",
    "coarse_composite[\"mass_size_distribution\"] = (\n",
    "    coarse_composite[\"mass_size_distribution\"] * coarse_composite[\"bin_width\"]\n",
    ")\n",
    "\n",
    "# use mean for radius and radius2D\n",
    "coarse_composite_radius = coarse_composite[\"radius\"].coarsen(radius=coarsen_factor).mean()\n",
    "coarse_composite_radius2D = coarse_composite[\"radius2D\"].coarsen(radius=coarsen_factor).mean()\n",
    "# use the sum for the rest\n",
    "coarse_composite = coarse_composite.coarsen(radius=coarsen_factor).sum()\n",
    "\n",
    "coarse_composite[\"radius\"] = coarse_composite_radius\n",
    "coarse_composite[\"radius2D\"] = coarse_composite_radius2D\n",
    "coarse_composite[\"diameter\"] = 2 * coarse_composite[\"radius\"]\n",
    "\n",
    "# make sure to have normalized data again\n",
    "coarse_composite[\"particle_size_distribution\"] = (\n",
    "    coarse_composite[\"particle_size_distribution\"] / coarse_composite[\"bin_width\"]\n",
    ")\n",
    "coarse_composite[\"mass_size_distribution\"] = (\n",
    "    coarse_composite[\"mass_size_distribution\"] / coarse_composite[\"bin_width\"]\n",
    ")\n",
    "\n",
    "coarse_composite[\"particle_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Number concentration\",\n",
    "    unit=cloud_composite[\"particle_size_distribution\"].attrs[\"unit\"],\n",
    ")\n",
    "coarse_composite[\"mass_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Mass concentration\",\n",
    "    unit=cloud_composite[\"mass_size_distribution\"].attrs[\"unit\"],\n",
    ")\n",
    "\n",
    "# merge the two composites with higher resoltion at small radii\n",
    "# and lower resolution at large radii\n",
    "coarse_composite = xr.merge(\n",
    "    [\n",
    "        coarse_composite.sel(radius=slice(radius_split, None)),\n",
    "        cloud_composite.sel(radius=slice(None, radius_split)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Test liquid water content is conserved\n",
    "np.testing.assert_allclose(\n",
    "    (coarse_composite[\"bin_width\"] * coarse_composite[\"mass_size_distribution\"]).sum(\"radius\"),\n",
    "    (cloud_composite[\"bin_width\"] * cloud_composite[\"mass_size_distribution\"]).sum(\"radius\"),\n",
    "    rtol=0.001,\n",
    ")\n",
    "# Test particle concentration is conserved\n",
    "np.testing.assert_allclose(\n",
    "    (coarse_composite[\"bin_width\"] * coarse_composite[\"particle_size_distribution\"]).sum(\"radius\"),\n",
    "    (cloud_composite[\"bin_width\"] * cloud_composite[\"particle_size_distribution\"]).sum(\"radius\"),\n",
    "    rtol=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSDBoundsWRONG:\n",
    "\n",
    "    mu1 = np.array([1e-6, 3e-6, 10e-6])\n",
    "    sig1 = np.array([1.1, 2, 4])\n",
    "    sc1 = np.log(np.array([0, 1e10, 1e16]))\n",
    "    mu2 = np.array([200e-6, 300e-6, 500e-6])\n",
    "    sig2 = np.array([1.1, 2, 3])\n",
    "    sc2 = np.log(np.array([0, 1e6, 1e10]))\n",
    "\n",
    "    _x0 = np.array([mu1[1], sig1[1], sc1[1], mu2[1], sig2[1], sc2[1]])\n",
    "\n",
    "    _x0_micrometer = np.array([mu1[1] * 1e6, sig1[1], sc1[1], mu2[1] * 1e6, sig2[1], sc2[1]])\n",
    "\n",
    "    _bounds = Bounds(\n",
    "        # mu1, sig1, sc1, mu2, sig2, sc2\n",
    "        lb=[mu1[0], sig1[0], sc1[0], mu2[0], sig2[0], sc2[0]],\n",
    "        ub=[mu1[2], sig1[2], sc1[2], mu2[2], sig2[2], sc2[2]],\n",
    "        # keep_feasible = [True, True, True, False, True, True]\n",
    "    )\n",
    "\n",
    "    _bounds_micrometer = Bounds(\n",
    "        # mu1, sig1, sc1, mu2, sig2, sc2\n",
    "        lb=[mu1[0] * 1e6, sig1[0], sc1[0], mu2[0] * 1e6, sig2[0], sc2[0]],\n",
    "        ub=[mu1[2] * 1e6, sig1[2], sc1[2], mu2[2] * 1e6, sig2[2], sc2[2]],\n",
    "        # keep_feasible = [True, True, True, False, True, True]\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds():\n",
    "        return PSDBounds._bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds_micrometer():\n",
    "        return PSDBounds._bounds_micrometer\n",
    "\n",
    "    @staticmethod\n",
    "    def x0():\n",
    "        return PSDBounds._x0\n",
    "\n",
    "    @staticmethod\n",
    "    def x0_micrometer():\n",
    "        return PSDBounds._x0_micrometer\n",
    "\n",
    "\n",
    "class PSDBounds:\n",
    "    _x0 = np.array([3e-6, 1.5, 1e13, 300e-6, 1.5, 1e3])\n",
    "    _bounds = Bounds(\n",
    "        # mu1, sig1, sc1, mu2, sig2, sc2\n",
    "        lb=[1e-6, 1.3, 1e-20, 100e-6, 1.3, 1e-20],\n",
    "        ub=[50e6, 3.0, 1e16, 0.5e-3, 3.0, 1e10],\n",
    "        # keep_feasible = [True, True, True, False, True, True]\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds():\n",
    "        return PSDBounds._bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def x0():\n",
    "        return PSDBounds._x0\n",
    "\n",
    "\n",
    "class MSDBoundsWRONG:\n",
    "\n",
    "    mu1 = np.array([1e-6, 3e-6, 10e-6])\n",
    "    sig1 = np.array([1.1, 2.0, 4.0])\n",
    "    sc1 = np.log(np.array([0, 1e-1, 1e2]))\n",
    "    mu2 = np.array([200e-6, 300e-6, 500e-6])\n",
    "    sig2 = np.array([1.3, 2, 3.0])\n",
    "    sc2 = np.log(np.array([0, 1e-3, 1e1]))\n",
    "\n",
    "    _x0 = np.array([mu1[1], sig1[1], sc1[1], mu2[1], sig2[1], sc2[1]])\n",
    "\n",
    "    _x0_micrometer = np.array([mu1[1] * 1e6, sig1[1], sc1[1], mu2[1] * 1e6, sig2[1], sc2[1]])\n",
    "\n",
    "    _bounds = Bounds(\n",
    "        # mu1, sig1, sc1, mu2, sig2, sc2\n",
    "        lb=[mu1[0], sig1[0], sc1[0], mu2[0], sig2[0], sc2[0]],\n",
    "        ub=[mu1[2], sig1[2], sc1[2], mu2[2], sig2[2], sc2[2]],\n",
    "        # keep_feasible = [True, True, True, False, True, True]\n",
    "    )\n",
    "\n",
    "    _bounds_micrometer = Bounds(\n",
    "        # mu1, sig1, sc1, mu2, sig2, sc2\n",
    "        lb=[mu1[0] * 1e6, sig1[0], sc1[0], mu2[0] * 1e6, sig2[0], sc2[0]],\n",
    "        ub=[mu1[2] * 1e6, sig1[2], sc1[2], mu2[2] * 1e6, sig2[2], sc2[2]],\n",
    "        # keep_feasible = [True, True, True, False, True, True]\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds():\n",
    "        return MSDBounds._bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds_micrometer():\n",
    "        return MSDBounds._bounds_micrometer\n",
    "\n",
    "    @staticmethod\n",
    "    def x0():\n",
    "        return MSDBounds._x0\n",
    "\n",
    "    @staticmethod\n",
    "    def x0_micrometer():\n",
    "        return MSDBounds._x0_micrometer\n",
    "\n",
    "\n",
    "class MSDBounds:\n",
    "    _x0 = np.array([3e-6, 1.5, 1e-1, 300e-6, 2, 1e-4])\n",
    "    _bounds = Bounds(\n",
    "        lb=[1e-6, 1.3, 1e-20, 100e-6, 1.3, 1e-20],\n",
    "        ub=[50e-6, 3.5, 1e2, 0.5e-3, 3.0, 1e1],\n",
    "        # keep_feasible = [True, True, True, False, True, True]\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds():\n",
    "        return MSDBounds._bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def x0():\n",
    "        return MSDBounds._x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_ln_normal_distribution(\n",
    "    t: np.ndarray,\n",
    "    mu1: float,\n",
    "    sigma1: float,\n",
    "    scale_factor1: float,\n",
    "    mu2: float,\n",
    "    sigma2: float,\n",
    "    scale_factor2: float,\n",
    ") -> np.ndarray:\n",
    "\n",
    "    result = t * 0\n",
    "\n",
    "    for mu, sigma, scale_factor in zip(\n",
    "        (mu1, mu2),\n",
    "        (sigma1, sigma2),\n",
    "        (scale_factor1, scale_factor2),\n",
    "    ):\n",
    "        sigtilda = np.log(sigma)\n",
    "        mutilda = np.log(mu)\n",
    "\n",
    "        norm = scale_factor / (np.sqrt(2 * np.pi) * sigtilda)\n",
    "        exponent = -((np.log(t) - mutilda) ** 2) / (2 * sigtilda**2)\n",
    "\n",
    "        dn_dlnr = norm * np.exp(exponent)  # eq.5.8 [lohmann intro 2 clouds]\n",
    "\n",
    "        result += dn_dlnr\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def double_ln_normal_distribution_cost(\n",
    "    x: Tuple[float, float, float, float, float, float],\n",
    "    t: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    variance: Union[None, float, int, np.ndarray] = None,\n",
    "    variance_scale: float = 0.01,\n",
    "    variance_minimal: float = 1e-12,\n",
    ") -> np.ndarray:\n",
    "\n",
    "    y_pred = double_ln_normal_distribution(t, *x)\n",
    "\n",
    "    var = 1\n",
    "    return np.ravel((y_pred - y) / np.sqrt(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "reload(smodels)\n",
    "\n",
    "\n",
    "class LinearDoubleLnNormal(smodels.LeastSquareFit):\n",
    "    \"\"\"\n",
    "    A class to perform least squares fitting for a double log-normal distribution.\n",
    "\n",
    "    Attributes:\n",
    "        name (str): The name of the fitting instance.\n",
    "        func (Callable): The model function to fit.\n",
    "        cost_func (Callable): The cost function to minimize.\n",
    "        x0 (np.ndarray): Initial guess for the parameters.\n",
    "        bounds (Bounds): Bounds on the parameters.\n",
    "        t_train (Union[np.ndarray, xr.DataArray]): Training data for the independent variable.\n",
    "        y_train (Union[np.ndarray, xr.DataArray]): Training data for the dependent variable.\n",
    "        fit_kwargs (Dict): Additional keyword arguments for the least_squares function.\n",
    "        plot_kwargs (Dict): Additional keyword arguments for plotting.\n",
    "        fit_result: The result of the fitting process.\n",
    "\n",
    "    Methods:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        x0: np.ndarray,\n",
    "        bounds: Bounds,\n",
    "        t_train: Union[xr.DataArray, np.ndarray],\n",
    "        y_train: Union[xr.DataArray, np.ndarray],\n",
    "        fit_kwargs: Dict = dict(),\n",
    "        plot_kwargs: Dict = dict(),\n",
    "        weighted_cost_use: bool = False,\n",
    "        func_kwargs: Dict = dict(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DoubleLnNormalLeastSquare instance.\n",
    "\n",
    "        Parameters:\n",
    "            name (str): The name of the fitting instance.\n",
    "            x0 (np.ndarray): Initial guess for the parameters.\n",
    "            bounds (Bounds): Bounds on the parameters.\n",
    "            t_train (np.ndarray): Training data for the independent variable.\n",
    "            y_train (np.ndarray): Training data for the dependent variable.\n",
    "        \"\"\"\n",
    "\n",
    "        def this_func(\n",
    "            t: ndarray,\n",
    "            mu1: float,\n",
    "            sigma1: float,\n",
    "            scale_factor1: float,\n",
    "            mu2: float,\n",
    "            sigma2: float,\n",
    "            scale_factor2: float,\n",
    "        ) -> np.ndarray:\n",
    "\n",
    "            d1 = smodels.log_normal_distribution_all(\n",
    "                x=t,\n",
    "                mu=mu1,\n",
    "                sigma=sigma1,\n",
    "                scale=scale_factor1,\n",
    "                parameter_space=\"exact\",\n",
    "                space=\"linear\",\n",
    "                **func_kwargs,\n",
    "            )\n",
    "            d2 = smodels.log_normal_distribution_all(\n",
    "                x=t,\n",
    "                mu=mu2,\n",
    "                sigma=sigma2,\n",
    "                scale=scale_factor2,\n",
    "                parameter_space=\"exact\",\n",
    "                space=\"linear\",\n",
    "                **func_kwargs,\n",
    "            )\n",
    "\n",
    "            return d1 + d2\n",
    "\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            func=this_func,\n",
    "            x0=x0,\n",
    "            bounds=bounds,\n",
    "            t_train=t_train,\n",
    "            y_train=y_train,\n",
    "            fit_kwargs=fit_kwargs,\n",
    "            plot_kwargs=plot_kwargs,\n",
    "        )\n",
    "\n",
    "        if weighted_cost_use == True:\n",
    "            self.cost_func = self.__weighted_cost_func__\n",
    "        else:\n",
    "            self.cost_func = self.__default_cost_func__\n",
    "\n",
    "    def __default_cost_func__(self, x: np.ndarray, t: np.ndarray, y: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The cost function to minimize.\n",
    "\n",
    "        Parameters:\n",
    "            x (np.ndarray): The parameters to estimate.\n",
    "            t (np.ndarray): The independent variable.\n",
    "            y (np.ndarray): The dependent variable.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The difference between the predicted and the actual data.\n",
    "        \"\"\"\n",
    "        diff = y - self.func(t, *x)\n",
    "\n",
    "        diff = np.ravel(diff)\n",
    "\n",
    "        # only use the non-NaN values\n",
    "        idx = np.where(~np.isnan(diff))\n",
    "        diff = diff[idx]\n",
    "        return diff\n",
    "\n",
    "    def __weighted_cost_func__(\n",
    "        self, x: np.ndarray, t: np.ndarray, y: np.ndarray, **kwargs\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The cost function to minimize.\n",
    "\n",
    "        Parameters:\n",
    "            x (np.ndarray): The parameters to estimate.\n",
    "            t (np.ndarray): The independent variable.\n",
    "            y (np.ndarray): The dependent variable.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The difference between the predicted and the actual data.\n",
    "        \"\"\"\n",
    "        diff = t**3 * (y - self.func(t, *x))\n",
    "        # diff = y - self.func(t, *x)\n",
    "\n",
    "        diff = np.ravel(diff)\n",
    "\n",
    "        # only use the non-NaN values\n",
    "        idx = np.where(~np.isnan(diff))\n",
    "        diff = diff[idx]\n",
    "        return diff\n",
    "\n",
    "\n",
    "class CleoDoubleLnNormal(smodels.LeastSquareFit):\n",
    "    \"\"\"\n",
    "    A class to perform least squares fitting for a double log-normal distribution.\n",
    "\n",
    "    Attributes:\n",
    "        name (str): The name of the fitting instance.\n",
    "        func (Callable): The model function to fit.\n",
    "        cost_func (Callable): The cost function to minimize.\n",
    "        x0 (np.ndarray): Initial guess for the parameters.\n",
    "        bounds (Bounds): Bounds on the parameters.\n",
    "        t_train (Union[np.ndarray, xr.DataArray]): Training data for the independent variable.\n",
    "        y_train (Union[np.ndarray, xr.DataArray]): Training data for the dependent variable.\n",
    "        fit_kwargs (Dict): Additional keyword arguments for the least_squares function.\n",
    "        plot_kwargs (Dict): Additional keyword arguments for plotting.\n",
    "        fit_result: The result of the fitting process.\n",
    "\n",
    "    Methods:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        x0: np.ndarray,\n",
    "        bounds: Bounds,\n",
    "        t_train: Union[xr.DataArray, np.ndarray],\n",
    "        y_train: Union[xr.DataArray, np.ndarray],\n",
    "        fit_kwargs: Dict = dict(),\n",
    "        plot_kwargs: Dict = dict(),\n",
    "        weighted_cost_use: bool = False,\n",
    "        func_kwargs: Dict = dict(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DoubleLnNormalLeastSquare instance.\n",
    "\n",
    "        Parameters:\n",
    "            name (str): The name of the fitting instance.\n",
    "            x0 (np.ndarray): Initial guess for the parameters.\n",
    "            bounds (Bounds): Bounds on the parameters.\n",
    "            t_train (np.ndarray): Training data for the independent variable.\n",
    "            y_train (np.ndarray): Training data for the dependent variable.\n",
    "        \"\"\"\n",
    "\n",
    "        def this_func(\n",
    "            t: ndarray,\n",
    "            mu1: float,\n",
    "            sigma1: float,\n",
    "            scale_factor1: float,\n",
    "            mu2: float,\n",
    "            sigma2: float,\n",
    "            scale_factor2: float,\n",
    "        ) -> np.ndarray:\n",
    "\n",
    "            d1 = smodels.log_normal_distribution_all(\n",
    "                x=t,\n",
    "                mu=mu1,\n",
    "                sigma=sigma1,\n",
    "                scale=scale_factor1,\n",
    "                parameter_space=\"geometric\",\n",
    "                space=\"cleo\",\n",
    "                **func_kwargs,\n",
    "            )\n",
    "            d2 = smodels.log_normal_distribution_all(\n",
    "                x=t,\n",
    "                mu=mu2,\n",
    "                sigma=sigma2,\n",
    "                scale=scale_factor2,\n",
    "                parameter_space=\"geometric\",\n",
    "                space=\"cleo\",\n",
    "                **func_kwargs,\n",
    "            )\n",
    "\n",
    "            return d1 + d2\n",
    "\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            func=double_ln_normal_distribution,\n",
    "            # cost_func=double_ln_normal_distribution_cost,\n",
    "            x0=x0,\n",
    "            bounds=bounds,\n",
    "            t_train=t_train,\n",
    "            y_train=y_train,\n",
    "            fit_kwargs=fit_kwargs,\n",
    "            plot_kwargs=plot_kwargs,\n",
    "        )\n",
    "\n",
    "        if weighted_cost_use == True:\n",
    "            self.cost_func = self.__weighted_cost_func__\n",
    "        else:\n",
    "            self.cost_func = self.__default_cost_func__\n",
    "\n",
    "    def __default_cost_func__(self, x: np.ndarray, t: np.ndarray, y: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The cost function to minimize.\n",
    "\n",
    "        Parameters:\n",
    "            x (np.ndarray): The parameters to estimate.\n",
    "            t (np.ndarray): The independent variable.\n",
    "            y (np.ndarray): The dependent variable.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The difference between the predicted and the actual data.\n",
    "        \"\"\"\n",
    "        diff = y - self.func(t, *x)\n",
    "\n",
    "        diff = np.ravel(diff)\n",
    "\n",
    "        # only use the non-NaN values\n",
    "        idx = np.where(~np.isnan(diff))\n",
    "        diff = diff[idx]\n",
    "        return diff\n",
    "\n",
    "    def __weighted_cost_func__(\n",
    "        self, x: np.ndarray, t: np.ndarray, y: np.ndarray, **kwargs\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The cost function to minimize.\n",
    "\n",
    "        Parameters:\n",
    "            x (np.ndarray): The parameters to estimate.\n",
    "            t (np.ndarray): The independent variable.\n",
    "            y (np.ndarray): The dependent variable.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The difference between the predicted and the actual data.\n",
    "        \"\"\"\n",
    "        diff = t**3 * (y - self.func(t, *x))\n",
    "        # diff = y - self.func(t, *x)\n",
    "\n",
    "        diff = np.ravel(diff)\n",
    "\n",
    "        # only use the non-NaN values\n",
    "        idx = np.where(~np.isnan(diff))\n",
    "        diff = diff[idx]\n",
    "        return diff\n",
    "\n",
    "\n",
    "class BasicDoubleLnNormalLeastSquare(smodels.LeastSquareFit):\n",
    "    \"\"\"\n",
    "    A class to perform least squares fitting for a double log-normal distribution.\n",
    "\n",
    "    Attributes:\n",
    "        name (str): The name of the fitting instance.\n",
    "        func (Callable): The model function to fit.\n",
    "        cost_func (Callable): The cost function to minimize.\n",
    "        x0 (np.ndarray): Initial guess for the parameters.\n",
    "        bounds (Bounds): Bounds on the parameters.\n",
    "        t_train (Union[np.ndarray, xr.DataArray]): Training data for the independent variable.\n",
    "        y_train (Union[np.ndarray, xr.DataArray]): Training data for the dependent variable.\n",
    "        fit_kwargs (Dict): Additional keyword arguments for the least_squares function.\n",
    "        plot_kwargs (Dict): Additional keyword arguments for plotting.\n",
    "        fit_result: The result of the fitting process.\n",
    "\n",
    "    Methods:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        x0: np.ndarray,\n",
    "        bounds: Bounds,\n",
    "        t_train: Union[xr.DataArray, np.ndarray],\n",
    "        y_train: Union[xr.DataArray, np.ndarray],\n",
    "        fit_kwargs: Dict = dict(),\n",
    "        plot_kwargs: Dict = dict(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DoubleLnNormalLeastSquare instance.\n",
    "\n",
    "        Parameters:\n",
    "            name (str): The name of the fitting instance.\n",
    "            x0 (np.ndarray): Initial guess for the parameters.\n",
    "            bounds (Bounds): Bounds on the parameters.\n",
    "            t_train (np.ndarray): Training data for the independent variable.\n",
    "            y_train (np.ndarray): Training data for the dependent variable.\n",
    "        \"\"\"\n",
    "\n",
    "        def this_func(\n",
    "            t: ndarray,\n",
    "            mu1: float,\n",
    "            sigma1: float,\n",
    "            scale_factor1: float,\n",
    "            mu2: float,\n",
    "            sigma2: float,\n",
    "            scale_factor2: float,\n",
    "        ) -> np.ndarray:\n",
    "\n",
    "            d1 = smodels.double_ln_normal_distribution(\n",
    "                t=t,\n",
    "                mu1=mu1,\n",
    "                sigma1=sigma1,\n",
    "                scale_factor1=scale_factor1,\n",
    "                mu2=mu2,\n",
    "                sigma2=sigma2,\n",
    "                scale_factor2=scale_factor2,\n",
    "            )\n",
    "\n",
    "            return d1\n",
    "\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            func=this_func,\n",
    "            x0=x0,\n",
    "            bounds=bounds,\n",
    "            t_train=t_train,\n",
    "            y_train=y_train,\n",
    "            fit_kwargs=fit_kwargs,\n",
    "            plot_kwargs=plot_kwargs,\n",
    "        )\n",
    "\n",
    "        self.cost_func = self.__default_cost_func__\n",
    "\n",
    "    def __default_cost_func__(self, x: np.ndarray, t: np.ndarray, y: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The cost function to minimize.\n",
    "\n",
    "        Parameters:\n",
    "            x (np.ndarray): The parameters to estimate.\n",
    "            t (np.ndarray): The independent variable.\n",
    "            y (np.ndarray): The dependent variable.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The difference between the predicted and the actual data.\n",
    "        \"\"\"\n",
    "        diff = y - self.func(t, *x)\n",
    "\n",
    "        diff = np.ravel(diff)\n",
    "\n",
    "        # only use the non-NaN values\n",
    "        idx = np.where(~np.isnan(diff))\n",
    "        diff = diff[idx]\n",
    "        return diff\n",
    "\n",
    "\n",
    "class WeightedDoubleLnNormalLeastSquare(smodels.LeastSquareFit):\n",
    "    \"\"\"\n",
    "    A class to perform least squares fitting for a double log-normal distribution.\n",
    "\n",
    "    Attributes:\n",
    "        name (str): The name of the fitting instance.\n",
    "        func (Callable): The model function to fit.\n",
    "        cost_func (Callable): The cost function to minimize.\n",
    "        x0 (np.ndarray): Initial guess for the parameters.\n",
    "        bounds (Bounds): Bounds on the parameters.\n",
    "        t_train (Union[np.ndarray, xr.DataArray]): Training data for the independent variable.\n",
    "        y_train (Union[np.ndarray, xr.DataArray]): Training data for the dependent variable.\n",
    "        fit_kwargs (Dict): Additional keyword arguments for the least_squares function.\n",
    "        plot_kwargs (Dict): Additional keyword arguments for plotting.\n",
    "        fit_result: The result of the fitting process.\n",
    "\n",
    "    Methods:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        x0: np.ndarray,\n",
    "        bounds: Bounds,\n",
    "        t_train: Union[xr.DataArray, np.ndarray],\n",
    "        y_train: Union[xr.DataArray, np.ndarray],\n",
    "        fit_kwargs: Dict = dict(),\n",
    "        plot_kwargs: Dict = dict(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DoubleLnNormalLeastSquare instance.\n",
    "\n",
    "        Parameters:\n",
    "            name (str): The name of the fitting instance.\n",
    "            x0 (np.ndarray): Initial guess for the parameters.\n",
    "            bounds (Bounds): Bounds on the parameters.\n",
    "            t_train (np.ndarray): Training data for the independent variable.\n",
    "            y_train (np.ndarray): Training data for the dependent variable.\n",
    "        \"\"\"\n",
    "\n",
    "        def this_func(\n",
    "            t: ndarray,\n",
    "            mu1: float,\n",
    "            sigma1: float,\n",
    "            scale_factor1: float,\n",
    "            mu2: float,\n",
    "            sigma2: float,\n",
    "            scale_factor2: float,\n",
    "        ) -> np.ndarray:\n",
    "\n",
    "            d1 = smodels.log_normal_distribution_all(\n",
    "                x=t,\n",
    "                mu=mu1,\n",
    "                sigma=sigma1,\n",
    "                scale=scale_factor1,\n",
    "                parameter_space=\"geometric\",\n",
    "                space=\"cleo\",\n",
    "            )\n",
    "            d2 = smodels.log_normal_distribution_all(\n",
    "                x=t,\n",
    "                mu=mu2,\n",
    "                sigma=sigma2,\n",
    "                scale=scale_factor2,\n",
    "                parameter_space=\"geometric\",\n",
    "                space=\"cleo\",\n",
    "            )\n",
    "\n",
    "            return d1 + d2\n",
    "\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            func=this_func,\n",
    "            x0=x0,\n",
    "            bounds=bounds,\n",
    "            t_train=t_train,\n",
    "            y_train=y_train,\n",
    "            fit_kwargs=fit_kwargs,\n",
    "            plot_kwargs=plot_kwargs,\n",
    "        )\n",
    "\n",
    "        self.cost_func = self.__default_cost_func__\n",
    "\n",
    "    def __default_cost_func__(self, x: np.ndarray, t: np.ndarray, y: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The cost function to minimize.\n",
    "\n",
    "        Parameters:\n",
    "            x (np.ndarray): The parameters to estimate.\n",
    "            t (np.ndarray): The independent variable.\n",
    "            y (np.ndarray): The dependent variable.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The difference between the predicted and the actual data.\n",
    "        \"\"\"\n",
    "        diff = t**3 * (y - self.func(t, *x))\n",
    "        # diff = y - self.func(t, *x)\n",
    "\n",
    "        diff = np.ravel(diff)\n",
    "\n",
    "        # only use the non-NaN values\n",
    "        idx = np.where(~np.isnan(diff))\n",
    "        diff = diff[idx]\n",
    "        return diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New approach:\n",
    "\n",
    "- limit radius range to 15µm to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:01<00:00, 80.97it/s]\n"
     ]
    }
   ],
   "source": [
    "list_lwc = []\n",
    "list_lwc_sem = []\n",
    "list_lwc_50um = []\n",
    "list_lwc_50um_sem = []\n",
    "\n",
    "list_nbc = []\n",
    "list_nbc_sem = []\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"]):\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    lwc = cc[\"liquid_water_content\"]\n",
    "\n",
    "    lwc_mean, lwc_sem = mean_and_stderror_of_mean(lwc, dims=(\"time\",))\n",
    "\n",
    "    list_lwc.append(lwc_mean)\n",
    "    list_lwc_sem.append(lwc_sem)\n",
    "\n",
    "    lwc_above_50um = (\n",
    "        (cc[\"mass_size_distribution\"] * cc[\"bin_width\"]).sel(radius=slice(50e-6, None)).sum(\"radius\")\n",
    "    )\n",
    "    lwc_above_50um_mean, lwc_above_50um_sem = mean_and_stderror_of_mean(lwc_above_50um, dims=(\"time\",))\n",
    "    list_lwc_50um.append(lwc_above_50um_mean)\n",
    "    list_lwc_50um_sem.append(lwc_above_50um_sem)\n",
    "\n",
    "    nbc = (cc[\"particle_size_distribution\"] * cc[\"bin_width\"]).sum(\"radius\")\n",
    "\n",
    "    nbc_mean, nbc_sem = mean_and_stderror_of_mean(nbc, dims=(\"time\",))\n",
    "    list_nbc.append(nbc_mean)\n",
    "    list_nbc_sem.append(nbc_sem)\n",
    "\n",
    "\n",
    "da_lwc = xr.concat(\n",
    "    list_lwc,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_lwc.attrs = dict(\n",
    "    long_name=\"Liquid water content\",\n",
    "    units=\"g m^{-3}\",\n",
    ")\n",
    "\n",
    "da_lwc_sem = xr.concat(\n",
    "    list_lwc_sem,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_lwc_sem.attrs = dict(\n",
    "    long_name=\"Standard error of the mean of the liquid water content\",\n",
    "    units=\"g m^{-3}\",\n",
    ")\n",
    "\n",
    "da_lwc_50um = xr.concat(\n",
    "    list_lwc_50um,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_lwc_50um.attrs = dict(\n",
    "    long_name=\"Liquid water content above 50 µm\",\n",
    "    units=\"g m^{-3}\",\n",
    ")\n",
    "\n",
    "da_lwc_50um_sem = xr.concat(\n",
    "    list_lwc_50um_sem,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_lwc_50um_sem.attrs = dict(\n",
    "    long_name=\"Standard error of the mean of the liquid water content above 50 µm\",\n",
    "    units=\"g m^{-3}\",\n",
    ")\n",
    "\n",
    "da_nbc = xr.concat(\n",
    "    list_nbc,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_nbc.attrs = dict(\n",
    "    long_name=\"Number concentration\",\n",
    "    units=\"m^{-3}\",\n",
    ")\n",
    "\n",
    "da_nbc_sem = xr.concat(\n",
    "    list_nbc_sem,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_nbc_sem.attrs = dict(\n",
    "    long_name=\"Standard error of the mean of the number concentration\",\n",
    "    units=\"m^{-3}\",\n",
    ")\n",
    "\n",
    "ds_observations = xr.Dataset(\n",
    "    dict(\n",
    "        liquid_water_content=da_lwc,\n",
    "        liquid_water_content_sem=da_lwc_sem,\n",
    "        liquid_water_content_50um=da_lwc_50um,\n",
    "        liquid_water_content_50um_sem=da_lwc_50um_sem,\n",
    "        particle_size_distribution=da_nbc,\n",
    "        particle_size_distribution_sem=da_nbc_sem,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit all clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = 1e-6\n",
    "# end = 1.5e-3\n",
    "# r = np.geomspace(start, end, 10000)\n",
    "# t_test = xr.DataArray(data=r, coords={\"radius\": r}, dims=[\"radius\"])\n",
    "# w_test = 0.5 * (t_test - t_test.shift(radius=2)).shift(radius=-1)\n",
    "# t_test = t_test.isel(radius=slice(1, -1))\n",
    "# w_test = w_test.isel(radius=slice(1, -1))\n",
    "\n",
    "r = np.geomspace(0.1e-6, 3e-3, 100)\n",
    "t_test = xr.DataArray(data=r, coords={\"radius\": r}, dims=[\"radius\"])\n",
    "w_test = (t_test - t_test.shift(radius=2)).shift(radius=-1)\n",
    "w_test = w_test.interpolate_na(\"radius\", method=\"linear\", fill_value=\"extrapolate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit in number concentration space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:06<00:00, 24.44it/s]\n"
     ]
    }
   ],
   "source": [
    "particle_size_distribution_parameters = dict()\n",
    "fitted_data = []\n",
    "\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"]):\n",
    "\n",
    "    # extract the cloud composite data for the cloud\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    # use the particle size distribution and the radius\n",
    "    psd = cc[\"particle_size_distribution\"]  # .sel(radius = slice(15e-6, None))\n",
    "    radius = cc[\"radius\"].expand_dims(time=cc[\"time\"])  # .sel(radius = slice(15e-6, None))\n",
    "\n",
    "    # make sure to have the same order of dimensions\n",
    "    psd = psd.transpose(\"time\", \"radius\")\n",
    "    radius = radius.transpose(\"time\", \"radius\")\n",
    "\n",
    "    # fit the double log-normal distribution\n",
    "    double_ln = CleoDoubleLnNormal(\n",
    "        name=\"PSD\",\n",
    "        x0=PSDBounds.x0(),\n",
    "        bounds=PSDBounds.bounds(),\n",
    "        t_train=radius.mean(\"time\", skipna=True),\n",
    "        y_train=psd.mean(\"time\", skipna=True),\n",
    "        fit_kwargs=dict(loss=\"linear\", kwargs=dict(variance=1)),\n",
    "    )\n",
    "    double_ln.fit(5)\n",
    "\n",
    "    # save the parameters\n",
    "    particle_size_distribution_parameters[str(cloud_id)] = double_ln.parameters\n",
    "\n",
    "    # predict the number concentration\n",
    "    dimension, prediction = double_ln.predict(t_test)\n",
    "\n",
    "    # # we can also use only the radii where we have data:\n",
    "    # radii_measured = cc[\"particle_size_distribution\"].mean(\"time\") > 0\n",
    "    # radius_end = cc[\"radius\"].sel(radius=radii_measured).max().values\n",
    "    # radius_start = cc[\"radius\"].sel(radius=radii_measured).min().values\n",
    "    # radius_end = 1.5 * radius_end\n",
    "    # prediction = prediction.where(dimension >= radius_start, other = np.nan)\n",
    "    # prediction = prediction.where(dimension <= radius_end, other = np.nan)\n",
    "\n",
    "    fitted_data.append(prediction)\n",
    "\n",
    "\n",
    "# create a data array with the fitted number concentration\n",
    "particle_size_distribution_fitted_data = xr.concat(\n",
    "    fitted_data,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "particle_size_distribution_fitted_data[\"cloud_id\"] = identified_clusters[\"cloud_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:03<00:00, 40.18it/s]\n"
     ]
    }
   ],
   "source": [
    "weighted_particle_size_distribution_parameters = dict()\n",
    "fitted_data = []\n",
    "\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"]):\n",
    "\n",
    "    # extract the cloud composite data for the cloud\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    # use the particle size distribution and the radius\n",
    "    psd = cc[\"particle_size_distribution\"]  # .sel(radius = slice(15e-6, None))\n",
    "    radius = cc[\"radius\"].expand_dims(time=cc[\"time\"])  # .sel(radius = slice(15e-6, None))\n",
    "\n",
    "    # make sure to have the same order of dimensions\n",
    "    psd = psd.transpose(\"time\", \"radius\")\n",
    "    radius = radius.transpose(\"time\", \"radius\")\n",
    "\n",
    "    # fit the double log-normal distribution\n",
    "    double_ln = CleoDoubleLnNormal(\n",
    "        name=\"PSD weighted\",\n",
    "        x0=PSDBounds.x0(),\n",
    "        bounds=PSDBounds.bounds(),\n",
    "        t_train=radius.mean(\"time\", skipna=True),\n",
    "        y_train=psd.mean(\"time\", skipna=True),\n",
    "        fit_kwargs=dict(loss=\"linear\", kwargs=dict(variance=1)),\n",
    "        weighted_cost_use=True,\n",
    "    )\n",
    "    double_ln.fit(5)\n",
    "\n",
    "    # save the parameters\n",
    "    weighted_particle_size_distribution_parameters[str(cloud_id)] = double_ln.parameters\n",
    "\n",
    "    # predict the number concentration\n",
    "    dimension, prediction = double_ln.predict(t_test)\n",
    "\n",
    "    # # we can also use only the radii where we have data:\n",
    "    # radii_measured = cc[\"particle_size_distribution\"].mean(\"time\") > 0\n",
    "    # radius_end = cc[\"radius\"].sel(radius=radii_measured).max().values\n",
    "    # radius_start = cc[\"radius\"].sel(radius=radii_measured).min().values\n",
    "    # radius_end = 1.5 * radius_end\n",
    "    # prediction = prediction.where(dimension >= radius_start, other = np.nan)\n",
    "    # prediction = prediction.where(dimension <= radius_end, other = np.nan)\n",
    "\n",
    "    fitted_data.append(prediction)\n",
    "\n",
    "\n",
    "# create a data array with the fitted number concentration\n",
    "weighted_particle_size_distribution_fitted_data = xr.concat(\n",
    "    fitted_data,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "weighted_particle_size_distribution_fitted_data[\"cloud_id\"] = identified_clusters[\"cloud_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit in mass concentration space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:07<00:00, 19.96it/s]\n"
     ]
    }
   ],
   "source": [
    "mass_size_distribution_parameters = dict()\n",
    "fitted_data = []\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"]):\n",
    "\n",
    "    # extract the cloud composite data for the cloud\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    # use the mass size distribution and the radius\n",
    "    msd = cc[\"mass_size_distribution\"]  # .sel(radius = slice(15e-6, None))\n",
    "    radius = cc[\"radius\"].expand_dims(time=cc[\"time\"])  # .sel(radius = slice(15e-6, None))\n",
    "\n",
    "    # make sure to have the same order of dimensions\n",
    "    msd = msd.transpose(\"time\", \"radius\")\n",
    "    radius = radius.transpose(\"time\", \"radius\")\n",
    "\n",
    "    # fit the double log-normal distribution\n",
    "\n",
    "    double_ln = CleoDoubleLnNormal(\n",
    "        name=\"MSD\",\n",
    "        x0=MSDBounds.x0(),\n",
    "        bounds=MSDBounds.bounds(),\n",
    "        t_train=radius.mean(\"time\", skipna=True),\n",
    "        y_train=msd.mean(\"time\", skipna=True),\n",
    "        fit_kwargs=dict(loss=\"linear\", kwargs=dict(variance=1)),\n",
    "    )\n",
    "    double_ln.fit(5)\n",
    "\n",
    "    # save the parameters\n",
    "    mass_size_distribution_parameters[str(cloud_id)] = double_ln.parameters\n",
    "\n",
    "    # predict the mass concentration\n",
    "    dimension, prediction = double_ln.predict(t_test)\n",
    "\n",
    "    # # we can also use only the radii where we have data:\n",
    "    # radii_measured = cc[\"particle_size_distribution\"].mean(\"time\") > 0\n",
    "    # radius_end = cc[\"radius\"].sel(radius=radii_measured).max().values\n",
    "    # radius_start = cc[\"radius\"].sel(radius=radii_measured).min().values\n",
    "    # radius_end = 1.5 * radius_end\n",
    "    # prediction = prediction.where(dimension >= radius_start, other = np.nan)\n",
    "    # prediction = prediction.where(dimension <= radius_end, other = np.nan)\n",
    "\n",
    "    fitted_data.append(prediction)\n",
    "\n",
    "# create a data array with the fitted mass concentration\n",
    "mass_size_distribution_fitted_data = xr.concat(\n",
    "    fitted_data,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "mass_size_distribution_fitted_data[\"cloud_id\"] = identified_clusters[\"cloud_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the data into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the datasets\n",
    "dataset_fitted = xr.Dataset(\n",
    "    dict(\n",
    "        particle_size_distribution=particle_size_distribution_fitted_data,\n",
    "        weighted_particle_size_distribution=weighted_particle_size_distribution_fitted_data,\n",
    "        mass_size_distribution=mass_size_distribution_fitted_data,\n",
    "    )\n",
    ")\n",
    "dataset_fitted[\"bin_width\"] = w_test\n",
    "dataset_fitted[\"bin_width\"].attrs = dict(\n",
    "    long_name=\"Bin width\",\n",
    "    unit=\"m\",\n",
    ")\n",
    "\n",
    "# Fit to the number concentration\n",
    "dataset_fitted[\"particle_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Number concentration\", unit=\"m^{-3} m^{-1}\", comment=\"Fit to the number concentration\"\n",
    ")\n",
    "\n",
    "dataset_fitted[\"mass_size_distribution_from_nc\"] = (\n",
    "    msd_from_psd_dataarray(\n",
    "        da=dataset_fitted[\"particle_size_distribution\"] * dataset_fitted[\"bin_width\"],\n",
    "    )\n",
    "    / dataset_fitted[\"bin_width\"]\n",
    ")\n",
    "dataset_fitted[\"mass_size_distribution_from_nc\"].attrs = dict(\n",
    "    long_name=\"Mass concentration from number concentration\",\n",
    "    unit=\"kg m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the number concentration\",\n",
    ")\n",
    "\n",
    "# Fit to the number concentration with weight\n",
    "dataset_fitted[\"weighted_particle_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Weighted number concentration\",\n",
    "    unit=\"m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the number concentration\\nWeighted by the cube of the radius\",\n",
    ")\n",
    "\n",
    "dataset_fitted[\"mass_size_distribution_from_wnc\"] = (\n",
    "    msd_from_psd_dataarray(\n",
    "        da=dataset_fitted[\"weighted_particle_size_distribution\"] * dataset_fitted[\"bin_width\"],\n",
    "    )\n",
    "    / dataset_fitted[\"bin_width\"]\n",
    ")\n",
    "dataset_fitted[\"mass_size_distribution_from_wnc\"].attrs = dict(\n",
    "    long_name=\"Mass concentration from weighted number concentration\",\n",
    "    unit=\"kg m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the number concentration\\nWeighted by the cube of the radius\",\n",
    ")\n",
    "\n",
    "\n",
    "# Fit to the mass concentration\n",
    "dataset_fitted[\"mass_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Mass concentration\", unit=\"kg m^{-3} m^{-1}\", comment=\"Fit to the mass concentration\"\n",
    ")\n",
    "\n",
    "dataset_fitted[\"particle_size_distribution_from_mc\"] = (\n",
    "    psd_from_msd_dataarray(\n",
    "        da=dataset_fitted[\"mass_size_distribution\"] * dataset_fitted[\"bin_width\"],\n",
    "    )\n",
    "    / dataset_fitted[\"bin_width\"]\n",
    ")\n",
    "dataset_fitted[\"particle_size_distribution_from_mc\"].attrs = dict(\n",
    "    long_name=\"Number concentration from mass concentration\",\n",
    "    unit=\"m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the mass concentration\",\n",
    ")\n",
    "\n",
    "dataset_fitted[\"radius\"].attrs.update(coarse_composite[\"radius\"].attrs)\n",
    "\n",
    "dataset_fitted[\"radius_micrometer\"] = dataset_fitted[\"radius\"] * 1e6\n",
    "dataset_fitted[\"radius_micrometer\"].attrs = dict(\n",
    "    long_name=\"Radius\",\n",
    "    unit=\"µm\",\n",
    ")\n",
    "\n",
    "colors = dict(\n",
    "    particle_size_distribution=\"orange\",\n",
    "    mass_size_distribution_from_nc=\"orange\",\n",
    "    weighted_particle_size_distribution=\"blue\",\n",
    "    mass_size_distribution_from_wnc=\"blue\",\n",
    "    particle_size_distribution_from_mc=\"purple\",\n",
    "    mass_size_distribution=\"purple\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(13, 5))\n",
    "\n",
    "\n",
    "axs[0].plot(\n",
    "    dataset_fitted[\"radius_micrometer\"],\n",
    "    dataset_fitted[\"particle_size_distribution\"].T,\n",
    "    color=colors[\"particle_size_distribution\"],\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "axs[0].plot(\n",
    "    dataset_fitted[\"radius_micrometer\"],\n",
    "    dataset_fitted[\"particle_size_distribution_from_mc\"].T,\n",
    "    color=colors[\"particle_size_distribution_from_mc\"],\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "axs[0].plot(\n",
    "    dataset_fitted[\"radius_micrometer\"],\n",
    "    dataset_fitted[\"weighted_particle_size_distribution\"].T,\n",
    "    color=colors[\"weighted_particle_size_distribution\"],\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "\n",
    "axs[0].set_xscale(\"log\")\n",
    "axs[0].set_yscale(\"symlog\", linthresh=1e-1, linscale=0.1)\n",
    "axs[0].set_yticks((1e0, 1e4, 1e8, 1e12, 1e16))\n",
    "axs[0].set_ylim(0, 1e14)\n",
    "\n",
    "axs[0].set_xlabel(label_from_attrs(dataset_fitted[\"radius_micrometer\"]))\n",
    "axs[0].set_ylabel(label_from_attrs(dataset_fitted[\"particle_size_distribution\"]))\n",
    "\n",
    "\n",
    "axs[1].plot(\n",
    "    dataset_fitted[\"radius_micrometer\"],\n",
    "    dataset_fitted[\"mass_size_distribution_from_nc\"].T,\n",
    "    color=colors[\"mass_size_distribution_from_nc\"],\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "axs[1].plot(\n",
    "    dataset_fitted[\"radius_micrometer\"],\n",
    "    dataset_fitted[\"mass_size_distribution_from_wnc\"].T,\n",
    "    color=colors[\"mass_size_distribution_from_wnc\"],\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "axs[1].plot(\n",
    "    dataset_fitted[\"radius_micrometer\"],\n",
    "    dataset_fitted[\"mass_size_distribution\"].T,\n",
    "    color=colors[\"mass_size_distribution\"],\n",
    "    alpha=0.2,\n",
    ")\n",
    "axs[1].set_xscale(\"log\")\n",
    "axs[1].set_yscale(\"symlog\", linthresh=1e-10, linscale=0.1)\n",
    "axs[1].set_yticks((1e-8, 1e-6, 1e-4, 1e-2, 1e0, 1e2, 1e4))\n",
    "axs[1].set_ylim(0, 1e4)\n",
    "\n",
    "\n",
    "axs[1].set_xlabel(label_from_attrs(dataset_fitted[\"radius_micrometer\"]))\n",
    "axs[1].set_ylabel(label_from_attrs(dataset_fitted[\"mass_size_distribution\"]))\n",
    "\n",
    "fig.suptitle(\"Comparison of the three different fits\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"107/fit_comparison.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the LWC from the fits to the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the full radius range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 4.5), sharex=True, sharey=True)\n",
    "\n",
    "i = 0\n",
    "for key in (\n",
    "    \"mass_size_distribution_from_nc\",\n",
    "    \"mass_size_distribution_from_wnc\",\n",
    "    \"mass_size_distribution\",\n",
    "):\n",
    "\n",
    "    lwc = 1e3 * (dataset_fitted[key] * dataset_fitted[\"bin_width\"]).sum(\"radius\")\n",
    "\n",
    "    axs[i].errorbar(\n",
    "        x=ds_observations[\"liquid_water_content\"],\n",
    "        xerr=ds_observations[\"liquid_water_content_sem\"],\n",
    "        y=lwc,\n",
    "        yerr=0,\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        label=dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=colors[key],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[i].set_title(\n",
    "        dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=adjust_lightness_array(\n",
    "            [\n",
    "                colors[key],\n",
    "            ],\n",
    "            0.75,\n",
    "        )[0],\n",
    "    )\n",
    "\n",
    "    # axs[i].set_title(dataset_fitted[key].attrs['comment'])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "\n",
    "for _ax in axs:\n",
    "    _ax.plot([0, 3], [0, 3], color=\"black\", linestyle=\"--\")\n",
    "    # _ax.set_aspect('equal')\n",
    "    _ax.set_xlabel(\"Observed LWC [g m$^{-3}$]\")\n",
    "    _ax.set_ylabel(\"Fitted LWC [g m$^{-3}$]\")\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed LWC for multiple double Log-Normal fits\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "_ax.set_xlim(0, 5)\n",
    "_ax.set_ylim(0, 5)\n",
    "\n",
    "fig.savefig(\"107/LWC_fit_comparison.png\", dpi=400)\n",
    "\n",
    "_ax.set_xlim(0, 0.7)\n",
    "_ax.set_ylim(0, 0.7)\n",
    "\n",
    "fig.savefig(\"107/LWC_fit_comparison_zoom.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use only radii larger than 50 µm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 4.5), sharex=True, sharey=True)\n",
    "\n",
    "i = 0\n",
    "for key in (\n",
    "    \"mass_size_distribution_from_nc\",\n",
    "    \"mass_size_distribution_from_wnc\",\n",
    "    \"mass_size_distribution\",\n",
    "):\n",
    "\n",
    "    lwc = 1e3 * (dataset_fitted[key] * dataset_fitted[\"bin_width\"]).sel(radius=slice(50e-6, None)).sum(\n",
    "        \"radius\"\n",
    "    )\n",
    "\n",
    "    axs[i].errorbar(\n",
    "        x=1e3 * ds_observations[\"liquid_water_content_50um\"],\n",
    "        xerr=1e3 * ds_observations[\"liquid_water_content_50um_sem\"],\n",
    "        y=lwc,\n",
    "        yerr=0,\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        label=dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=colors[key],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "    axs[i].set_title(\n",
    "        dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=adjust_lightness_array(\n",
    "            [\n",
    "                colors[key],\n",
    "            ],\n",
    "            0.75,\n",
    "        )[0],\n",
    "    )\n",
    "    i += 1\n",
    "\n",
    "for _ax in axs:\n",
    "    _ax.plot([0, 3], [0, 3], color=\"black\", linestyle=\"--\")\n",
    "    # _ax.set_aspect('equal')\n",
    "    _ax.set_xlabel(\"Observed LWC [g m$^{-3}$]\")\n",
    "    _ax.set_ylabel(\"Fitted LWC [g m$^{-3}$]\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Comparison of fitted and observed LWC for multiple double Log-Normal fits\\n Only Radii above 50 µm\"\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "_ax.set_xlim(0, 5)\n",
    "_ax.set_ylim(0, 5)\n",
    "\n",
    "fig.savefig(\"107/LWC_fit_comparison-50um.png\", dpi=400)\n",
    "\n",
    "_ax.set_xlim(0, 0.7)\n",
    "_ax.set_ylim(0, 0.7)\n",
    "\n",
    "fig.savefig(\"107/LWC_fit_comparison-50um-zoom.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distributions of some random clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cloud_ids = np.random.choice(identified_clusters[\"cloud_id\"], 3, replace=False)\n",
    "\n",
    "large_cloud_ids = identified_clusters[\"cloud_id\"].where(\n",
    "    identified_clusters[\"liquid_water_content\"] / identified_clusters[\"duration\"].dt.seconds > 0.5,\n",
    "    drop=True,\n",
    ")\n",
    "large_cloud_ids = np.random.choice(large_cloud_ids, 3, replace=False)\n",
    "\n",
    "cloud_ids = np.concatenate([small_cloud_ids, large_cloud_ids])\n",
    "\n",
    "ncols_nrows = ncols_nrows_from_N(len(cloud_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "particle size distirbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10, 7), sharex=True, sharey=True, **ncols_nrows)\n",
    "\n",
    "# plot the PSDs of the selected clouds\n",
    "for i, cloud_id in enumerate(cloud_ids):\n",
    "    _ax = axs.flatten()[i]\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    lwc_mean, lwc_sem = mean_and_stderror_of_mean(cc[\"liquid_water_content\"], dims=(\"time\",))\n",
    "\n",
    "    _ax.set_title(\n",
    "        f\"Cloud ID: {cloud_id:.0f}\\n{lwc_mean.values:.2f} ± {lwc_sem.values:.2f} g\" + \"$m^{-3}$\"\n",
    "    )\n",
    "\n",
    "    observations = cc[\"particle_size_distribution\"]\n",
    "\n",
    "    _ax.plot(\n",
    "        1e6 * observations[\"radius\"],\n",
    "        observations.mean(\"time\"),\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        color=\"black\",\n",
    "    )\n",
    "\n",
    "    for key in (\n",
    "        \"particle_size_distribution\",\n",
    "        \"weighted_particle_size_distribution\",\n",
    "        \"particle_size_distribution_from_mc\",\n",
    "    ):\n",
    "\n",
    "        fit = dataset_fitted[key].sel(cloud_id=cloud_id)\n",
    "        _ax.plot(\n",
    "            dataset_fitted[\"radius_micrometer\"],\n",
    "            fit,\n",
    "            color=colors[key],\n",
    "            linestyle=\"-\",\n",
    "            label=fit.attrs[\"comment\"],\n",
    "        )\n",
    "\n",
    "    _ax.set_xscale(\"log\")\n",
    "    _ax.set_yscale(\"symlog\", linthresh=1e4, linscale=0.1)\n",
    "    _ax.set_yticks([0, 1e6, 1e9, 1e12])\n",
    "# axs.flatten()[0].legend()\n",
    "_ax.set_ylim(0, None)\n",
    "\n",
    "fig.supxlabel(label_from_attrs(dataset_fitted[\"radius_micrometer\"]))\n",
    "fig.supylabel(label_from_attrs(dataset_fitted[\"particle_size_distribution\"]))\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed PSDs for multiple double Log-Normal fits\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"107/PSD_fit_comparison.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10, 7), sharex=True, sharey=True, **ncols_nrows)\n",
    "\n",
    "# plot the PSDs of the selected clouds\n",
    "for i, cloud_id in enumerate(cloud_ids):\n",
    "    _ax = axs.flatten()[i]\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    lwc_mean, lwc_sem = mean_and_stderror_of_mean(cc[\"liquid_water_content\"], dims=(\"time\",))\n",
    "\n",
    "    _ax.set_title(\n",
    "        f\"Cloud ID: {cloud_id:.0f}\\n{lwc_mean.values:.2f} ± {lwc_sem.values:.2f} g\" + \"$m^{-3}$\"\n",
    "    )\n",
    "\n",
    "    observations = cc[\"mass_size_distribution\"]\n",
    "\n",
    "    _ax.plot(1e6 * cc[\"radius\"], observations.mean(\"time\"), marker=\".\", linestyle=\"None\", color=\"black\")\n",
    "\n",
    "    for key in (\n",
    "        \"mass_size_distribution\",\n",
    "        \"mass_size_distribution_from_wnc\",\n",
    "        \"mass_size_distribution_from_nc\",\n",
    "    ):\n",
    "\n",
    "        fit = dataset_fitted[key].sel(cloud_id=cloud_id)\n",
    "        _ax.plot(\n",
    "            dataset_fitted[\"radius_micrometer\"],\n",
    "            fit,\n",
    "            color=colors[key],\n",
    "            linestyle=\"-\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "    _ax.set_xscale(\"log\")\n",
    "    _ax.set_yscale(\"symlog\", linthresh=1e-6, linscale=0.1)\n",
    "    _ax.set_yticks([0, 1e-3, 1e0, 1e3])\n",
    "    _ax.set_xlim(1, 3e3)\n",
    "_ax.set_ylim(0, None)\n",
    "\n",
    "fig.supxlabel(\"Radius [$µm$]\")\n",
    "fig.supylabel(label_from_attrs(dataset_fitted[\"mass_size_distribution\"]))\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed MSDs for multiple double Log-Normal fits\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"107/MSD_fit_comparison.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10, 7), sharex=True, sharey=False, **ncols_nrows)\n",
    "\n",
    "# plot the PSDs of the selected clouds\n",
    "for i, cloud_id in enumerate(cloud_ids):\n",
    "    _ax = axs.flatten()[i]\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    lwc_mean, lwc_sem = mean_and_stderror_of_mean(cc[\"liquid_water_content\"], dims=(\"time\",))\n",
    "\n",
    "    _ax.set_title(\n",
    "        f\"Cloud ID: {cloud_id:.0f}\\n{lwc_mean.values:.2f} ± {lwc_sem.values:.2f} g\" + \"$m^{-3}$\"\n",
    "    )\n",
    "\n",
    "    observations = cc[\"mass_size_distribution\"] * cc[\"bin_width\"]\n",
    "\n",
    "    _ax.plot(\n",
    "        1e3 * observations[\"radius\"],\n",
    "        1e3 * observations.mean(\"time\"),\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        color=\"black\",\n",
    "    )\n",
    "\n",
    "    for key in (\n",
    "        \"mass_size_distribution_from_nc\",\n",
    "        \"mass_size_distribution_from_wnc\",\n",
    "        \"mass_size_distribution\",\n",
    "    ):\n",
    "\n",
    "        fit = dataset_fitted[key].sel(cloud_id=cloud_id)\n",
    "        _ax.plot(\n",
    "            1e3 * dataset_fitted[\"radius\"],\n",
    "            1e3 * fit * dataset_fitted[\"bin_width\"],\n",
    "            color=colors[key],\n",
    "            linestyle=\"-\",\n",
    "            label=fit.attrs[\"comment\"],\n",
    "            alpha=0.8,\n",
    "        )\n",
    "    _ax.set_xlim([0, 3])\n",
    "\n",
    "    # _ax.set_xscale('log')\n",
    "# _ax.set_yscale('symlog', linthresh = 1e4)\n",
    "for _ax in axs[0]:\n",
    "    _ax.set_ylim(0, 0.008)\n",
    "\n",
    "for _ax in axs[1]:\n",
    "    _ax.set_ylim(0, 0.15)\n",
    "\n",
    "\n",
    "fig.supxlabel(\"Radius [$mm$]\")\n",
    "fig.supylabel(\"Mass concentration [$g m^{-3}$]\")\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed MSDs for multiple double Log-Normal fits\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"107/MSD_fit_comparison_linear.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdm_eurec4a_env312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
