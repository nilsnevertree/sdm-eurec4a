{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import datetime\n",
    "\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple, List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import Bounds\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "from sdm_eurec4a import RepositoryPath, get_git_revision_hash\n",
    "from sdm_eurec4a.visulization import (\n",
    "    set_custom_rcParams,\n",
    "    adjust_lightness_array,\n",
    "    ncols_nrows_from_N,\n",
    "    label_from_attrs,\n",
    ")\n",
    "import sdm_eurec4a.input_processing.models as smodels\n",
    "from sdm_eurec4a.reductions import mean_and_stderror_of_mean\n",
    "from sdm_eurec4a.identifications import match_clouds_and_cloudcomposite\n",
    "from sdm_eurec4a.conversions import msd_from_psd_dataarray, psd_from_msd_dataarray\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "default_colors = set_custom_rcParams()\n",
    "dark_colors = adjust_lightness_array(default_colors, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LnParameters:\n",
    "\n",
    "    def __init__(self, mu, sigma, scale):\n",
    "        self.parameters = dict(\n",
    "            mu=mu,\n",
    "            sigma=sigma,\n",
    "            scale=scale,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def custom_parameters(self):\n",
    "        \"\"\"Return the parameters in a dictionary with custom keys.\n",
    "        Keys are:\n",
    "        - mu1\n",
    "        - sigma1\n",
    "        - scale_factor1\n",
    "        \"\"\"\n",
    "        result = dict(\n",
    "            mu1=self.parameters[\"mu\"],\n",
    "            sigma1=self.parameters[\"sigma\"],\n",
    "            scale_factor1=self.parameters[\"scale\"],\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def scipy_parameters(self):\n",
    "        result = dict(\n",
    "            s=self.parameters[\"sigma\"],\n",
    "            loc=self.parameters[\"mu\"],\n",
    "            scale=self.parameters[\"scale\"],\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def geom_parameters(self):\n",
    "        result = dict(\n",
    "            geomean=self.parameters[\"mu\"],\n",
    "            geosig=self.parameters[\"sigma\"],\n",
    "            scalefac=self.parameters[\"scale\"],\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def normal_parameters(self):\n",
    "        result = dict(\n",
    "            mu=self.parameters[\"mu\"],\n",
    "            sigma=self.parameters[\"sigma\"],\n",
    "            scale=self.parameters[\"scale\"],\n",
    "        )\n",
    "        return result\n",
    "\n",
    "\n",
    "def mass_from_number(t: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    return 4 / 3 * np.pi * t**3 * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometrical_mu_logspace2linear(\n",
    "    mu: Union[float, np.ndarray],\n",
    "    sigma: Union[float, np.ndarray],\n",
    ") -> Union[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    With this function, one can transform the\n",
    "    \"\"\"\n",
    "\n",
    "    return mu * np.exp(np.log(sigma) ** 2)\n",
    "\n",
    "\n",
    "def scale_logspace2linear(\n",
    "    scale,\n",
    "    mu,\n",
    "    sigma,\n",
    "):\n",
    "\n",
    "    x_max = np.exp(mu)\n",
    "\n",
    "    return x_max * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PSDBounds:\n",
    "\n",
    "#     mu1 = np.array([10e-6,  14e-6,  50e6])\n",
    "#     mu2 = np.array([0.1e-6, 0.3e-6, 0.5e-3])\n",
    "#     sigma1 = np.array([1.3, 1.5, 3.0])\n",
    "#     sigma2 = np.array([1.2, 1.4, 2.0])\n",
    "#     scale1 = np.array([1e-20, 1e13, 1e16])\n",
    "#     scale2 = np.array([1e-20, 1e3,  1e10])\n",
    "\n",
    "#     def get_x0_bounds(\n",
    "#             mu1 : np.ndarray,\n",
    "#             sigma1 : np.ndarray,\n",
    "#             scale1 : np.ndarray,\n",
    "#             mu2 : np.ndarray,\n",
    "#             sigma2 : np.ndarray,\n",
    "#             scale2 : np.ndarray,\n",
    "#             ) -> np.ndarray:\n",
    "\n",
    "\n",
    "#         x0 = np.array([\n",
    "#             mu1[1], sigma1[1], scale1[1],\n",
    "#             mu2[1], sigma2[1], scale2[1],\n",
    "#         ])\n",
    "#         lower_bound = np.array([\n",
    "#             mu1[0], sigma1[0], scale1[0],\n",
    "#             mu2[0], sigma2[0], scale2[0],\n",
    "#         ])\n",
    "#         upper_bound = np.array([\n",
    "#             mu1[2], sigma1[2], scale1[2],\n",
    "#             mu2[2], sigma2[2], scale2[2],\n",
    "#         ])\n",
    "\n",
    "#         bounds = Bounds(\n",
    "#             lb = lower_bound,\n",
    "#             ub = upper_bound,\n",
    "#         )\n",
    "\n",
    "#         return x0, bounds\n",
    "\n",
    "#     @staticmethod\n",
    "#     def bounds():\n",
    "#         x0, bounds = PSDBounds.get_x0_bounds(\n",
    "#             mu1 = PSDBounds.mu1,\n",
    "#             sigma1 = PSDBounds.sigma1,\n",
    "#             scale1 = PSDBounds.scale1,\n",
    "#             mu2 = PSDBounds.mu2,\n",
    "#             sigma2 = PSDBounds.sigma2,\n",
    "#             scale2 = PSDBounds.scale2,\n",
    "#         )\n",
    "#         return bounds\n",
    "\n",
    "#     @staticmethod\n",
    "#     def x0():\n",
    "#         x0, bounds = PSDBounds.get_x0_bounds(\n",
    "#             mu1 = PSDBounds.mu1,\n",
    "#             sigma1 = PSDBounds.sigma1,\n",
    "#             scale1 = PSDBounds.scale1,\n",
    "#             mu2 = PSDBounds.mu2,\n",
    "#             sigma2 = PSDBounds.sigma2,\n",
    "#             scale2 = PSDBounds.scale2,\n",
    "#         )\n",
    "#         return x0\n",
    "\n",
    "\n",
    "#     @staticmethod\n",
    "#     def bounds_linear():\n",
    "#         sf = 1/ 2\n",
    "#         x0, bounds = PSDBounds.get_x0_bounds(\n",
    "#             mu1 = PSDBounds.mu1 + (PSDBounds.sigma1[1] * sf)  ** 2,\n",
    "#             sigma1 = PSDBounds.sigma1 * sf,\n",
    "#             scale1 = PSDBounds.scale1,\n",
    "#             mu2 = PSDBounds.mu2 + (PSDBounds.sigma2[1] * sf)  ** 2,\n",
    "#             sigma2 = PSDBounds.sigma2 * sf,\n",
    "#             scale2 = PSDBounds.scale2,\n",
    "#         )\n",
    "#         return bounds\n",
    "\n",
    "#     @staticmethod\n",
    "#     def x0_linear():\n",
    "#         sf = 1/ 2\n",
    "#         x0, bounds = PSDBounds.get_x0_bounds(\n",
    "#             mu1 = PSDBounds.mu1 + (PSDBounds.sigma1[1] * sf)  ** 2,\n",
    "#             sigma1 = PSDBounds.sigma1 * sf,\n",
    "#             scale1 = PSDBounds.scale1,\n",
    "#             mu2 = PSDBounds.mu2 + (PSDBounds.sigma2[1] * sf)  ** 2,\n",
    "#             sigma2 = PSDBounds.sigma2 * sf,\n",
    "#             scale2 = PSDBounds.scale2,\n",
    "#         )\n",
    "#         return x0\n",
    "\n",
    "# class StandardizedParameters:\n",
    "#     \"\"\"\n",
    "#     Represents the full set of log-normal parameters derived from the base triple.\n",
    "\n",
    "#     Parameters:\n",
    "#         mu_L: Union[float, np.ndarray]\n",
    "#             Mean of the log-normal distribution in log space (L(x)).\n",
    "#         sigma_L: Union[float, np.ndarray]\n",
    "#             Standard deviation of the log-normal distribution in log space (L(x)).\n",
    "#         scale_L: Union[float, np.ndarray]\n",
    "#             Scale factor for the L(x) distribution.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, mu_L: Union[float, np.ndarray], sigma_L: Union[float, np.ndarray], scale_L: Union[float, np.ndarray]):\n",
    "#         self.mu_L = mu_L\n",
    "#         self.sigma_L = sigma_L\n",
    "#         self.scale_L = scale_L\n",
    "\n",
    "\n",
    "#     # make the individual parameters accessible by dict like access\n",
    "#     def __getitem__(self, key: str) -> Union[float, np.ndarray]:\n",
    "#         return self.__dict__()[key]\n",
    "\n",
    "\n",
    "#     @property\n",
    "#     def mu_l(self) -> Union[float, np.ndarray]:\n",
    "#         \"\"\"Compute mu_l for l(y).\"\"\"\n",
    "#         return self.mu_L - self.sigma_L**2\n",
    "\n",
    "#     @property\n",
    "#     def scale_l(self) -> Union[float, np.ndarray]:\n",
    "#         \"\"\"Compute scale_l for l(y).\"\"\"\n",
    "#         return self.scale_L / np.exp(-0.5 * self.sigma_L**2)\n",
    "\n",
    "#     @property\n",
    "#     def x_max(self) -> Union[float, np.ndarray]:\n",
    "#         \"\"\"Compute x_max (mode of L(x)).\"\"\"\n",
    "#         return np.exp(self.mu_L - self.sigma_L**2)\n",
    "\n",
    "#     @property\n",
    "#     def max_L(self) -> Union[float, np.ndarray]:\n",
    "#         \"\"\"Compute maximum value of L(x).\"\"\"\n",
    "#         max_L = scale_L / (self.x_max * self.sigma_L * np.sqrt(2 * np.pi) * np.exp(0.5 * self.sigma_L ** 2))\n",
    "#         return max_L\n",
    "\n",
    "#     @property\n",
    "#     def max_l(self) -> Union[float, np.ndarray]:\n",
    "#         \"\"\"Compute maximum value of l(y).\"\"\"\n",
    "#         return self.scale_l / self.sigma_L / np.sqrt(2 * np.pi)\n",
    "\n",
    "#     @property\n",
    "#     def geometric_mean_L(self) -> Union[float, np.ndarray]:\n",
    "#         \"\"\"Compute geometric mean of the distribution.\"\"\"\n",
    "#         return np.exp(self.mu_L)\n",
    "\n",
    "#     @property\n",
    "#     def geometric_std_dev_L(self) -> Union[float, np.ndarray]:\n",
    "#         \"\"\"Compute geometric standard deviation of the distribution.\"\"\"\n",
    "#         return np.exp(self.sigma_L)\n",
    "\n",
    "#     @property\n",
    "#     def sigma_l(self) -> Union[float, np.ndarray]:\n",
    "#         return self.sigma_L\n",
    "\n",
    "#     @property\n",
    "#     def geometric_mean_l(self) -> Union[float, np.ndarray]:\n",
    "#         \"\"\"Compute geometric mean of the distribution.\"\"\"\n",
    "#         return np.exp(self.mu_l)\n",
    "\n",
    "#     @property\n",
    "#     def geometric_std_dev_l(self) -> Union[float, np.ndarray]:\n",
    "#         return np.exp(self.sigma_L)\n",
    "\n",
    "#     def __params_to_dict__(\n",
    "#             self,\n",
    "#             get_keys: Tuple[str, str, str],\n",
    "#             dict_keys: Tuple[str, str, str],\n",
    "#             ) -> dict:\n",
    "\n",
    "#         return {dict_keys[i]: getattr(self, get_keys[i]) for i in range(3)}\n",
    "\n",
    "#     def get_L_parameters(self, dict_keys : Union[None, Tuple[str, str, str]] = None) -> dict:\n",
    "#         get_keys = (\"mu_L\", \"sigma_L\", \"scale_L\")\n",
    "#         if dict_keys is None:\n",
    "#             dict_keys = get_keys\n",
    "\n",
    "#         return self.__params_to_dict__(get_keys, dict_keys)\n",
    "\n",
    "#     def get_l_parameters(self, dict_keys : Union[None, Tuple[str, str, str]] = None) -> dict:\n",
    "#         get_keys = (\"mu_l\", \"sigma_l\", \"scale_l\")\n",
    "#         if dict_keys is None:\n",
    "#             dict_keys = get_keys\n",
    "\n",
    "#         return self.__params_to_dict__(get_keys, dict_keys)\n",
    "\n",
    "#     def get_x_parameters(self, dict_keys : Union[None, Tuple[str, str, str]] = None) -> dict:\n",
    "#         get_keys = (\"x_max\", \"max_L\", \"sigma_L\")\n",
    "#         if dict_keys is None:\n",
    "#             dict_keys = get_keys\n",
    "\n",
    "#         return self.__params_to_dict__(get_keys, dict_keys)\n",
    "\n",
    "\n",
    "#     def get_geometric_parametersL(self, dict_keys : Union[None, Tuple[str, str, str]] = None) -> dict:\n",
    "#         get_keys = (\"geometric_mean_L\", \"geometric_std_dev_L\", \"scale_L\")\n",
    "#         if dict_keys is None:\n",
    "#             dict_keys = get_keys\n",
    "#         return self.__params_to_dict__(get_keys, dict_keys)\n",
    "\n",
    "#     def get_geometric_parametersl(self, dict_keys : Union[None, Tuple[str, str, str]] = None) -> dict:\n",
    "#         get_keys = (\"geometric_mean_l\", \"geometric_std_dev_l\", \"scale_l\")\n",
    "#         if dict_keys is None:\n",
    "#             dict_keys = get_keys\n",
    "#         return self.__params_to_dict__(get_keys, dict_keys)\n",
    "\n",
    "\n",
    "#     def get_xmax_geometric_std_dev_maxvalue_L(self, dict_keys : Union[None, Tuple[str, str, str]] = None) -> dict:\n",
    "#         get_keys = (\"x_max\", \"geometric_std_dev_L\", \"max_L\")\n",
    "#         if dict_keys is None:\n",
    "#             dict_keys = get_keys\n",
    "#         return self.__params_to_dict__(get_keys, dict_keys)\n",
    "#     def get_xmax_geometric_std_dev_maxvalue_l(self, dict_keys : Union[None, Tuple[str, str, str]] = None) -> dict:\n",
    "#         get_keys = (\"x_max\", \"geometric_std_dev_l\", \"max_l\")\n",
    "#         if dict_keys is None:\n",
    "#             dict_keys = get_keys\n",
    "#         return self.__params_to_dict__(get_keys, dict_keys)\n",
    "\n",
    "#     def get_xmax_geometric_std_dev_L(self, dict_keys : Union[None, Tuple[str, str, str]] = None) -> dict:\n",
    "#         get_keys = (\"x_max\", \"geometric_std_dev_L\", \"scale_L\")\n",
    "#         if dict_keys is None:\n",
    "#             dict_keys = get_keys\n",
    "#         return self.__params_to_dict__(get_keys, dict_keys)\n",
    "\n",
    "#     def get_xmax_geometric_std_dev_l(self, dict_keys : Union[None, Tuple[str, str, str]] = None) -> dict:\n",
    "#         get_keys = (\"x_max\", \"geometric_std_dev_l\", \"scale_l\")\n",
    "#         if dict_keys is None:\n",
    "#             dict_keys = get_keys\n",
    "#         return self.__params_to_dict__(get_keys, dict_keys)\n",
    "\n",
    "#     def summary(self) -> dict:\n",
    "#         \"\"\"Provide a summary of all parameters.\"\"\"\n",
    "#         return {\n",
    "#             \"mu_L\": self.mu_L,\n",
    "#             \"sigma_L\": self.sigma_L,\n",
    "#             \"scale_L\": self.scale_L,\n",
    "#             \"mu_l\": self.mu_l,\n",
    "#             \"sigma_l\": self.sigma_L,\n",
    "#             \"scale_l\": self.scale_l,\n",
    "#             \"x_max\": self.x_max,\n",
    "#             \"max_L\": self.max_L,\n",
    "#             \"max_l\": self.max_l,\n",
    "#             \"geometric_mean_L\": self.geometric_mean_L,\n",
    "#             \"geometric_std_dev_L\": self.geometric_std_dev_L,\n",
    "#         }\n",
    "\n",
    "# from abc import ABC, abstractmethod\n",
    "# import numpy as np\n",
    "\n",
    "# class LogNormalParameters():\n",
    "#     \"\"\"Abstract base class for parameter sets.\"\"\"\n",
    "\n",
    "#     def to_base_triple(self):\n",
    "#         \"\"\"Convert to the canonical form: mu_L, sigma_L, scale_L.\"\"\"\n",
    "#         pass\n",
    "\n",
    "#     def standardize(self):\n",
    "#         \"\"\"Convert the Child Class to the standard class.\"\"\"\n",
    "#         return StandardizedParameters(*self.to_base_triple())\n",
    "\n",
    "#     # make the individual parameters accessible by dict like access\n",
    "#     def __getitem__(self, key: str) -> Union[float, np.ndarray]:\n",
    "#         return self.__dict__()[key]\n",
    "\n",
    "# class GeometricMeanSigmaL(LogNormalParameters):\n",
    "#     \"\"\"\n",
    "#     Represents the log-normal distribution using the geometric mean (G),\n",
    "#     geometric standard deviation (GSD), and scale for L(x).\n",
    "\n",
    "#     Parameters:\n",
    "#         geometric_mean_L: Union[float, np.ndarray]\n",
    "#             The geometric mean (G) of the distribution.\n",
    "#         geometric_std_dev_L: Union[float, np.ndarray]\n",
    "#             The geometric standard deviation (GSD) of the distribution.\n",
    "#         scale_L: Union[float, np.ndarray]\n",
    "#             The scale for the L(x) distribution.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, geometric_mean_L: Union[float, np.ndarray], geometric_std_dev_L: Union[float, np.ndarray], scale_L: Union[float, np.ndarray]):\n",
    "#         self.geometric_mean_L = geometric_mean_L\n",
    "#         self.geometric_std_dev_L = geometric_std_dev_L\n",
    "#         self.scale_L = scale_L\n",
    "\n",
    "#     def to_base_triple(self) -> Tuple[Union[float, np.ndarray], Union[float, np.ndarray], Union[float, np.ndarray]]:\n",
    "#         \"\"\"\n",
    "#         Convert geometric mean and geometric standard deviation to the base triple.\n",
    "\n",
    "#         Returns:\n",
    "#             Tuple containing (mu_L, sigma_L, scale_L).\n",
    "#         \"\"\"\n",
    "#         mu_L = np.log(self.geometric_mean_L)\n",
    "#         sigma_L = np.log(self.geometric_std_dev_L)\n",
    "#         return mu_L, sigma_L, self.scale_L\n",
    "\n",
    "# class GeometricMeanSigmaLogspace(LogNormalParameters):\n",
    "#     \"\"\"\n",
    "#     Represents the log-normal distribution using the geometric mean (G),\n",
    "#     geometric standard deviation (GSD), and scale for l(x).\n",
    "\n",
    "#     Parameters:\n",
    "#         geometric_mean_L: Union[float, np.ndarray]\n",
    "#             The geometric mean (G) of the distribution.\n",
    "#         geometric_std_dev_L: Union[float, np.ndarray]\n",
    "#             The geometric standard deviation (GSD) of the distribution.\n",
    "#         scale_L: Union[float, np.ndarray]\n",
    "#             The scale for the L(x) distribution.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, geometric_mean_l: Union[float, np.ndarray], geometric_std_dev_l: Union[float, np.ndarray], scale_l: Union[float, np.ndarray]):\n",
    "#         self.geometric_mean_l = geometric_mean_l\n",
    "#         self.geometric_std_dev_l = geometric_std_dev_l\n",
    "#         self.scale_l = scale_l\n",
    "\n",
    "#     def to_base_triple(self) -> Tuple[Union[float, np.ndarray], Union[float, np.ndarray], Union[float, np.ndarray]]:\n",
    "#         \"\"\"\n",
    "#         Convert geometric mean and geometric standard deviation to the base triple.\n",
    "\n",
    "#         Returns:\n",
    "#             Tuple containing (mu_L, sigma_L, scale_L).\n",
    "#         \"\"\"\n",
    "#         mu_l = np.log(self.geometric_mean_L)\n",
    "#         sigma_l = np.log(self.geometric_std_dev_L)\n",
    "#         sigma_L = sigma_l\n",
    "#         mu_L = mu_l + sigma_l ** 2\n",
    "\n",
    "#         fraction =\n",
    "#         scale_L = self.scale_l *\n",
    "\n",
    "#         return mu_L, sigma_L, self.scale_L\n",
    "\n",
    "\n",
    "# class MaxXAndMaxL(LogNormalParameters):\n",
    "#     \"\"\"\n",
    "#     Represents the log-normal distribution using the mode (x_max)\n",
    "#     and maximum value (max_L) of L(x), and sigma_L.\n",
    "\n",
    "#     Parameters:\n",
    "#         x_max: Union[float, np.ndarray]\n",
    "#             The mode of the distribution (x_max).\n",
    "#         sigma_L: Union[float, np.ndarray]\n",
    "#             The standard deviation of the distribution in log space.\n",
    "#         max_L: Union[float, np.ndarray]\n",
    "#             The maximum value of L(x).\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, x_max: Union[float, np.ndarray], sigma_L: Union[float, np.ndarray], max_L: Union[float, np.ndarray]):\n",
    "#         self.x_max = x_max\n",
    "#         self.sigma_L = sigma_L\n",
    "#         self.max_L = max_L\n",
    "\n",
    "#     def to_base_triple(self) -> Tuple[Union[float, np.ndarray], Union[float, np.ndarray], Union[float, np.ndarray]]:\n",
    "#         \"\"\"Convert to the base triple (mu_L, sigma_L, scale_L).\"\"\"\n",
    "#         sigma_L = self.sigma_L\n",
    "#         mu_L = np.log(self.x_max) + sigma_L**2\n",
    "\n",
    "#         scale_L = self.max_L * self.x_max * sigma_L * np.sqrt(2 * np.pi) * np.exp(0.5 * sigma_L ** 2)\n",
    "#         return mu_L, sigma_L, scale_L\n",
    "\n",
    "# class MaxXAndMaxLGeometricSigma(LogNormalParameters):\n",
    "#     \"\"\"\n",
    "#     Represents the log-normal distribution using the mode (x_max)\n",
    "#     and maximum value (max_L) of L(x), and sigma_L.\n",
    "\n",
    "#     Parameters:\n",
    "#         x_max: Union[float, np.ndarray]\n",
    "#             The mode of the distribution (x_max).\n",
    "#         geometric_sigma_L: Union[float, np.ndarray]\n",
    "#             The geometric standard deviation of the distribution in log space.\n",
    "#         max_L: Union[float, np.ndarray]\n",
    "#             The maximum value of L(x).\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, x_max: Union[float, np.ndarray], geometric_std_dev_L: Union[float, np.ndarray], max_L: Union[float, np.ndarray]):\n",
    "#         self.x_max = x_max\n",
    "#         self.geometric_std_dev_L = geometric_std_dev_L\n",
    "#         self.max_L = max_L\n",
    "\n",
    "#     def to_base_triple(self) -> Tuple[Union[float, np.ndarray], Union[float, np.ndarray], Union[float, np.ndarray]]:\n",
    "#         \"\"\"Convert to the base triple (mu_L, sigma_L, scale_L).\"\"\"\n",
    "#         sigma_L = np.log(self.geometric_std_dev_L)\n",
    "#         mu_L = np.log(self.x_max) + sigma_L**2\n",
    "\n",
    "#         scale_L = self.max_L * self.x_max * sigma_L * np.sqrt(2 * np.pi) * np.exp(0.5 * sigma_L ** 2)\n",
    "#         return mu_L, sigma_L, scale_L\n",
    "\n",
    "# class MaxXAndMaxlGeometricSigma(LogNormalParameters):\n",
    "#     \"\"\"\n",
    "#     Represents the log-normal distribution using the mode (x_max)\n",
    "#     and maximum value (max_l) of l(x), and sigma_l.\n",
    "\n",
    "#     Parameters:\n",
    "#         x_max: Union[float, np.ndarray]\n",
    "#             The mode of the distribution (x_max).\n",
    "#         geometric_sigma_L: Union[float, np.ndarray]\n",
    "#             The geometric standard deviation of the distribution in log space.\n",
    "#         max_l: Union[float, np.ndarray]\n",
    "#             The maximum value of L(x).\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, x_max: Union[float, np.ndarray], geometric_std_dev_l: Union[float, np.ndarray], max_l: Union[float, np.ndarray]):\n",
    "#         self.x_max = x_max\n",
    "#         self.geometric_std_dev_l = geometric_std_dev_l\n",
    "#         self.max_l = max_l\n",
    "\n",
    "#     def to_base_triple(self) -> Tuple[Union[float, np.ndarray], Union[float, np.ndarray], Union[float, np.ndarray]]:\n",
    "#         \"\"\"Convert to the base triple (mu_L, sigma_L, scale_L).\"\"\"\n",
    "#         sigma_L = np.log(self.geometric_std_dev_l)\n",
    "#         mu_L = np.log(self.x_max) + sigma_L**2\n",
    "#         scale_l = self.max_l * sigma_L * np.sqrt(2 * np.pi)\n",
    "#         scale_L = scale_l * np.exp(-0.5 * sigma_L ** 2)\n",
    "#         return mu_L, sigma_L, scale_L\n",
    "\n",
    "\n",
    "# class MuSigmaScaleLinear(LogNormalParameters):\n",
    "#     \"\"\"\n",
    "#     Represents the log-normal distribution using mu_L, sigma_L, and scale_L\n",
    "#     directly, and transforms them into their respective child classes.\n",
    "\n",
    "#     Parameters:\n",
    "#         mu_L: Union[float, np.ndarray]\n",
    "#             Mean of the log-normal distribution in log space (L(x)).\n",
    "#         sigma_L: Union[float, np.ndarray]\n",
    "#             Standard deviation of the log-normal distribution in log space (L(x)).\n",
    "#         scale_L: Union[float, np.ndarray]\n",
    "#             Scale factor for the L(x) distribution.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, mu_L: Union[float, np.ndarray], sigma_L: Union[float, np.ndarray], scale_L: Union[float, np.ndarray]):\n",
    "#         self.mu_L = mu_L\n",
    "#         self.sigma_L = sigma_L\n",
    "#         self.scale_L = scale_L\n",
    "\n",
    "#     def to_base_triple(self) -> Tuple[Union[float, np.ndarray], Union[float, np.ndarray], Union[float, np.ndarray]]:\n",
    "#         \"\"\"Convert to the base triple (mu_L, sigma_L, scale_L).\"\"\"\n",
    "#         return self.mu_L, self.sigma_L, self.scale_L\n",
    "\n",
    "# class MuSigmaScaleLog(LogNormalParameters):\n",
    "#     \"\"\"\n",
    "#     Represents the log-normal distribution using mu_l, sigma_l, and scale_l\n",
    "#     directly, and transforms them into their respective child classes.\n",
    "\n",
    "#     Parameters:\n",
    "#         mu_l: Union[float, np.ndarray]\n",
    "#             Mean of the log-normal distribution in log space (l(x)).\n",
    "#         sigma_l: Union[float, np.ndarray]\n",
    "#             Standard deviation of the log-normal distribution in log space (l(x)).\n",
    "#         scale_l: Union[float, np.ndarray]\n",
    "#             Scale factor for the l(x) distribution.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, mu_l: Union[float, np.ndarray], sigma_l: Union[float, np.ndarray], scale_l: Union[float, np.ndarray]):\n",
    "#         self.mu_l = mu_l\n",
    "#         self.sigma_l = sigma_l\n",
    "#         self.scale_l = scale_l\n",
    "\n",
    "#     def to_base_triple(self) -> Tuple[Union[float, np.ndarray], Union[float, np.ndarray], Union[float, np.ndarray]]:\n",
    "#         \"\"\"Convert to the base triple (mu_L, sigma_L, scale_L).\"\"\"\n",
    "#         mu_L = self.mu_l + self.sigma_l ** 2\n",
    "#         scale_L = self.scale_l * np.exp(-0.5 * self.sigma_l ** 2)\n",
    "#         return mu_L, self.sigma_l, scale_L\n",
    "\n",
    "# # Example Usage\n",
    "\n",
    "\n",
    "# # Example Usage\n",
    "# geometric_mean_L = 1.0  # Example geometric mean\n",
    "# geometric_std_dev_L = 2  # Example geometric standard deviation\n",
    "# scale_L = 1.0  # Scale for L(x)\n",
    "\n",
    "# par = GeometricMeanSigmaL(geometric_mean_L, geometric_std_dev_L, scale_L).standardize()\n",
    "\n",
    "\n",
    "# print(GeometricMeanSigmaL(** par.get_geometric_parametersL()).standardize().summary())\n",
    "# print(MaxXAndMaxL(** par.get_x_parameters()).standardize().summary())\n",
    "# print(MuSigmaScaleLinear(** par.get_L_parameters()).standardize().summary())\n",
    "# print(MuSigmaScaleLog(** par.get_l_parameters()).standardize().summary())\n",
    "# print(MaxXAndMaxLGeometricSigma(** par.get_xmax_geometric_std_dev_maxvalue_L()).standardize().summary())\n",
    "# print(MaxXAndMaxlGeometricSigma(** par.get_xmax_geometric_std_dev_maxvalue_l()).standardize().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSDBounds:\n",
    "\n",
    "    mu1 = np.array([10e-6, 14e-6, 50e6])\n",
    "    mu2 = np.array([0.1e-6, 0.3e-6, 0.5e-3])\n",
    "    sigma1 = np.array([1.3, 1.5, 3.0])\n",
    "    sigma2 = np.array([1.2, 1.4, 2.0])\n",
    "    scale1 = np.array([1e-20, 1e13, 1e16])\n",
    "    scale2 = np.array([1e-20, 1e3, 1e10])\n",
    "\n",
    "    def get_x0_bounds(\n",
    "        mu1: np.ndarray,\n",
    "        sigma1: np.ndarray,\n",
    "        scale1: np.ndarray,\n",
    "        mu2: np.ndarray,\n",
    "        sigma2: np.ndarray,\n",
    "        scale2: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        x0 = np.array(\n",
    "            [\n",
    "                mu1[1],\n",
    "                sigma1[1],\n",
    "                scale1[1],\n",
    "                mu2[1],\n",
    "                sigma2[1],\n",
    "                scale2[1],\n",
    "            ]\n",
    "        )\n",
    "        lower_bound = np.array(\n",
    "            [\n",
    "                mu1[0],\n",
    "                sigma1[0],\n",
    "                scale1[0],\n",
    "                mu2[0],\n",
    "                sigma2[0],\n",
    "                scale2[0],\n",
    "            ]\n",
    "        )\n",
    "        upper_bound = np.array(\n",
    "            [\n",
    "                mu1[2],\n",
    "                sigma1[2],\n",
    "                scale1[2],\n",
    "                mu2[2],\n",
    "                sigma2[2],\n",
    "                scale2[2],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        bounds = Bounds(\n",
    "            lb=lower_bound,\n",
    "            ub=upper_bound,\n",
    "        )\n",
    "\n",
    "        return x0, bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds():\n",
    "        x0, bounds = PSDBounds.get_x0_bounds(\n",
    "            mu1=PSDBounds.mu1,\n",
    "            sigma1=PSDBounds.sigma1,\n",
    "            scale1=PSDBounds.scale1,\n",
    "            mu2=PSDBounds.mu2,\n",
    "            sigma2=PSDBounds.sigma2,\n",
    "            scale2=PSDBounds.scale2,\n",
    "        )\n",
    "        return bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def x0():\n",
    "        x0, bounds = PSDBounds.get_x0_bounds(\n",
    "            mu1=PSDBounds.mu1,\n",
    "            sigma1=PSDBounds.sigma1,\n",
    "            scale1=PSDBounds.scale1,\n",
    "            mu2=PSDBounds.mu2,\n",
    "            sigma2=PSDBounds.sigma2,\n",
    "            scale2=PSDBounds.scale2,\n",
    "        )\n",
    "        return x0\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds_linear():\n",
    "        sf = 1 / 2\n",
    "        x0, bounds = PSDBounds.get_x0_bounds(\n",
    "            mu1=PSDBounds.mu1 + (PSDBounds.sigma1[1] * sf) ** 2,\n",
    "            sigma1=PSDBounds.sigma1 * sf,\n",
    "            scale1=PSDBounds.scale1,\n",
    "            mu2=PSDBounds.mu2 + (PSDBounds.sigma2[1] * sf) ** 2,\n",
    "            sigma2=PSDBounds.sigma2 * sf,\n",
    "            scale2=PSDBounds.scale2,\n",
    "        )\n",
    "        return bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def x0_linear():\n",
    "        sf = 1 / 2\n",
    "        x0, bounds = PSDBounds.get_x0_bounds(\n",
    "            mu1=PSDBounds.mu1 + (PSDBounds.sigma1[1] * sf) ** 2,\n",
    "            sigma1=PSDBounds.sigma1 * sf,\n",
    "            scale1=PSDBounds.scale1,\n",
    "            mu2=PSDBounds.mu2 + (PSDBounds.sigma2[1] * sf) ** 2,\n",
    "            sigma2=PSDBounds.sigma2 * sf,\n",
    "            scale2=PSDBounds.scale2,\n",
    "        )\n",
    "        return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = 1e-2\n",
    "upper = 10\n",
    "N = 1000\n",
    "x = np.geomspace(lower, upper, N)\n",
    "x1 = np.linspace(lower, upper, N * 2)\n",
    "x2 = np.logspace(np.log10(lower), np.log10(upper), N)\n",
    "x3 = np.logspace(np.log(lower), np.log(upper), N, base=np.exp(1))\n",
    "\n",
    "params = LnParameters(3, 2, 3)\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "mode = 5.0  # Example geometric mean\n",
    "geometric_std_dev = 1.1  # Example geometric standard deviation\n",
    "scale_L = 10  # Scale for L(x)\n",
    "\n",
    "\n",
    "mmg = smodels.MaximumPointGeometricSigma(\n",
    "    x_max=mode, geometric_std_dev_l=geometric_std_dev, maximum_value=scale_L\n",
    ")\n",
    "params = mmg.standardize()\n",
    "\n",
    "\n",
    "results = dict()\n",
    "for ps in [\"direct\", \"geometric\"]:\n",
    "    results[ps] = dict()\n",
    "    for space in [\"linear\", \"ln\"]:\n",
    "        if space == \"linear\":\n",
    "            if ps == \"direct\":\n",
    "                p = params.get_L_parameters(dict_keys=(\"mu\", \"sigma\", \"scale\"))\n",
    "            elif ps == \"geometric\":\n",
    "                p = params.get_geometric_parametersL(dict_keys=(\"mu\", \"sigma\", \"scale\"))\n",
    "        elif space == \"ln\":\n",
    "            if ps == \"direct\":\n",
    "                p = params.get_l_parameters(dict_keys=(\"mu\", \"sigma\", \"scale\"))\n",
    "            elif ps == \"geometric\":\n",
    "                p = params.get_geometric_parametersl(dict_keys=(\"mu\", \"sigma\", \"scale\"))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown space: {space}\")\n",
    "\n",
    "        results[ps][space] = smodels.log_normal_distribution_all(\n",
    "            x=x,\n",
    "            parameter_space=ps,\n",
    "            x_space=space,\n",
    "            mu=p[\"mu\"],\n",
    "            sigma=p[\"sigma\"],\n",
    "            scale=p[\"scale\"],\n",
    "            density_scaled=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "ax1 = axs[0]\n",
    "ax2 = axs[1]\n",
    "\n",
    "i = 0\n",
    "for ps in [\"direct\", \"geometric\"]:\n",
    "    color = dark_colors[i]\n",
    "    for _ax in axs:\n",
    "        _ax.plot(x, results[ps][\"linear\"], label=f\"{ps} linear\", alpha=0.3, color=color, linestyle=\"-\")\n",
    "        _ax.plot(x, results[ps][\"ln\"], label=f\"{ps} cleo\", color=color, alpha=0.6, linestyle=\":\")\n",
    "    i += 1\n",
    "\n",
    "for _ax in axs:\n",
    "    _ax.axvline(params.x_max, color=\"red\", linestyle=\"--\")\n",
    "    _ax.axhline(params.maximum_value, color=\"red\", linestyle=\"--\")\n",
    "    _ax.legend()\n",
    "\n",
    "ax1.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "ax1 = axs[0]\n",
    "ax2 = axs[1]\n",
    "\n",
    "i = 0\n",
    "for ps in [\"direct\", \"geometric\"]:\n",
    "    color = dark_colors[i]\n",
    "    for _ax in axs:\n",
    "        _ax.plot(x, np.cumsum(results[ps][\"linear\"]), label=f\"{ps} linear\", color=color, linestyle=\"-\")\n",
    "        _ax.plot(x, np.cumsum(results[ps][\"ln\"]), label=f\"{ps} cleo\", color=color, linestyle=\":\")\n",
    "    i += 1\n",
    "\n",
    "for _ax in axs:\n",
    "    _ax.axvline(params[\"x_max\"], color=\"red\", linestyle=\"--\")\n",
    "    _ax.axhline(params[\"maximum_value\"], color=\"red\", linestyle=\"--\")\n",
    "    _ax.legend()\n",
    "ax1.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import observation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = RepositoryPath(\"nils_private\").get_data_dir()\n",
    "\n",
    "cloud_composite = xr.open_dataset(\n",
    "    data_dir / Path(\"observation/cloud_composite/processed/cloud_composite_SI_units_20241025.nc\"),\n",
    ")\n",
    "identified_clusters = xr.open_dataset(\n",
    "    data_dir\n",
    "    / Path(\n",
    "        \"observation/cloud_composite/processed/identified_clusters/identified_clusters_rain_mask_5.nc\"\n",
    "    )\n",
    ")\n",
    "identified_clusters = identified_clusters.swap_dims({\"time\": \"cloud_id\"})\n",
    "\n",
    "# attrs = cloud_composite[\"radius\"].attrs.copy()\n",
    "# attrs.update({\"units\": \"µm\"})\n",
    "# cloud_composite[\"radius\"] = cloud_composite[\"radius\"]\n",
    "# cloud_composite[\"radius_micro\"] = 1e6 * cloud_composite[\"radius\"]\n",
    "# cloud_composite[\"radius\"].attrs = attrs\n",
    "\n",
    "cloud_composite[\"radius2D\"] = cloud_composite[\"radius\"].expand_dims(time=cloud_composite[\"time\"])\n",
    "cloud_composite = cloud_composite.transpose(\"radius\", ...)\n",
    "\n",
    "\n",
    "# cloud_composite = cloud_composite.sel(radius=slice(10e-6, None))\n",
    "\n",
    "identified_clusters = identified_clusters.where(\n",
    "    (\n",
    "        (identified_clusters.duration.dt.seconds >= 3)\n",
    "        & (identified_clusters.altitude < 1200)\n",
    "        & (identified_clusters.altitude > 500)\n",
    "    ),\n",
    "    drop=True,\n",
    ")\n",
    "# identified_clusters = identified_clusters.isel(cloud_id=slice(0, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to coarsen the results, we need to make sure to apply the coarsening on the **NON** normalized data.\n",
    "Then we can normalized afterwards again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_radius_split = 95e-6  # 50 µm\n",
    "higher_radius_split = 5e-3  # 1 mm\n",
    "coarsen_factor = 3\n",
    "\n",
    "# lower_radius_split = 90e-6  # 50 µm\n",
    "# higher_radius_split = 0.5e-3  # 1 mm\n",
    "# coarsen_factor = 3\n",
    "\n",
    "\n",
    "coarse_composite = cloud_composite.sel(radius=slice(lower_radius_split, higher_radius_split)).copy()\n",
    "\n",
    "# make sure to have non normalized data to be coarsened\n",
    "# otherwise, the sum will not be conserved\n",
    "coarse_composite[\"particle_size_distribution\"] = (\n",
    "    coarse_composite[\"particle_size_distribution\"] * coarse_composite[\"bin_width\"]\n",
    ")\n",
    "coarse_composite[\"mass_size_distribution\"] = (\n",
    "    coarse_composite[\"mass_size_distribution\"] * coarse_composite[\"bin_width\"]\n",
    ")\n",
    "\n",
    "# use mean for radius and radius2D\n",
    "coarse_composite_radius = coarse_composite[\"radius\"].coarsen(radius=coarsen_factor).mean()\n",
    "coarse_composite_radius2D = coarse_composite[\"radius2D\"].coarsen(radius=coarsen_factor).mean()\n",
    "# use the sum for the rest\n",
    "coarse_composite = coarse_composite.coarsen(radius=coarsen_factor).sum()\n",
    "\n",
    "coarse_composite[\"radius\"] = coarse_composite_radius\n",
    "coarse_composite[\"radius2D\"] = coarse_composite_radius2D\n",
    "coarse_composite[\"diameter\"] = 2 * coarse_composite[\"radius\"]\n",
    "\n",
    "# make sure to have normalized data again\n",
    "coarse_composite[\"particle_size_distribution\"] = (\n",
    "    coarse_composite[\"particle_size_distribution\"] / coarse_composite[\"bin_width\"]\n",
    ")\n",
    "coarse_composite[\"mass_size_distribution\"] = (\n",
    "    coarse_composite[\"mass_size_distribution\"] / coarse_composite[\"bin_width\"]\n",
    ")\n",
    "\n",
    "coarse_composite[\"particle_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Number concentration\",\n",
    "    unit=cloud_composite[\"particle_size_distribution\"].attrs[\"unit\"],\n",
    ")\n",
    "coarse_composite[\"mass_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Mass concentration\",\n",
    "    unit=cloud_composite[\"mass_size_distribution\"].attrs[\"unit\"],\n",
    ")\n",
    "\n",
    "# merge the two composites with higher resoltion at small radii\n",
    "# and lower resolution at large radii\n",
    "coarse_composite = xr.merge(\n",
    "    [\n",
    "        coarse_composite.sel(radius=slice(lower_radius_split, higher_radius_split)),\n",
    "        cloud_composite.sel(radius=slice(None, lower_radius_split)),\n",
    "        cloud_composite.sel(radius=slice(higher_radius_split, None)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Test liquid water content is conserved\n",
    "np.testing.assert_allclose(\n",
    "    (coarse_composite[\"bin_width\"] * coarse_composite[\"mass_size_distribution\"]).sum(\"radius\"),\n",
    "    (cloud_composite[\"bin_width\"] * cloud_composite[\"mass_size_distribution\"]).sum(\"radius\"),\n",
    "    rtol=0.001,\n",
    ")\n",
    "# Test particle concentration is conserved\n",
    "np.testing.assert_allclose(\n",
    "    (coarse_composite[\"bin_width\"] * coarse_composite[\"particle_size_distribution\"]).sum(\"radius\"),\n",
    "    (cloud_composite[\"bin_width\"] * cloud_composite[\"particle_size_distribution\"]).sum(\"radius\"),\n",
    "    rtol=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundaries for the fitting parameters\n",
    "\n",
    "Please note, that the value of the fitting parameter are very very sensible!!!\n",
    "\n",
    "They need to be adjusted with a lot of effort to make the fit work well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better bounds are:\n",
    "````python\n",
    "class PSDBounds:\n",
    "    _x0 = np.array([14e-6, 1.5, 1e13, 300e-6, 1.5, 1e3])\n",
    "    _bounds = Bounds(\n",
    "        # mu1, sig1, sc1, mu2, sig2, sc2\n",
    "        lb=[1e-6, 1.3, 1e-20, 100e-6, 1.3, 1e-20],\n",
    "        ub=[50e6, 3.0, 1e16, 0.5e-3, 3.0, 1e10],\n",
    "        # keep_feasible = [True, True, True, False, True, True]\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds():\n",
    "        return PSDBounds._bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def x0():\n",
    "        return PSDBounds._x0\n",
    "\n",
    "\n",
    "class MSDBounds:\n",
    "    _x0 = np.array([14e-6, 1.5, 1e-1, 300e-6, 2, 1e-4])\n",
    "    _bounds = Bounds(\n",
    "        lb=[1e-6, 1.3, 1e-20, 100e-6, 1.3, 1e-20],\n",
    "        ub=[50e-6, 3.5, 1e2, 0.5e-3, 3.0, 1e1],\n",
    "        # keep_feasible = [True, True, True, False, True, True]\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds():\n",
    "        return MSDBounds._bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def x0():\n",
    "        return MSDBounds._x0\n",
    "\n",
    "````\n",
    "\n",
    "Good bound are:\n",
    "````python\n",
    "class PSDBounds():\n",
    "    _x0 = np.array([3e-6, 1.5, 1e13, 300e-6, 1.5, 1e3])\n",
    "    _bounds = Bounds(\n",
    "        # mu1, sig1, sc1, mu2, sig2, sc2\n",
    "        lb=[1e-6, 1.3, 1e-20, 100e-6, 1.3, 1e-20],\n",
    "        ub=[50e6, 3.0, 1e16, 0.5e-3, 3.0, 1e10],\n",
    "        # keep_feasible = [True, True, True, False, True, True]\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds():\n",
    "        return PSDBounds._bounds\n",
    "    \n",
    "    @staticmethod\n",
    "    def x0():\n",
    "        return PSDBounds._x0\n",
    "    \n",
    "\n",
    "class MSDBounds():\n",
    "    _x0 = np.array([3e-6, 1.5, 1e-1, 300e-6, 2, 1e-4])\n",
    "    _bounds = Bounds(\n",
    "        lb=[1e-6, 1.3, 1e-20, 100e-6, 1.3, 1e-20],\n",
    "        ub=[50e-6, 3.5, 1e2, 0.5e-3, 3.0, 1e1],\n",
    "        # keep_feasible = [True, True, True, False, True, True]\n",
    "    ) \n",
    "\n",
    "    @staticmethod\n",
    "    def bounds():\n",
    "        return MSDBounds._bounds\n",
    "        \n",
    "    @staticmethod\n",
    "    def x0():\n",
    "        return MSDBounds._x0\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSDBounds:\n",
    "\n",
    "    mu1 = np.array([1e-6, 14e-6, 50e6])\n",
    "    mu2 = np.array([0.1e-3, 0.3e-3, 0.5e-3])\n",
    "    sigma1 = np.array([1.3, 1.5, 3.0])\n",
    "    sigma2 = np.array([1.2, 1.4, 2.0])\n",
    "    scale1 = np.array([1e-20, 1e13, 1e16])\n",
    "    scale2 = np.array([1e-20, 1e3, 1e10])\n",
    "\n",
    "    def get_x0_bounds(\n",
    "        mu1: np.ndarray,\n",
    "        sigma1: np.ndarray,\n",
    "        scale1: np.ndarray,\n",
    "        mu2: np.ndarray,\n",
    "        sigma2: np.ndarray,\n",
    "        scale2: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        x0 = np.array(\n",
    "            [\n",
    "                mu1[1],\n",
    "                sigma1[1],\n",
    "                scale1[1],\n",
    "                mu2[1],\n",
    "                sigma2[1],\n",
    "                scale2[1],\n",
    "            ]\n",
    "        )\n",
    "        lower_bound = np.array(\n",
    "            [\n",
    "                mu1[0],\n",
    "                sigma1[0],\n",
    "                scale1[0],\n",
    "                mu2[0],\n",
    "                sigma2[0],\n",
    "                scale2[0],\n",
    "            ]\n",
    "        )\n",
    "        upper_bound = np.array(\n",
    "            [\n",
    "                mu1[2],\n",
    "                sigma1[2],\n",
    "                scale1[2],\n",
    "                mu2[2],\n",
    "                sigma2[2],\n",
    "                scale2[2],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        bounds = Bounds(\n",
    "            lb=lower_bound,\n",
    "            ub=upper_bound,\n",
    "        )\n",
    "\n",
    "        return x0, bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds():\n",
    "        x0, bounds = PSDBounds.get_x0_bounds(\n",
    "            mu1=PSDBounds.mu1,\n",
    "            sigma1=PSDBounds.sigma1,\n",
    "            scale1=PSDBounds.scale1,\n",
    "            mu2=PSDBounds.mu2,\n",
    "            sigma2=PSDBounds.sigma2,\n",
    "            scale2=PSDBounds.scale2,\n",
    "        )\n",
    "        return bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def x0():\n",
    "        x0, bounds = PSDBounds.get_x0_bounds(\n",
    "            mu1=PSDBounds.mu1,\n",
    "            sigma1=PSDBounds.sigma1,\n",
    "            scale1=PSDBounds.scale1,\n",
    "            mu2=PSDBounds.mu2,\n",
    "            sigma2=PSDBounds.sigma2,\n",
    "            scale2=PSDBounds.scale2,\n",
    "        )\n",
    "        return x0\n",
    "\n",
    "\n",
    "class MSDBounds:\n",
    "    _x0 = np.array([14e-6, 1.5, 1e-1, 300e-6, 1.4, 1e-4])\n",
    "    _bounds = Bounds(\n",
    "        lb=[1e-6, 1.3, 1e-20, 100e-6, 1.2, 1e-20],\n",
    "        ub=[50e-6, 3.5, 1e2, 0.5e-3, 2.0, 1e1],\n",
    "        # keep_feasible = [True, True, True, False, True, True]\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds():\n",
    "        return MSDBounds._bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def x0():\n",
    "        return MSDBounds._x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSDBounds:\n",
    "\n",
    "    mu1 = np.array([1e-6, 14e-6, 50e6])\n",
    "    mu2 = np.array([0.1e-3, 0.3e-3, 0.5e-3])\n",
    "    sigma1 = np.array([1.5, 1.6, 3.0])\n",
    "    sigma2 = np.array([1.4, 1.5, 2.0])\n",
    "    scale1 = np.array([1e-20, 1e13, 1e16])\n",
    "    scale2 = np.array([1e-20, 1e3, 1e10])\n",
    "\n",
    "    def get_x0_bounds(\n",
    "        mu1: np.ndarray,\n",
    "        sigma1: np.ndarray,\n",
    "        scale1: np.ndarray,\n",
    "        mu2: np.ndarray,\n",
    "        sigma2: np.ndarray,\n",
    "        scale2: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        x0 = np.array(\n",
    "            [\n",
    "                mu1[1],\n",
    "                sigma1[1],\n",
    "                scale1[1],\n",
    "                mu2[1],\n",
    "                sigma2[1],\n",
    "                scale2[1],\n",
    "            ]\n",
    "        )\n",
    "        lower_bound = np.array(\n",
    "            [\n",
    "                mu1[0],\n",
    "                sigma1[0],\n",
    "                scale1[0],\n",
    "                mu2[0],\n",
    "                sigma2[0],\n",
    "                scale2[0],\n",
    "            ]\n",
    "        )\n",
    "        upper_bound = np.array(\n",
    "            [\n",
    "                mu1[2],\n",
    "                sigma1[2],\n",
    "                scale1[2],\n",
    "                mu2[2],\n",
    "                sigma2[2],\n",
    "                scale2[2],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        bounds = Bounds(\n",
    "            lb=lower_bound,\n",
    "            ub=upper_bound,\n",
    "        )\n",
    "\n",
    "        return x0, bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds():\n",
    "        x0, bounds = PSDBounds.get_x0_bounds(\n",
    "            mu1=PSDBounds.mu1,\n",
    "            sigma1=PSDBounds.sigma1,\n",
    "            scale1=PSDBounds.scale1,\n",
    "            mu2=PSDBounds.mu2,\n",
    "            sigma2=PSDBounds.sigma2,\n",
    "            scale2=PSDBounds.scale2,\n",
    "        )\n",
    "        return bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def x0():\n",
    "        x0, bounds = PSDBounds.get_x0_bounds(\n",
    "            mu1=PSDBounds.mu1,\n",
    "            sigma1=PSDBounds.sigma1,\n",
    "            scale1=PSDBounds.scale1,\n",
    "            mu2=PSDBounds.mu2,\n",
    "            sigma2=PSDBounds.sigma2,\n",
    "            scale2=PSDBounds.scale2,\n",
    "        )\n",
    "        return x0\n",
    "\n",
    "\n",
    "class MSDBounds:\n",
    "    _x0 = np.array([14e-6, 1.5, 1e-1, 300e-6, 1.4, 1e-4])\n",
    "    _bounds = Bounds(\n",
    "        lb=[1e-6, 1.3, 1e-20, 100e-6, 1.2, 1e-20],\n",
    "        ub=[50e-6, 3.5, 1e2, 0.5e-3, 2.0, 1e1],\n",
    "        # keep_feasible = [True, True, True, False, True, True]\n",
    "    )\n",
    "\n",
    "    @staticmethod\n",
    "    def bounds():\n",
    "        return MSDBounds._bounds\n",
    "\n",
    "    @staticmethod\n",
    "    def x0():\n",
    "        return MSDBounds._x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PSDBounds:\n",
    "#     _x0 = np.array([14e-6, 1.5, 1e13, 300e-6, 1.4, 1e3])\n",
    "#     _bounds = Bounds(\n",
    "#         # mu1, sig1, sc1, mu2, sig2, sc2\n",
    "#         lb=[1e-6, 1.3, 1e-20, 100e-6, 1.2, 1e-20],\n",
    "#         ub=[50e6, 3.0, 1e16, 0.5e-3, 2.0, 1e10],\n",
    "#         # keep_feasible = [True, True, True, False, True, True]\n",
    "#     )\n",
    "\n",
    "#     @staticmethod\n",
    "#     def bounds():\n",
    "#         return PSDBounds._bounds\n",
    "\n",
    "#     @staticmethod\n",
    "#     def x0():\n",
    "#         return PSDBounds._x0\n",
    "\n",
    "\n",
    "# class MSDBounds:\n",
    "#     _x0 = np.array([14e-6, 1.5, 1e-1, 300e-6, 1.4, 1e-4])\n",
    "#     _bounds = Bounds(\n",
    "#         lb=[1e-6, 1.3, 1e-20, 100e-6, 1.2, 1e-20],\n",
    "#         ub=[50e-6, 3.5, 1e2, 0.5e-3, 2.0, 1e1],\n",
    "#         # keep_feasible = [True, True, True, False, True, True]\n",
    "#     )\n",
    "\n",
    "#     @staticmethod\n",
    "#     def bounds():\n",
    "#         return MSDBounds._bounds\n",
    "\n",
    "#     @staticmethod\n",
    "#     def x0():\n",
    "#         return MSDBounds._x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chose the correct representation of the log normal disribution.\n",
    "\n",
    "Because there are multiple functions which are used as log normal distribution, the correct usage and transformation is crucial!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_DataArray(d: Dict, new_coords: Dict) -> xr.Dataset:\n",
    "    new_dims = list(new_coords.keys())\n",
    "\n",
    "    d_new = {}\n",
    "    for key in d:\n",
    "        value = d[key]\n",
    "        if isinstance(value, (np.ndarray, list, xr.DataArray)):\n",
    "            d_new[key] = xr.DataArray(\n",
    "                value,\n",
    "                coords=new_coords,\n",
    "            )\n",
    "        else:\n",
    "            d_new[key] = xr.DataArray(\n",
    "                value,\n",
    "                coords=new_coords,\n",
    "                dims=new_dims,\n",
    "            )\n",
    "\n",
    "    result = xr.Dataset(\n",
    "        # coords= new_coords,\n",
    "        data_vars=d_new,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different approaches:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:12<00:00, 12.29it/s]\n"
     ]
    }
   ],
   "source": [
    "list_lwc = []\n",
    "list_lwc_sem = []\n",
    "list_lwc_50um = []\n",
    "list_lwc_50um_sem = []\n",
    "\n",
    "list_nbc = []\n",
    "list_nbc_sem = []\n",
    "list_nbc_50um = []\n",
    "list_nbc_50um_sem = []\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"]):\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    lwc = cc[\"liquid_water_content\"]\n",
    "\n",
    "    lwc_mean, lwc_sem = mean_and_stderror_of_mean(lwc, dims=(\"time\",))\n",
    "    list_lwc.append(lwc_mean)\n",
    "    list_lwc_sem.append(lwc_sem)\n",
    "\n",
    "    lwc_above_50um = (\n",
    "        (cc[\"mass_size_distribution\"] * cc[\"bin_width\"]).sel(radius=slice(50e-6, None)).sum(\"radius\")\n",
    "    )\n",
    "    lwc_above_50um_mean, lwc_above_50um_sem = mean_and_stderror_of_mean(lwc_above_50um, dims=(\"time\",))\n",
    "    list_lwc_50um.append(lwc_above_50um_mean)\n",
    "    list_lwc_50um_sem.append(lwc_above_50um_sem)\n",
    "\n",
    "    nbc = (cc[\"particle_size_distribution\"] * cc[\"bin_width\"]).sum(\"radius\")\n",
    "\n",
    "    nbc_mean, nbc_sem = mean_and_stderror_of_mean(nbc, dims=(\"time\",))\n",
    "    list_nbc.append(nbc_mean)\n",
    "    list_nbc_sem.append(nbc_sem)\n",
    "\n",
    "    nbc_above_50um = (\n",
    "        (cc[\"particle_size_distribution\"] * cc[\"bin_width\"]).sel(radius=slice(50e-6, None)).sum(\"radius\")\n",
    "    )\n",
    "    nbc_above_50um_mean, nbc_above_50um_sem = mean_and_stderror_of_mean(nbc_above_50um, dims=(\"time\",))\n",
    "    list_nbc_50um.append(nbc_above_50um_mean)\n",
    "    list_nbc_50um_sem.append(nbc_above_50um_sem)\n",
    "\n",
    "\n",
    "da_lwc = xr.concat(\n",
    "    list_lwc,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_lwc.attrs = dict(\n",
    "    long_name=\"Liquid water content\",\n",
    "    units=\"g m^{-3}\",\n",
    ")\n",
    "\n",
    "da_lwc_sem = xr.concat(\n",
    "    list_lwc_sem,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_lwc_sem.attrs = dict(\n",
    "    long_name=\"Standard error of the mean of the liquid water content\",\n",
    "    units=\"g m^{-3}\",\n",
    ")\n",
    "\n",
    "da_lwc_50um = xr.concat(\n",
    "    list_lwc_50um,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_lwc_50um.attrs = dict(\n",
    "    long_name=\"Liquid water content above 50 µm\",\n",
    "    units=\"g m^{-3}\",\n",
    ")\n",
    "\n",
    "da_lwc_50um_sem = xr.concat(\n",
    "    list_lwc_50um_sem,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_lwc_50um_sem.attrs = dict(\n",
    "    long_name=\"Standard error of the mean of the liquid water content above 50 µm\",\n",
    "    units=\"g m^{-3}\",\n",
    ")\n",
    "\n",
    "da_nbc = xr.concat(\n",
    "    list_nbc,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_nbc.attrs = dict(\n",
    "    long_name=\"Number concentration\",\n",
    "    units=\"m^{-3}\",\n",
    ")\n",
    "\n",
    "da_nbc_sem = xr.concat(\n",
    "    list_nbc_sem,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_nbc_sem.attrs = dict(\n",
    "    long_name=\"Standard error of the mean of the number concentration\",\n",
    "    units=\"m^{-3}\",\n",
    ")\n",
    "\n",
    "da_nbc_50um = xr.concat(\n",
    "    list_nbc_50um,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_nbc_50um.attrs = dict(\n",
    "    long_name=\"Number concentration above 50 µm\",\n",
    "    units=\"m^{-3}\",\n",
    ")\n",
    "\n",
    "da_nbc_50um_sem = xr.concat(\n",
    "    list_nbc_50um_sem,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_nbc_50um_sem.attrs = dict(\n",
    "    long_name=\"Standard error of the mean of the number concentration above 50 µm\",\n",
    "    units=\"m^{-3}\",\n",
    ")\n",
    "\n",
    "\n",
    "ds_observations = xr.Dataset(\n",
    "    dict(\n",
    "        liquid_water_content=da_lwc,\n",
    "        liquid_water_content_sem=da_lwc_sem,\n",
    "        liquid_water_content_50um=da_lwc_50um,\n",
    "        liquid_water_content_50um_sem=da_lwc_50um_sem,\n",
    "        particle_size_distribution=da_nbc,\n",
    "        particle_size_distribution_sem=da_nbc_sem,\n",
    "        particle_size_distribution_50um=da_nbc_50um,\n",
    "        particle_size_distribution_50um_sem=da_nbc_50um_sem,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit all clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = 1e-6\n",
    "# end = 1.5e-3\n",
    "# r = np.geomspace(start, end, 10000)\n",
    "# t_test = xr.DataArray(data=r, coords={\"radius\": r}, dims=[\"radius\"])\n",
    "# w_test = 0.5 * (t_test - t_test.shift(radius=2)).shift(radius=-1)\n",
    "# t_test = t_test.isel(radius=slice(1, -1))\n",
    "# w_test = w_test.isel(radius=slice(1, -1))\n",
    "\n",
    "r = np.geomspace(0.1e-6, 3e-3, 100)\n",
    "t_test = xr.DataArray(data=r, coords={\"radius\": r}, dims=[\"radius\"])\n",
    "w_test = (t_test - t_test.shift(radius=2)).shift(radius=-1)\n",
    "w_test = w_test.interpolate_na(\"radius\", method=\"linear\", fill_value=\"extrapolate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit in number concentration space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sdm_eurec4a.input_processing.models' from 'C:\\\\Users\\\\Niebaum\\\\Documents\\\\Repositories\\\\sdm-eurec4a\\\\src\\\\sdm_eurec4a\\\\input_processing\\\\models.py'>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(smodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:11<00:00, 13.43it/s]\n"
     ]
    }
   ],
   "source": [
    "particle_size_distribution_parameters = []\n",
    "fitted_data = []\n",
    "\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"].values):\n",
    "\n",
    "    # extract the cloud composite data for the cloud\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    # use the particle size distribution and the radius\n",
    "    psd = cc[\"particle_size_distribution\"]  # .sel(radius = slice(15e-6, None))\n",
    "    radius = cc[\"radius\"].expand_dims(time=cc[\"time\"])  # .sel(radius = slice(15e-6, None))\n",
    "\n",
    "    # make sure to have the same order of dimensions\n",
    "    psd = psd.transpose(\"time\", \"radius\")\n",
    "    radius = radius.transpose(\"time\", \"radius\")\n",
    "\n",
    "    # fit the double log-normal distribution\n",
    "    double_ln = smodels.CleoDoubleLnNormalFit(\n",
    "        name=\"PSD\",\n",
    "        x0=PSDBounds.x0(),\n",
    "        bounds=PSDBounds.bounds(),\n",
    "        t_train=radius.mean(\"time\", skipna=True),\n",
    "        y_train=psd.mean(\"time\", skipna=True),\n",
    "        fit_kwargs=dict(\n",
    "            loss=\"linear\",\n",
    "            kwargs=dict(\n",
    "                # parameter_space= 'geometric',\n",
    "                # x_space = 'linear',\n",
    "                # density_scaled = False,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    double_ln.fit(3)\n",
    "\n",
    "    # save the parameters\n",
    "    particle_size_distribution_parameters.append(\n",
    "        dict_to_DataArray(\n",
    "            d=double_ln.parameters,\n",
    "            new_coords=dict(\n",
    "                cloud_id=[\n",
    "                    cloud_id,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # predict the number concentration\n",
    "    dimension, prediction = double_ln.predict(t_test)\n",
    "\n",
    "    prediction = prediction.expand_dims(cloud_id=(cloud_id,))\n",
    "\n",
    "    # # we can also use only the radii where we have data:\n",
    "    # radii_measured = cc[\"particle_size_distribution\"].mean(\"time\") > 0\n",
    "    # radius_end = cc[\"radius\"].sel(radius=radii_measured).max().values\n",
    "    # radius_start = cc[\"radius\"].sel(radius=radii_measured).min().values\n",
    "    # radius_end = 1.5 * radius_end\n",
    "    # prediction = prediction.where(dimension >= radius_start, other = np.nan)\n",
    "    # prediction = prediction.where(dimension <= radius_end, other = np.nan)\n",
    "\n",
    "    fitted_data.append(prediction)\n",
    "\n",
    "\n",
    "# create a data array with the fitted number concentration\n",
    "particle_size_distribution_fitted_data = xr.concat(\n",
    "    fitted_data,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "particle_size_distribution_parameters = xr.concat(\n",
    "    particle_size_distribution_parameters,\n",
    "    dim=\"cloud_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = \"\"\n",
    "details += \"Parameters of the double log-normal distribution fitted to the particle size distribution.\\n\"\n",
    "details += \"The fitting function was set in the CleoDoubleLnNormalFit class.\\n\"\n",
    "details += \"No weighting used in the cost function.\\n\"\n",
    "details += \"A coarsened input data was used.\\n\"\n",
    "details += \"The loss function was set to 'linear'.\\n\"\n",
    "\n",
    "particle_size_distribution_parameters.attrs = {\n",
    "    \"description\": \"parameters of a double log-normal distribution fitted to the particle size distribution\",\n",
    "    \"details\": details,\n",
    "    \"creation_time\": datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"author\": \"Nils Niebaum\",\n",
    "    \"email\": \"nils-ole.niebaum@mpimet.mpg.de\",\n",
    "    \"institution\": \"Max Planck Institute for Meteorology\",\n",
    "    \"github_repository\": \"https://github.com/nilsnevertree/sdm-eurec4a\",\n",
    "    \"git_commit\": get_git_revision_hash(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit with weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Square $^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:14<00:00, 10.93it/s]\n"
     ]
    }
   ],
   "source": [
    "weighted2_particle_size_distribution_parameters = []\n",
    "fitted_data = []\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"].data):\n",
    "\n",
    "    # extract the cloud composite data for the cloud\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    # use the particle size distribution and the radius\n",
    "    psd = cc[\"particle_size_distribution\"]  # .sel(radius = slice(15e-6, None))\n",
    "    radius = cc[\"radius\"].expand_dims(time=cc[\"time\"])  # .sel(radius = slice(15e-6, None))\n",
    "\n",
    "    # make sure to have the same order of dimensions\n",
    "    psd = psd.transpose(\"time\", \"radius\")\n",
    "    radius = radius.transpose(\"time\", \"radius\")\n",
    "\n",
    "    # fit the double log-normal distribution\n",
    "    double_ln = smodels.CleoDoubleLnNormalFit(\n",
    "        name=\"PSD weighted\",\n",
    "        x0=PSDBounds.x0(),\n",
    "        bounds=PSDBounds.bounds(),\n",
    "        t_train=radius.mean(\"time\", skipna=True),\n",
    "        y_train=psd.mean(\"time\", skipna=True),\n",
    "        fit_kwargs=dict(\n",
    "            loss=\"linear\",\n",
    "            kwargs=dict(\n",
    "                # parameter_space= 'geometric',\n",
    "                # x_space = 'linear',\n",
    "                # density_scaled = False,\n",
    "            ),\n",
    "        ),\n",
    "        t_weight_power=2,\n",
    "    )\n",
    "    double_ln.fit(3)\n",
    "\n",
    "    # save the parameters\n",
    "    weighted2_particle_size_distribution_parameters.append(\n",
    "        dict_to_DataArray(\n",
    "            d=double_ln.parameters,\n",
    "            new_coords=dict(\n",
    "                cloud_id=[\n",
    "                    cloud_id,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # predict the number concentration\n",
    "    dimension, prediction = double_ln.predict(t_test)\n",
    "\n",
    "    prediction = prediction.expand_dims(cloud_id=(cloud_id,))\n",
    "\n",
    "    # # we can also use only the radii where we have data:\n",
    "    # radii_measured = cc[\"particle_size_distribution\"].mean(\"time\") > 0\n",
    "    # radius_end = cc[\"radius\"].sel(radius=radii_measured).max().values\n",
    "    # radius_start = cc[\"radius\"].sel(radius=radii_measured).min().values\n",
    "    # radius_end = 1.5 * radius_end\n",
    "    # prediction = prediction.where(dimension >= radius_start, other = np.nan)\n",
    "    # prediction = prediction.where(dimension <= radius_end, other = np.nan)\n",
    "\n",
    "    fitted_data.append(prediction)\n",
    "\n",
    "\n",
    "# create a data array with the fitted number concentration\n",
    "weighted2_particle_size_distribution_fitted_data = xr.concat(\n",
    "    fitted_data,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "weighted2_particle_size_distribution_parameters = xr.concat(\n",
    "    weighted2_particle_size_distribution_parameters,\n",
    "    dim=\"cloud_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = \"\"\n",
    "details += \"Parameters of the double log-normal distribution fitted to the particle size distribution.\\n\"\n",
    "details += \"The fitting function was set in the CleoDoubleLnNormalFit class.\\n\"\n",
    "details += \"Weighting by the radius to power of 2 (surface) used in the cost function.\\n\"\n",
    "details += \"A coarsened input data was used.\\n\"\n",
    "details += \"The loss function was set to 'linear'.\\n\"\n",
    "\n",
    "weighted2_particle_size_distribution_parameters.attrs = {\n",
    "    \"description\": \"parameters of a double log-normal distribution fitted to the particle size distribution\",\n",
    "    \"details\": details,\n",
    "    \"creation_time\": datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"author\": \"Nils Niebaum\",\n",
    "    \"email\": \"nils-ole.niebaum@mpimet.mpg.de\",\n",
    "    \"institution\": \"Max Planck Institute for Meteorology\",\n",
    "    \"github_repository\": \"https://github.com/nilsnevertree/sdm-eurec4a\",\n",
    "    \"git_commit\": get_git_revision_hash(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cubic $^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/154 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:10<00:00, 15.32it/s]\n"
     ]
    }
   ],
   "source": [
    "weighted3_particle_size_distribution_parameters = []\n",
    "fitted_data = []\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"].data):\n",
    "\n",
    "    # extract the cloud composite data for the cloud\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    # use the particle size distribution and the radius\n",
    "    psd = cc[\"particle_size_distribution\"]  # .sel(radius = slice(15e-6, None))\n",
    "    radius = cc[\"radius\"].expand_dims(time=cc[\"time\"])  # .sel(radius = slice(15e-6, None))\n",
    "\n",
    "    # make sure to have the same order of dimensions\n",
    "    psd = psd.transpose(\"time\", \"radius\")\n",
    "    radius = radius.transpose(\"time\", \"radius\")\n",
    "\n",
    "    # fit the double log-normal distribution\n",
    "    double_ln = smodels.CleoDoubleLnNormalFit(\n",
    "        name=\"PSD weighted\",\n",
    "        x0=PSDBounds.x0(),\n",
    "        bounds=PSDBounds.bounds(),\n",
    "        t_train=radius.mean(\"time\", skipna=True),\n",
    "        y_train=psd.mean(\"time\", skipna=True),\n",
    "        fit_kwargs=dict(\n",
    "            loss=\"linear\",\n",
    "            kwargs=dict(\n",
    "                # parameter_space= 'geometric',\n",
    "                # x_space = 'linear',\n",
    "                # density_scaled = False,\n",
    "            ),\n",
    "        ),\n",
    "        t_weight_power=3,\n",
    "    )\n",
    "    double_ln.fit(3)\n",
    "\n",
    "    # save the parameters\n",
    "    weighted3_particle_size_distribution_parameters.append(\n",
    "        dict_to_DataArray(\n",
    "            d=double_ln.parameters,\n",
    "            new_coords=dict(\n",
    "                cloud_id=[\n",
    "                    cloud_id,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # predict the number concentration\n",
    "    dimension, prediction = double_ln.predict(t_test)\n",
    "\n",
    "    prediction = prediction.expand_dims(cloud_id=(cloud_id,))\n",
    "\n",
    "    # # we can also use only the radii where we have data:\n",
    "    # radii_measured = cc[\"particle_size_distribution\"].mean(\"time\") > 0\n",
    "    # radius_end = cc[\"radius\"].sel(radius=radii_measured).max().values\n",
    "    # radius_start = cc[\"radius\"].sel(radius=radii_measured).min().values\n",
    "    # radius_end = 1.5 * radius_end\n",
    "    # prediction = prediction.where(dimension >= radius_start, other = np.nan)\n",
    "    # prediction = prediction.where(dimension <= radius_end, other = np.nan)\n",
    "\n",
    "    fitted_data.append(prediction)\n",
    "\n",
    "\n",
    "# create a data array with the fitted number concentration\n",
    "weighted3_particle_size_distribution_fitted_data = xr.concat(\n",
    "    fitted_data,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "weighted3_particle_size_distribution_parameters = xr.concat(\n",
    "    weighted3_particle_size_distribution_parameters,\n",
    "    dim=\"cloud_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = \"\"\n",
    "details += \"Parameters of the double log-normal distribution fitted to the particle size distribution.\\n\"\n",
    "details += \"The fitting function was set in the CleoDoubleLnNormalFit class.\\n\"\n",
    "details += \"Weighting by the radius to power of 3 (volume) used in the cost function.\\n\"\n",
    "details += \"A coarsened input data was used.\\n\"\n",
    "details += \"The loss function was set to 'linear'.\\n\"\n",
    "\n",
    "weighted3_particle_size_distribution_parameters.attrs = {\n",
    "    \"description\": \"parameters of a double log-normal distribution fitted to the particle size distribution\",\n",
    "    \"details\": details,\n",
    "    \"creation_time\": datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"author\": \"Nils Niebaum\",\n",
    "    \"email\": \"nils-ole.niebaum@mpimet.mpg.de\",\n",
    "    \"institution\": \"Max Planck Institute for Meteorology\",\n",
    "    \"github_repository\": \"https://github.com/nilsnevertree/sdm-eurec4a\",\n",
    "    \"git_commit\": get_git_revision_hash(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit in mass concentration space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:13<00:00, 11.76it/s]\n"
     ]
    }
   ],
   "source": [
    "mass_size_distribution_parameters = []\n",
    "fitted_data = []\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"]):\n",
    "\n",
    "    # extract the cloud composite data for the cloud\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    # use the mass size distribution and the radius\n",
    "    msd = cc[\"mass_size_distribution\"]  # .sel(radius = slice(15e-6, None))\n",
    "    radius = cc[\"radius\"].expand_dims(time=cc[\"time\"])  # .sel(radius = slice(15e-6, None))\n",
    "\n",
    "    # make sure to have the same order of dimensions\n",
    "    msd = msd.transpose(\"time\", \"radius\")\n",
    "    radius = radius.transpose(\"time\", \"radius\")\n",
    "\n",
    "    # fit the double log-normal distribution\n",
    "\n",
    "    double_ln = smodels.CleoDoubleLnNormalFit(\n",
    "        name=\"MSD\",\n",
    "        x0=MSDBounds.x0(),\n",
    "        bounds=MSDBounds.bounds(),\n",
    "        t_train=radius.mean(\"time\", skipna=True),\n",
    "        y_train=msd.mean(\"time\", skipna=True),\n",
    "        fit_kwargs=dict(\n",
    "            loss=\"linear\",\n",
    "            kwargs=dict(\n",
    "                # parameter_space= 'geometric',\n",
    "                # x_space = 'ln',\n",
    "                # density_scaled = False,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    double_ln.fit(3)\n",
    "\n",
    "    # save the parameters\n",
    "    mass_size_distribution_parameters.append(\n",
    "        dict_to_DataArray(\n",
    "            d=double_ln.parameters,\n",
    "            new_coords=dict(\n",
    "                cloud_id=[\n",
    "                    cloud_id,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # predict the number concentration\n",
    "    dimension, prediction = double_ln.predict(t_test)\n",
    "\n",
    "    prediction = prediction.expand_dims(cloud_id=(cloud_id,))\n",
    "\n",
    "    # # we can also use only the radii where we have data:\n",
    "    # radii_measured = cc[\"particle_size_distribution\"].mean(\"time\") > 0\n",
    "    # radius_end = cc[\"radius\"].sel(radius=radii_measured).max().values\n",
    "    # radius_start = cc[\"radius\"].sel(radius=radii_measured).min().values\n",
    "    # radius_end = 1.5 * radius_end\n",
    "    # prediction = prediction.where(dimension >= radius_start, other = np.nan)\n",
    "    # prediction = prediction.where(dimension <= radius_end, other = np.nan)\n",
    "\n",
    "    fitted_data.append(prediction)\n",
    "\n",
    "\n",
    "# create a data array with the fitted number concentration\n",
    "mass_size_distribution_fitted_data = xr.concat(\n",
    "    fitted_data,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "mass_size_distribution_parameters = xr.concat(\n",
    "    mass_size_distribution_parameters,\n",
    "    dim=\"cloud_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = \"\"\n",
    "details += \"Parameters of the double log-normal distribution fitted to the mass size distribution.\\n\"\n",
    "details += \"The fitting function was set in the CleoDoubleLnNormalFit class.\\n\"\n",
    "details += \"No weighting used in the cost function.\\n\"\n",
    "details += \"A coarsened input data was used.\\n\"\n",
    "details += \"The loss function was set to 'linear'.\\n\"\n",
    "\n",
    "mass_size_distribution_parameters.attrs = {\n",
    "    \"description\": \"parameters of a double log-normal distribution fitted to the particle size distribution\",\n",
    "    \"details\": details,\n",
    "    \"creation_time\": datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"author\": \"Nils Niebaum\",\n",
    "    \"email\": \"nils-ole.niebaum@mpimet.mpg.de\",\n",
    "    \"institution\": \"Max Planck Institute for Meteorology\",\n",
    "    \"github_repository\": \"https://github.com/nilsnevertree/sdm-eurec4a\",\n",
    "    \"git_commit\": get_git_revision_hash(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the data into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the datasets\n",
    "dataset_fitted = xr.Dataset(\n",
    "    dict(\n",
    "        particle_size_distribution=particle_size_distribution_fitted_data,\n",
    "        weighted3_particle_size_distribution=weighted3_particle_size_distribution_fitted_data,\n",
    "        weighted2_particle_size_distribution=weighted2_particle_size_distribution_fitted_data,\n",
    "        mass_size_distribution=mass_size_distribution_fitted_data,\n",
    "    )\n",
    ")\n",
    "dataset_fitted[\"bin_width\"] = w_test\n",
    "dataset_fitted[\"bin_width\"].attrs = dict(\n",
    "    long_name=\"Bin width\",\n",
    "    unit=\"m\",\n",
    ")\n",
    "\n",
    "# Fit to the number concentration\n",
    "dataset_fitted[\"particle_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Number concentration\", unit=\"m^{-3} m^{-1}\", comment=\"Fit to the number concentration\"\n",
    ")\n",
    "\n",
    "dataset_fitted[\"mass_size_distribution_from_nc\"] = (\n",
    "    msd_from_psd_dataarray(\n",
    "        da=dataset_fitted[\"particle_size_distribution\"] * dataset_fitted[\"bin_width\"],\n",
    "    )\n",
    "    / dataset_fitted[\"bin_width\"]\n",
    ")\n",
    "dataset_fitted[\"mass_size_distribution_from_nc\"].attrs = dict(\n",
    "    long_name=\"Mass concentration from number concentration\",\n",
    "    unit=\"kg m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the number concentration\",\n",
    ")\n",
    "\n",
    "# Fit to the number concentration with weight\n",
    "dataset_fitted[\"weighted3_particle_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Weighted number concentration\",\n",
    "    unit=\"m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the number concentration\\nWeighted by the cube of the radius\",\n",
    ")\n",
    "\n",
    "dataset_fitted[\"mass_size_distribution_from_wnc3\"] = (\n",
    "    msd_from_psd_dataarray(\n",
    "        da=dataset_fitted[\"weighted3_particle_size_distribution\"] * dataset_fitted[\"bin_width\"],\n",
    "    )\n",
    "    / dataset_fitted[\"bin_width\"]\n",
    ")\n",
    "dataset_fitted[\"mass_size_distribution_from_wnc3\"].attrs = dict(\n",
    "    long_name=\"Mass concentration from weighted number concentration\",\n",
    "    unit=\"kg m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the number concentration\\nWeighted by the cube of the radius\",\n",
    ")\n",
    "\n",
    "# Fit to the number concentration with weight of radius ^ 2\n",
    "dataset_fitted[\"weighted2_particle_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Weighted number concentration\",\n",
    "    unit=\"m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the number concentration\\nWeighted by the square of the radius\",\n",
    ")\n",
    "\n",
    "dataset_fitted[\"mass_size_distribution_from_wnc2\"] = (\n",
    "    msd_from_psd_dataarray(\n",
    "        da=dataset_fitted[\"weighted2_particle_size_distribution\"] * dataset_fitted[\"bin_width\"],\n",
    "    )\n",
    "    / dataset_fitted[\"bin_width\"]\n",
    ")\n",
    "dataset_fitted[\"mass_size_distribution_from_wnc2\"].attrs = dict(\n",
    "    long_name=\"Mass concentration from weighted number concentration\",\n",
    "    unit=\"kg m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the number concentration\\nWeighted by the square of the radius\",\n",
    ")\n",
    "\n",
    "\n",
    "# Fit to the mass concentration\n",
    "dataset_fitted[\"mass_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Mass concentration\", unit=\"kg m^{-3} m^{-1}\", comment=\"Fit to the mass concentration\"\n",
    ")\n",
    "\n",
    "dataset_fitted[\"particle_size_distribution_from_mc\"] = (\n",
    "    psd_from_msd_dataarray(\n",
    "        da=dataset_fitted[\"mass_size_distribution\"] * dataset_fitted[\"bin_width\"],\n",
    "    )\n",
    "    / dataset_fitted[\"bin_width\"]\n",
    ")\n",
    "dataset_fitted[\"particle_size_distribution_from_mc\"].attrs = dict(\n",
    "    long_name=\"Number concentration from mass concentration\",\n",
    "    unit=\"m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the mass concentration\",\n",
    ")\n",
    "\n",
    "dataset_fitted[\"radius\"].attrs.update(coarse_composite[\"radius\"].attrs)\n",
    "\n",
    "dataset_fitted[\"radius_micrometer\"] = dataset_fitted[\"radius\"] * 1e6\n",
    "dataset_fitted[\"radius_micrometer\"].attrs = dict(\n",
    "    long_name=\"Radius\",\n",
    "    unit=\"µm\",\n",
    ")\n",
    "\n",
    "colors = dict(\n",
    "    particle_size_distribution=\"orange\",\n",
    "    mass_size_distribution_from_nc=\"orange\",\n",
    "    weighted3_particle_size_distribution=\"blue\",\n",
    "    mass_size_distribution_from_wnc3=\"blue\",\n",
    "    weighted2_particle_size_distribution=\"red\",\n",
    "    mass_size_distribution_from_wnc2=\"red\",\n",
    "    particle_size_distribution_from_mc=\"purple\",\n",
    "    mass_size_distribution=\"purple\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "individual parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(9, 6))\n",
    "\n",
    "for i, ds in enumerate(\n",
    "    [\n",
    "        particle_size_distribution_parameters,\n",
    "        weighted2_particle_size_distribution_parameters,\n",
    "        weighted3_particle_size_distribution_parameters,\n",
    "        mass_size_distribution_parameters,\n",
    "    ]\n",
    "):\n",
    "    axs[i, 0].scatter(\n",
    "        1e6 * ds[\"mu1\"],\n",
    "        1e6 * ds[\"mu2\"],\n",
    "        marker=\".\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[i, 0].set_xlabel(\"mu1\")\n",
    "    axs[i, 0].set_ylabel(\"mu2\")\n",
    "\n",
    "    axs[i, 1].scatter(\n",
    "        1e6 * ds[\"sigma1\"],\n",
    "        1e6 * ds[\"sigma2\"],\n",
    "        marker=\".\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[i, 1].set_xlabel(\"sigma1\")\n",
    "    axs[i, 1].set_ylabel(\"sigma2\")\n",
    "\n",
    "    try:\n",
    "        axs[i, 2].scatter(\n",
    "            ds[\"scale_factor1\"],\n",
    "            ds[\"scale_factor2\"],\n",
    "            marker=\".\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "    except Exception:\n",
    "        axs[i, 2].scatter(\n",
    "            ds[\"scale1\"],\n",
    "            ds[\"scale2\"],\n",
    "            marker=\".\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "    axs[i, 2].set_xlabel(\"scale1\")\n",
    "    axs[i, 2].set_ylabel(\"scale2\")\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distirbution of PSD and MSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(13, 5))\n",
    "\n",
    "\n",
    "# plot the particle size distribution\n",
    "\n",
    "for key in [\n",
    "    \"particle_size_distribution\",\n",
    "    \"particle_size_distribution_from_mc\",\n",
    "    \"weighted3_particle_size_distribution\",\n",
    "    \"weighted2_particle_size_distribution\",\n",
    "]:\n",
    "    axs[0].plot(\n",
    "        dataset_fitted[\"radius\"],\n",
    "        dataset_fitted[key].T,\n",
    "        color=colors[key],\n",
    "        label=key,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "\n",
    "    axs[0].plot(\n",
    "        coarse_composite[\"radius\"],\n",
    "        coarse_composite[\"particle_size_distribution\"].mean(\"time\"),\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "    )\n",
    "\n",
    "axs[0].set_xscale(\"log\")\n",
    "# axs[0].set_yscale(\"log\")\n",
    "axs[0].set_yscale(\"symlog\", linthresh=1e-1, linscale=0.1)\n",
    "axs[0].set_yticks((1e0, 1e4, 1e8, 1e12, 1e16))\n",
    "axs[0].set_ylim(0, 1e14)\n",
    "\n",
    "axs[0].set_xlabel(label_from_attrs(dataset_fitted[\"radius_micrometer\"]))\n",
    "axs[0].set_ylabel(label_from_attrs(dataset_fitted[\"particle_size_distribution\"]))\n",
    "\n",
    "\n",
    "# plot the mass size distribution\n",
    "for key in [\n",
    "    \"mass_size_distribution_from_nc\",\n",
    "    \"mass_size_distribution_from_wnc3\",\n",
    "    \"mass_size_distribution_from_wnc2\",\n",
    "    \"mass_size_distribution\",\n",
    "]:\n",
    "    axs[1].plot(\n",
    "        dataset_fitted[\"radius\"],\n",
    "        dataset_fitted[key].T,\n",
    "        color=colors[key],\n",
    "        label=key,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "\n",
    "axs[1].set_xscale(\"log\")\n",
    "# axs[1].set_yscale(\"log\")\n",
    "axs[1].set_yscale(\"symlog\", linthresh=1e-10, linscale=0.1)\n",
    "axs[1].set_yticks((1e-8, 1e-6, 1e-4, 1e-2, 1e0, 1e2, 1e4))\n",
    "axs[1].set_ylim(0, 1e4)\n",
    "\n",
    "\n",
    "axs[1].set_xlabel(label_from_attrs(dataset_fitted[\"radius_micrometer\"]))\n",
    "axs[1].set_ylabel(label_from_attrs(dataset_fitted[\"mass_size_distribution\"]))\n",
    "\n",
    "fig.suptitle(\"Comparison of the three different fits\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"107/fit_comparison.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x22786212770>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(13, 5))\n",
    "\n",
    "\n",
    "im = axs[0].hist2d(\n",
    "    np.ravel(dataset_fitted[\"particle_size_distribution_from_mc\"]),\n",
    "    np.ravel(dataset_fitted[\"weighted3_particle_size_distribution\"]),\n",
    "    bins=np.geomspace(1e-12, 1e14, 100),\n",
    "    cmap=\"plasma\",\n",
    "    norm=mcolors.LogNorm(),\n",
    ")\n",
    "axs[0].set_xscale(\"log\")\n",
    "axs[0].set_yscale(\"log\")\n",
    "fig.colorbar(im[-1], ax=axs[0])\n",
    "\n",
    "im = axs[1].hist2d(\n",
    "    np.ravel(dataset_fitted[\"mass_size_distribution\"]),\n",
    "    np.ravel(dataset_fitted[\"mass_size_distribution_from_wnc3\"]),\n",
    "    bins=np.geomspace(1e-14, 6e1, 100),\n",
    "    cmap=\"plasma\",\n",
    "    norm=mcolors.LogNorm(vmin=1, vmax=300),\n",
    ")\n",
    "axs[1].set_xscale(\"log\")\n",
    "axs[1].set_yscale(\"log\")\n",
    "fig.colorbar(im[-1], ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the LWC from the fits to the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full radius range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, figsize=(15, 4.5), sharex=True, sharey=True)\n",
    "\n",
    "i = 0\n",
    "for key in (\n",
    "    \"mass_size_distribution_from_nc\",\n",
    "    \"mass_size_distribution_from_wnc3\",\n",
    "    \"mass_size_distribution_from_wnc2\",\n",
    "    \"mass_size_distribution\",\n",
    "):\n",
    "\n",
    "    lwc = 1e3 * (dataset_fitted[key] * dataset_fitted[\"bin_width\"]).sum(\"radius\")\n",
    "\n",
    "    axs[i].errorbar(\n",
    "        x=ds_observations[\"liquid_water_content\"],\n",
    "        xerr=ds_observations[\"liquid_water_content_sem\"],\n",
    "        y=lwc,\n",
    "        yerr=0,\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        label=dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=colors[key],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[i].set_title(\n",
    "        dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=adjust_lightness_array(\n",
    "            [\n",
    "                colors[key],\n",
    "            ],\n",
    "            0.75,\n",
    "        )[0],\n",
    "    )\n",
    "\n",
    "    # axs[i].set_title(dataset_fitted[key].attrs['comment'])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "\n",
    "for _ax in axs:\n",
    "    _ax.plot([0, 3], [0, 3], color=\"black\", linestyle=\"--\")\n",
    "    # _ax.set_aspect('equal')\n",
    "    _ax.set_xlabel(\"Observed LWC [g m$^{-3}$]\")\n",
    "    _ax.set_ylabel(\"Fitted LWC [g m$^{-3}$]\")\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed LWC for multiple double Log-Normal fits\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "_ax.set_xlim(0, 5)\n",
    "_ax.set_ylim(0, 5)\n",
    "\n",
    "fig.savefig(\"107/LWC_fit_comparison.png\", dpi=400)\n",
    "\n",
    "_ax.set_xlim(0, 0.7)\n",
    "_ax.set_ylim(0, 0.7)\n",
    "\n",
    "fig.savefig(\"107/LWC_fit_comparison_zoom.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Radii larger than 50 µm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, figsize=(15, 4.5), sharex=True, sharey=True)\n",
    "\n",
    "i = 0\n",
    "for key in (\n",
    "    \"mass_size_distribution_from_nc\",\n",
    "    \"mass_size_distribution_from_wnc3\",\n",
    "    \"mass_size_distribution_from_wnc2\",\n",
    "    \"mass_size_distribution\",\n",
    "):\n",
    "\n",
    "    lwc = 1e3 * (dataset_fitted[key] * dataset_fitted[\"bin_width\"]).sel(radius=slice(50e-6, None)).sum(\n",
    "        \"radius\"\n",
    "    )\n",
    "\n",
    "    axs[i].errorbar(\n",
    "        x=1e3 * ds_observations[\"liquid_water_content_50um\"],\n",
    "        xerr=1e3 * ds_observations[\"liquid_water_content_50um_sem\"],\n",
    "        y=lwc,\n",
    "        yerr=0,\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        label=dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=colors[key],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "    axs[i].set_title(\n",
    "        dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=adjust_lightness_array(\n",
    "            [\n",
    "                colors[key],\n",
    "            ],\n",
    "            0.75,\n",
    "        )[0],\n",
    "    )\n",
    "    i += 1\n",
    "\n",
    "for _ax in axs:\n",
    "    _ax.plot([0, 3], [0, 3], color=\"black\", linestyle=\"--\")\n",
    "    # _ax.set_aspect('equal')\n",
    "    _ax.set_xlabel(\"Observed LWC [g m$^{-3}$]\")\n",
    "    _ax.set_ylabel(\"Fitted LWC [g m$^{-3}$]\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Comparison of fitted and observed LWC for multiple double Log-Normal fits\\n Only Radii above 50 µm\"\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "_ax.set_xlim(0, 5)\n",
    "_ax.set_ylim(0, 5)\n",
    "\n",
    "fig.savefig(\"107/LWC_fit_comparison-50um.png\", dpi=400)\n",
    "\n",
    "_ax.set_xlim(0, 0.7)\n",
    "_ax.set_ylim(0, 0.7)\n",
    "\n",
    "fig.savefig(\"107/LWC_fit_comparison-50um-zoom.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare NBC from the fits with the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full radius range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, figsize=(15, 4.5), sharex=True, sharey=True)\n",
    "\n",
    "i = 0\n",
    "for key in (\n",
    "    \"particle_size_distribution\",\n",
    "    \"weighted3_particle_size_distribution\",\n",
    "    \"weighted2_particle_size_distribution\",\n",
    "    \"particle_size_distribution_from_mc\",\n",
    "):\n",
    "\n",
    "    nbc = (dataset_fitted[key] * dataset_fitted[\"bin_width\"]).sum(\"radius\")\n",
    "\n",
    "    axs[i].errorbar(\n",
    "        x=ds_observations[\"particle_size_distribution\"],\n",
    "        xerr=ds_observations[\"particle_size_distribution_sem\"],\n",
    "        y=nbc,\n",
    "        yerr=0,\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        label=dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=colors[key],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[i].set_title(\n",
    "        dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=adjust_lightness_array(\n",
    "            [\n",
    "                colors[key],\n",
    "            ],\n",
    "            0.75,\n",
    "        )[0],\n",
    "    )\n",
    "\n",
    "    # axs[i].set_title(dataset_fitted[key].attrs['comment'])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "\n",
    "for _ax in axs:\n",
    "\n",
    "    _ax.plot([0, 8e8], [0, 8e8], color=\"black\", linestyle=\"--\")\n",
    "    # _ax.set_aspect('equal')\n",
    "    _ax.set_xlabel(\"Observed NBC [m$^{-3}$]\")\n",
    "    _ax.set_ylabel(\"Fitted NBC [m$^{-3}$]\")\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed NBC for multiple double Log-Normal fits\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "_ax.set_xlim(0, 8e8)\n",
    "_ax.set_ylim(0, 8e8)\n",
    "\n",
    "fig.savefig(\"107/NBC_fit_comparison.png\", dpi=400)\n",
    "\n",
    "_ax.set_xlim(0, 2.5e8)\n",
    "_ax.set_ylim(0, 2.5e8)\n",
    "\n",
    "fig.savefig(\"107/NBC_fit_comparison_zoom.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Radii larger than 50 µm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, figsize=(15, 4.5), sharex=True, sharey=True)\n",
    "\n",
    "i = 0\n",
    "for key in (\n",
    "    \"particle_size_distribution\",\n",
    "    \"weighted3_particle_size_distribution\",\n",
    "    \"weighted2_particle_size_distribution\",\n",
    "    \"particle_size_distribution_from_mc\",\n",
    "):\n",
    "\n",
    "    nbc = (dataset_fitted[key].sel(radius=slice(50e-6, None)) * dataset_fitted[\"bin_width\"]).sum(\n",
    "        \"radius\"\n",
    "    )\n",
    "\n",
    "    axs[i].errorbar(\n",
    "        x=ds_observations[\"particle_size_distribution_50um\"],\n",
    "        xerr=ds_observations[\"particle_size_distribution_50um_sem\"],\n",
    "        y=nbc,\n",
    "        yerr=0,\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        label=dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=colors[key],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[i].set_title(\n",
    "        dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=adjust_lightness_array(\n",
    "            [\n",
    "                colors[key],\n",
    "            ],\n",
    "            0.75,\n",
    "        )[0],\n",
    "    )\n",
    "\n",
    "    # axs[i].set_title(dataset_fitted[key].attrs['comment'])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "\n",
    "for _ax in axs:\n",
    "\n",
    "    _ax.plot([0, 2e5], [0, 2e5], color=\"black\", linestyle=\"--\")\n",
    "    # _ax.set_aspect('equal')\n",
    "    _ax.set_xlabel(\"Observed NBC [m$^{-3}$]\")\n",
    "    _ax.set_ylabel(\"Fitted NBC [m$^{-3}$]\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Comparison of fitted and observed NBC for multiple double Log-Normal fits\\n Only Radii above 50 µm\"\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "_ax.set_xlim(0, 20e3)\n",
    "_ax.set_ylim(0, 20e3)\n",
    "\n",
    "\n",
    "# _ax.set_xscale(\"log\")\n",
    "# _ax.set_yscale(\"log\")\n",
    "fig.savefig(\"107/NBC_fit_comparison-50um.png\", dpi=400)\n",
    "\n",
    "_ax.set_xlim(0, 5e3)\n",
    "_ax.set_ylim(0, 5e3)\n",
    "\n",
    "fig.savefig(\"107/NBC_fit_comparison-50um-zoom.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distributions of some random clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cloud_ids = np.random.choice(identified_clusters[\"cloud_id\"], 3, replace=False)\n",
    "\n",
    "large_cloud_ids = identified_clusters[\"cloud_id\"].where(\n",
    "    identified_clusters[\"liquid_water_content\"] / identified_clusters[\"duration\"].dt.seconds > 0.5,\n",
    "    drop=True,\n",
    ")\n",
    "large_cloud_ids = np.random.choice(large_cloud_ids, 3, replace=False)\n",
    "\n",
    "cloud_ids = np.concatenate([small_cloud_ids, large_cloud_ids])\n",
    "\n",
    "ncols_nrows = ncols_nrows_from_N(len(cloud_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Particle size distirbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10, 7), sharex=True, sharey=True, **ncols_nrows)\n",
    "\n",
    "# plot the PSDs of the selected clouds\n",
    "for i, cloud_id in enumerate(cloud_ids):\n",
    "    _ax = axs.flatten()[i]\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    lwc_mean, lwc_sem = mean_and_stderror_of_mean(cc[\"liquid_water_content\"], dims=(\"time\",))\n",
    "\n",
    "    _ax.set_title(\n",
    "        f\"Cloud ID: {cloud_id:.0f}\\n{lwc_mean.values:.2f} ± {lwc_sem.values:.2f} g\" + \"$m^{-3}$\"\n",
    "    )\n",
    "\n",
    "    observations = cc[\"particle_size_distribution\"]\n",
    "\n",
    "    _ax.plot(\n",
    "        1e6 * observations[\"radius\"],\n",
    "        observations.mean(\"time\"),\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        color=\"black\",\n",
    "    )\n",
    "\n",
    "    for key in (\n",
    "        \"particle_size_distribution\",\n",
    "        \"weighted3_particle_size_distribution\",\n",
    "        \"weighted2_particle_size_distribution\",\n",
    "        \"particle_size_distribution_from_mc\",\n",
    "    ):\n",
    "\n",
    "        fit = dataset_fitted[key].sel(cloud_id=cloud_id)\n",
    "        _ax.plot(\n",
    "            dataset_fitted[\"radius_micrometer\"],\n",
    "            fit,\n",
    "            color=colors[key],\n",
    "            linestyle=\"-\",\n",
    "            label=fit.attrs[\"comment\"],\n",
    "        )\n",
    "\n",
    "    _ax.set_xscale(\"log\")\n",
    "    _ax.set_yscale(\"symlog\", linthresh=1e4, linscale=0.1)\n",
    "    _ax.set_yticks([0, 1e6, 1e9, 1e12])\n",
    "# axs.flatten()[0].legend()\n",
    "_ax.set_ylim(0, None)\n",
    "\n",
    "fig.supxlabel(label_from_attrs(dataset_fitted[\"radius_micrometer\"]))\n",
    "fig.supylabel(label_from_attrs(dataset_fitted[\"particle_size_distribution\"]))\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed PSDs for multiple double Log-Normal fits\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"107/PSD_fit_comparison.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10, 7), sharex=True, sharey=True, **ncols_nrows)\n",
    "\n",
    "# plot the PSDs of the selected clouds\n",
    "for i, cloud_id in enumerate(cloud_ids):\n",
    "    _ax = axs.flatten()[i]\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    lwc_mean, lwc_sem = mean_and_stderror_of_mean(cc[\"liquid_water_content\"], dims=(\"time\",))\n",
    "\n",
    "    _ax.set_title(\n",
    "        f\"Cloud ID: {cloud_id:.0f}\\n{lwc_mean.values:.2f} ± {lwc_sem.values:.2f} g\" + \"$m^{-3}$\"\n",
    "    )\n",
    "\n",
    "    observations = cc[\"mass_size_distribution\"]\n",
    "\n",
    "    _ax.plot(1e6 * cc[\"radius\"], observations.mean(\"time\"), marker=\".\", linestyle=\"None\", color=\"black\")\n",
    "\n",
    "    for key in (\n",
    "        \"mass_size_distribution\",\n",
    "        \"mass_size_distribution_from_wnc3\",\n",
    "        \"mass_size_distribution_from_wnc2\",\n",
    "        \"mass_size_distribution_from_nc\",\n",
    "    ):\n",
    "\n",
    "        fit = dataset_fitted[key].sel(cloud_id=cloud_id)\n",
    "        _ax.plot(\n",
    "            dataset_fitted[\"radius_micrometer\"],\n",
    "            fit,\n",
    "            color=colors[key],\n",
    "            linestyle=\"-\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "    _ax.set_xscale(\"log\")\n",
    "    _ax.set_yscale(\"symlog\", linthresh=1e-6, linscale=0.1)\n",
    "    _ax.set_yticks([0, 1e-3, 1e0, 1e3])\n",
    "    _ax.set_xlim(1, 3e3)\n",
    "_ax.set_ylim(0, None)\n",
    "\n",
    "fig.supxlabel(\"Radius [$µm$]\")\n",
    "fig.supylabel(label_from_attrs(dataset_fitted[\"mass_size_distribution\"]))\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed MSDs for multiple double Log-Normal fits\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"107/MSD_fit_comparison.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear axes - mass size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10, 7), sharex=True, sharey=False, **ncols_nrows)\n",
    "\n",
    "# plot the PSDs of the selected clouds\n",
    "for i, cloud_id in enumerate(cloud_ids):\n",
    "    _ax = axs.flatten()[i]\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    lwc_mean, lwc_sem = mean_and_stderror_of_mean(cc[\"liquid_water_content\"], dims=(\"time\",))\n",
    "\n",
    "    _ax.set_title(\n",
    "        f\"Cloud ID: {cloud_id:.0f}\\n{lwc_mean.values:.2f} ± {lwc_sem.values:.2f} g\" + \"$m^{-3}$\"\n",
    "    )\n",
    "\n",
    "    observations = cc[\"mass_size_distribution\"] * cc[\"bin_width\"]\n",
    "\n",
    "    _ax.plot(\n",
    "        1e3 * observations[\"radius\"],\n",
    "        1e3 * observations.mean(\"time\"),\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        color=\"black\",\n",
    "    )\n",
    "\n",
    "    for key in (\n",
    "        \"mass_size_distribution_from_nc\",\n",
    "        \"mass_size_distribution_from_wnc3\",\n",
    "        \"mass_size_distribution_from_wnc2\",\n",
    "        \"mass_size_distribution\",\n",
    "    ):\n",
    "\n",
    "        fit = dataset_fitted[key].sel(cloud_id=cloud_id)\n",
    "        _ax.plot(\n",
    "            1e3 * dataset_fitted[\"radius\"],\n",
    "            1e3 * fit * dataset_fitted[\"bin_width\"],\n",
    "            color=colors[key],\n",
    "            linestyle=\"-\",\n",
    "            label=fit.attrs[\"comment\"],\n",
    "            alpha=0.8,\n",
    "        )\n",
    "    _ax.set_xlim([0, 3])\n",
    "\n",
    "    # _ax.set_xscale('log')\n",
    "# _ax.set_yscale('symlog', linthresh = 1e4)\n",
    "for _ax in axs[0]:\n",
    "    _ax.set_ylim(0, 0.008)\n",
    "\n",
    "for _ax in axs[1]:\n",
    "    _ax.set_ylim(0, 0.15)\n",
    "\n",
    "\n",
    "fig.supxlabel(\"Radius [$mm$]\")\n",
    "fig.supylabel(\"Mass concentration [$g m^{-3}$]\")\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed MSDs for multiple double Log-Normal fits\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"107/MSD_fit_comparison_linear.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide for the correct fits to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think, that the best solution would be the:\n",
    "- <span style=\"color:#8A2BE2\">**Mass Concentration fit**</span>\n",
    "\n",
    "The easiest and still good estimate is the \n",
    "- <span style=\"color:#6495ED\">**Number Concentration fit with weight to $r^3$**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = data_dir / Path(\"model/inputv4.0\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "dataset_fitted.to_netcdf(output_dir / Path(\"fitted_distributions.nc\"))\n",
    "\n",
    "mass_size_distribution_parameters.to_netcdf(output_dir / Path(\"mass_size_distribution_parameters.nc\"))\n",
    "weighted3_particle_size_distribution_parameters.to_netcdf(\n",
    "    output_dir / Path(\"particle_size_distribution_parameters.nc\")\n",
    ")\n",
    "\n",
    "ds_observations.to_netcdf(output_dir / Path(\"lwc_and_nbc_cloud_composite.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = double_ln.parameters\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    smodels.double_ln_normal_distribution(t_test, *p.values()),\n",
    "    smodels.double_log_normal_distribution_all(\n",
    "        t_test, *p.values(), parameter_space=\"geometric\", x_space=\"ln\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = weighted3_particle_size_distribution_parameters\n",
    "\n",
    "p = dict(\n",
    "    mu1=p[\"mu1\"],\n",
    "    mu2=p[\"mu2\"],\n",
    "    sigma1=p[\"sigma1\"],\n",
    "    sigma2=p[\"sigma2\"],\n",
    "    scale_factor1=p[\"scale_factor1\"],\n",
    "    scale_factor2=p[\"scale_factor2\"],\n",
    ")\n",
    "\n",
    "p_all = dict(\n",
    "    mu1=p[\"mu1\"],\n",
    "    mu2=p[\"mu2\"],\n",
    "    sigma1=p[\"sigma1\"],\n",
    "    sigma2=p[\"sigma2\"],\n",
    "    scale1=p[\"scale_factor1\"],\n",
    "    scale2=p[\"scale_factor2\"],\n",
    ")\n",
    "\n",
    "a = smodels.double_log_normal_distribution_all(\n",
    "    t_test, **p_all, parameter_space=\"geometric\", x_space=\"ln\"\n",
    ")\n",
    "\n",
    "l = smodels.double_ln_normal_distribution(t_test, **p).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp1 = (\n",
    "    GeometricMuSigmaScaleLog(\n",
    "        geometric_mu_l=p[\"mu1\"],\n",
    "        geometric_std_dev=p[\"sigma1\"],\n",
    "        scale_l=p[\"scale_factor1\"],\n",
    "    )\n",
    "    .standardize()\n",
    "    .get_L_parameters(dict_keys=(\"mu1\", \"sigma1\", \"scale1\"))\n",
    ")\n",
    "pp2 = (\n",
    "    GeometricMuSigmaScaleLog(\n",
    "        geometric_mu_l=p[\"mu2\"],\n",
    "        geometric_std_dev=p[\"sigma2\"],\n",
    "        scale_l=p[\"scale_factor2\"],\n",
    "    )\n",
    "    .standardize()\n",
    "    .get_L_parameters(dict_keys=(\"mu2\", \"sigma2\", \"scale2\"))\n",
    ")\n",
    "\n",
    "ppp = dict()\n",
    "ppp.update(pp1)\n",
    "ppp.update(pp2)\n",
    "b = smodels.double_log_normal_distribution_all(t_test, **ppp, parameter_space=\"direct\", x_space=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(\n",
    "    a,\n",
    "    b,\n",
    "    rtol=1e-12,\n",
    ")\n",
    "np.testing.assert_allclose(\n",
    "    a,\n",
    "    l,\n",
    "    rtol=1e-12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climNum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
