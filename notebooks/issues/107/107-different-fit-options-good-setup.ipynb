{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import datetime\n",
    "\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple, List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import Bounds\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "from sdm_eurec4a import RepositoryPath, get_git_revision_hash\n",
    "from sdm_eurec4a.visulization import (\n",
    "    set_custom_rcParams,\n",
    "    adjust_lightness_array,\n",
    "    ncols_nrows_from_N,\n",
    "    label_from_attrs,\n",
    ")\n",
    "import sdm_eurec4a.input_processing.models as smodels\n",
    "from sdm_eurec4a.reductions import mean_and_stderror_of_mean\n",
    "from sdm_eurec4a.identifications import match_clouds_and_cloudcomposite\n",
    "from sdm_eurec4a.conversions import msd_from_psd_dataarray, psd_from_msd_dataarray\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "default_colors = set_custom_rcParams()\n",
    "dark_colors = adjust_lightness_array(default_colors, 0.7)\n",
    "\n",
    "from sdm_eurec4a import slurm_cluster\n",
    "\n",
    "data_dir = RepositoryPath(\"levante\").data_dir\n",
    "\n",
    "fig_dir = Path(\"107_v4.2\")\n",
    "fig_dir.mkdir(exist_ok=True, parents=False)\n",
    "\n",
    "output_dir = data_dir / Path(\"model/input_v4.2\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client, cluster = slurm_cluster.init_dask_slurm_cluster(\n",
    "#     scale=1, processes=16, walltime=\"00:35:00\", memory=\"16GB\",\n",
    "#     scheduler_options={\"dashboard_address\": \":8686\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import observation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_composite = xr.open_dataset(\n",
    "    data_dir / Path(\"observation/cloud_composite/processed/cloud_composite_SI_units_20241025.nc\"),\n",
    ")\n",
    "identified_clusters = xr.open_dataset(\n",
    "    data_dir\n",
    "    / Path(\n",
    "        \"observation/cloud_composite/processed/identified_clusters/identified_clusters_rain_mask_5.nc\"\n",
    "    )\n",
    ")\n",
    "identified_clusters = identified_clusters.swap_dims({\"time\": \"cloud_id\"})\n",
    "\n",
    "# attrs = cloud_composite[\"radius\"].attrs.copy()\n",
    "# attrs.update({\"units\": \"µm\"})\n",
    "# cloud_composite[\"radius\"] = cloud_composite[\"radius\"]\n",
    "# cloud_composite[\"radius_micro\"] = 1e6 * cloud_composite[\"radius\"]\n",
    "# cloud_composite[\"radius\"].attrs = attrs\n",
    "\n",
    "cloud_composite[\"radius2D\"] = cloud_composite[\"radius\"].expand_dims(time=cloud_composite[\"time\"])\n",
    "cloud_composite = cloud_composite.transpose(\"radius\", ...)\n",
    "\n",
    "\n",
    "# cloud_composite = cloud_composite.sel(radius=slice(10e-6, None))\n",
    "\n",
    "identified_clusters = identified_clusters.where(\n",
    "    (\n",
    "        (identified_clusters.duration.dt.seconds >= 3)\n",
    "        & (identified_clusters.altitude < 1200)\n",
    "        & (identified_clusters.altitude > 500)\n",
    "    ),\n",
    "    drop=True,\n",
    ")\n",
    "# identified_clusters = identified_clusters.isel(cloud_id=slice(0, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to coarsen the results, we need to make sure to apply the coarsening on the **NON** normalized data.\n",
    "Then we can normalized afterwards again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_radius_split = 95e-6  # 50 µm\n",
    "higher_radius_split = 5e-3  # 1 mm\n",
    "coarsen_factor = 3\n",
    "\n",
    "# lower_radius_split = 90e-6  # 50 µm\n",
    "# higher_radius_split = 0.5e-3  # 1 mm\n",
    "# coarsen_factor = 3\n",
    "\n",
    "coarse_composite = cloud_composite.sel(radius=slice(lower_radius_split, higher_radius_split)).copy()\n",
    "\n",
    "# make sure to have non normalized data to be coarsened\n",
    "# otherwise, the sum will not be conserved\n",
    "coarse_composite[\"particle_size_distribution\"] = (\n",
    "    coarse_composite[\"particle_size_distribution\"] * coarse_composite[\"bin_width\"]\n",
    ")\n",
    "coarse_composite[\"mass_size_distribution\"] = (\n",
    "    coarse_composite[\"mass_size_distribution\"] * coarse_composite[\"bin_width\"]\n",
    ")\n",
    "\n",
    "# use mean for radius and radius2D\n",
    "coarse_composite_radius = coarse_composite[\"radius\"].coarsen(radius=coarsen_factor).mean()\n",
    "coarse_composite_radius2D = coarse_composite[\"radius2D\"].coarsen(radius=coarsen_factor).mean()\n",
    "# use the sum for the rest\n",
    "coarse_composite = coarse_composite.coarsen(radius=coarsen_factor).sum()\n",
    "\n",
    "coarse_composite[\"radius\"] = coarse_composite_radius\n",
    "coarse_composite[\"radius2D\"] = coarse_composite_radius2D\n",
    "coarse_composite[\"diameter\"] = 2 * coarse_composite[\"radius\"]\n",
    "\n",
    "# make sure to have normalized data again\n",
    "coarse_composite[\"particle_size_distribution\"] = (\n",
    "    coarse_composite[\"particle_size_distribution\"] / coarse_composite[\"bin_width\"]\n",
    ")\n",
    "coarse_composite[\"mass_size_distribution\"] = (\n",
    "    coarse_composite[\"mass_size_distribution\"] / coarse_composite[\"bin_width\"]\n",
    ")\n",
    "\n",
    "coarse_composite[\"particle_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Number concentration\",\n",
    "    unit=cloud_composite[\"particle_size_distribution\"].attrs[\"unit\"],\n",
    ")\n",
    "coarse_composite[\"mass_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Mass concentration\",\n",
    "    unit=cloud_composite[\"mass_size_distribution\"].attrs[\"unit\"],\n",
    ")\n",
    "\n",
    "# merge the two composites with higher resoltion at small radii\n",
    "# and lower resolution at large radii\n",
    "coarse_composite = xr.merge(\n",
    "    [\n",
    "        coarse_composite.sel(radius=slice(lower_radius_split, higher_radius_split)),\n",
    "        cloud_composite.sel(radius=slice(None, lower_radius_split)),\n",
    "        cloud_composite.sel(radius=slice(higher_radius_split, None)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Test liquid water content is conserved\n",
    "np.testing.assert_allclose(\n",
    "    (coarse_composite[\"bin_width\"] * coarse_composite[\"mass_size_distribution\"]).sum(\"radius\"),\n",
    "    (cloud_composite[\"bin_width\"] * cloud_composite[\"mass_size_distribution\"]).sum(\"radius\"),\n",
    "    rtol=0.001,\n",
    ")\n",
    "# Test particle concentration is conserved\n",
    "np.testing.assert_allclose(\n",
    "    (coarse_composite[\"bin_width\"] * coarse_composite[\"particle_size_distribution\"]).sum(\"radius\"),\n",
    "    (cloud_composite[\"bin_width\"] * cloud_composite[\"particle_size_distribution\"]).sum(\"radius\"),\n",
    "    rtol=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundaries for the fitting parameters\n",
    "\n",
    "Please note, that the value of the fitting parameter are very very sensible!!!\n",
    "\n",
    "They need to be adjusted with a lot of effort to make the fit work well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better bounds are:\n",
    "````\n",
    "class PSDBounds:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mu1 = np.array([1e-6, 14e-6, 50e6])\n",
    "        self.mu2 = np.array([0.1e-3, 0.3e-3, 0.5e-3])\n",
    "        self.sigma1 = np.array([1.5, 1.6, 3.0])\n",
    "        self.sigma2 = np.array([1.4, 1.5, 2.0])\n",
    "        self.scale1 = np.array([1e-20, 1e13, 1e16])\n",
    "        self.scale2 = np.array([1e-20, 1e3, 1e10])\n",
    "class MSDBounds:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mu1 = np.array([1e-6, 14e-6, 50e6])\n",
    "        self.mu2 = np.array([0.1e-3, 0.3e-3, 0.5e-3])\n",
    "        self.sigma1 = np.array([1.3, 1.5, 3.5])\n",
    "        self.sigma2 = np.array([1.2, 1.4, 2.0])\n",
    "        self.scale1 = np.array([1e-20, 1e-1, 1e2])\n",
    "        self.scale2 = np.array([1e-20, 1e-4, 1e1])\n",
    "\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSDBounds:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mu1 = np.array([1e-6, 14e-6, 50e6])  # µm\n",
    "        self.mu2 = np.array([0.1e-3, 0.3e-3, 0.5e-3])  # µm\n",
    "        self.sigma1 = np.array([1.5, 1.6, 3.0])  # ??\n",
    "        self.sigma2 = np.array([1.4, 1.5, 2.0])  # ??\n",
    "        self.scale1 = np.array([1e-20, 1e13, 1e16])  # (Number of droplets)\n",
    "        self.scale2 = np.array([1e-20, 1e3, 1e10])  # (Number of droplets)\n",
    "        ## to omit the scale factor issue, we could try and set them higher for a better fit\n",
    "        ## (see issue #131)\n",
    "        # self.scale1 = np.array([1e-20, 2e13, 2e16])\n",
    "        # self.scale2 = np.array([1e-20, 2e3, 2e10])\n",
    "\n",
    "    def get_x0_bounds(\n",
    "        self,\n",
    "    ) -> Tuple[np.ndarray, Bounds]:\n",
    "\n",
    "        x0 = np.array(\n",
    "            [\n",
    "                self.mu1[1],\n",
    "                self.sigma1[1],\n",
    "                self.scale1[1],\n",
    "                self.mu2[1],\n",
    "                self.sigma2[1],\n",
    "                self.scale2[1],\n",
    "            ]\n",
    "        )\n",
    "        lower_bound = np.array(\n",
    "            [\n",
    "                self.mu1[0],\n",
    "                self.sigma1[0],\n",
    "                self.scale1[0],\n",
    "                self.mu2[0],\n",
    "                self.sigma2[0],\n",
    "                self.scale2[0],\n",
    "            ]\n",
    "        )\n",
    "        upper_bound = np.array(\n",
    "            [\n",
    "                self.mu1[2],\n",
    "                self.sigma1[2],\n",
    "                self.scale1[2],\n",
    "                self.mu2[2],\n",
    "                self.sigma2[2],\n",
    "                self.scale2[2],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        bounds = Bounds(\n",
    "            lb=lower_bound,\n",
    "            ub=upper_bound,\n",
    "        )\n",
    "\n",
    "        return x0, bounds\n",
    "\n",
    "    @property\n",
    "    def bounds(self):\n",
    "        x0, bounds = self.get_x0_bounds()\n",
    "        return bounds\n",
    "\n",
    "    @property\n",
    "    def x0(self):\n",
    "        x0, bounds = self.get_x0_bounds()\n",
    "        return x0\n",
    "\n",
    "\n",
    "class MSDBounds:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mu1 = np.array([1e-6, 14e-6, 50e6])  # µm\n",
    "        self.mu2 = np.array([0.1e-3, 0.3e-3, 0.5e-3])  # µm\n",
    "        self.sigma1 = np.array([1.3, 1.5, 3.5])  # ??\n",
    "        self.sigma2 = np.array([1.2, 1.4, 2.0])  # ??\n",
    "        self.scale1 = np.array([1e-20, 1e-1, 1e2])  # kg (mass of all droplets)\n",
    "        self.scale2 = np.array([1e-20, 1e-4, 1e1])  # kg (mass of all droplets)\n",
    "        ## to omit the scale factor issue, we could try and set them higher for a better fit\n",
    "        ## (see issue #131)\n",
    "        # self.scale1 = np.array([1e-20, 2e-1, 2e2])\n",
    "        # self.scale2 = np.array([1e-20, 2e-4, 2e1])\n",
    "\n",
    "    def get_x0_bounds(\n",
    "        self,\n",
    "    ) -> Tuple[np.ndarray, Bounds]:\n",
    "\n",
    "        x0 = np.array(\n",
    "            [\n",
    "                self.mu1[1],\n",
    "                self.sigma1[1],\n",
    "                self.scale1[1],\n",
    "                self.mu2[1],\n",
    "                self.sigma2[1],\n",
    "                self.scale2[1],\n",
    "            ]\n",
    "        )\n",
    "        lower_bound = np.array(\n",
    "            [\n",
    "                self.mu1[0],\n",
    "                self.sigma1[0],\n",
    "                self.scale1[0],\n",
    "                self.mu2[0],\n",
    "                self.sigma2[0],\n",
    "                self.scale2[0],\n",
    "            ]\n",
    "        )\n",
    "        upper_bound = np.array(\n",
    "            [\n",
    "                self.mu1[2],\n",
    "                self.sigma1[2],\n",
    "                self.scale1[2],\n",
    "                self.mu2[2],\n",
    "                self.sigma2[2],\n",
    "                self.scale2[2],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        bounds = Bounds(\n",
    "            lb=lower_bound,\n",
    "            ub=upper_bound,\n",
    "        )\n",
    "\n",
    "        return x0, bounds\n",
    "\n",
    "    @property\n",
    "    def bounds(self):\n",
    "        x0, bounds = self.get_x0_bounds()\n",
    "        return bounds\n",
    "\n",
    "    @property\n",
    "    def x0(self):\n",
    "        x0, bounds = self.get_x0_bounds()\n",
    "        return x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chose the correct representation of the log normal disribution.\n",
    "\n",
    "Because there are multiple functions which are used as log normal distribution, the correct usage and transformation is crucial!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_DataArray(d: Dict, new_coords: Dict) -> xr.Dataset:\n",
    "    new_dims = list(new_coords.keys())\n",
    "\n",
    "    d_new = {}\n",
    "    for key in d:\n",
    "        value = d[key]\n",
    "\n",
    "        # if value is a DataArray, list or numpy array, we can direclty create a DataArray\n",
    "        if isinstance(value, (np.ndarray, list, xr.DataArray)):\n",
    "            d_new[key] = xr.DataArray(\n",
    "                value,\n",
    "                coords=new_coords,\n",
    "            )\n",
    "        # if value is a number, we create a list with this number\n",
    "        elif isinstance(value, float):\n",
    "            # create a list, if value is only a number\n",
    "            d_new[key] = xr.DataArray(\n",
    "                [value],\n",
    "                coords=new_coords,\n",
    "                dims=new_dims,\n",
    "            )\n",
    "\n",
    "    result = xr.Dataset(\n",
    "        # coords= new_coords,\n",
    "        data_vars=d_new,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare individual datasets to be used for the fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:04<00:00, 35.65it/s]\n"
     ]
    }
   ],
   "source": [
    "list_lwc = []\n",
    "list_lwc_sem = []\n",
    "list_lwc_50um = []\n",
    "list_lwc_50um_sem = []\n",
    "\n",
    "list_nbc = []\n",
    "list_nbc_sem = []\n",
    "list_nbc_50um = []\n",
    "list_nbc_50um_sem = []\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"]):\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    lwc = cc[\"liquid_water_content\"]\n",
    "\n",
    "    lwc_mean, lwc_sem = mean_and_stderror_of_mean(lwc, dims=(\"time\",))\n",
    "    list_lwc.append(lwc_mean)\n",
    "    list_lwc_sem.append(lwc_sem)\n",
    "\n",
    "    lwc_above_50um = (\n",
    "        (cc[\"mass_size_distribution\"] * cc[\"bin_width\"]).sel(radius=slice(50e-6, None)).sum(\"radius\")\n",
    "    )\n",
    "    lwc_above_50um_mean, lwc_above_50um_sem = mean_and_stderror_of_mean(lwc_above_50um, dims=(\"time\",))\n",
    "    list_lwc_50um.append(lwc_above_50um_mean)\n",
    "    list_lwc_50um_sem.append(lwc_above_50um_sem)\n",
    "\n",
    "    nbc = (cc[\"particle_size_distribution\"] * cc[\"bin_width\"]).sum(\"radius\")\n",
    "\n",
    "    nbc_mean, nbc_sem = mean_and_stderror_of_mean(nbc, dims=(\"time\",))\n",
    "    list_nbc.append(nbc_mean)\n",
    "    list_nbc_sem.append(nbc_sem)\n",
    "\n",
    "    nbc_above_50um = (\n",
    "        (cc[\"particle_size_distribution\"] * cc[\"bin_width\"]).sel(radius=slice(50e-6, None)).sum(\"radius\")\n",
    "    )\n",
    "    nbc_above_50um_mean, nbc_above_50um_sem = mean_and_stderror_of_mean(nbc_above_50um, dims=(\"time\",))\n",
    "    list_nbc_50um.append(nbc_above_50um_mean)\n",
    "    list_nbc_50um_sem.append(nbc_above_50um_sem)\n",
    "\n",
    "\n",
    "da_lwc = xr.concat(\n",
    "    list_lwc,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_lwc.attrs = dict(\n",
    "    long_name=\"Liquid water content\",\n",
    "    units=\"g m^{-3}\",\n",
    ")\n",
    "\n",
    "da_lwc_sem = xr.concat(\n",
    "    list_lwc_sem,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_lwc_sem.attrs = dict(\n",
    "    long_name=\"Standard error of the mean of the liquid water content\",\n",
    "    units=\"g m^{-3}\",\n",
    ")\n",
    "\n",
    "da_lwc_50um = xr.concat(\n",
    "    list_lwc_50um,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_lwc_50um.attrs = dict(\n",
    "    long_name=\"Liquid water content above 50 µm\",\n",
    "    units=\"g m^{-3}\",\n",
    ")\n",
    "\n",
    "da_lwc_50um_sem = xr.concat(\n",
    "    list_lwc_50um_sem,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_lwc_50um_sem.attrs = dict(\n",
    "    long_name=\"Standard error of the mean of the liquid water content above 50 µm\",\n",
    "    units=\"g m^{-3}\",\n",
    ")\n",
    "\n",
    "da_nbc = xr.concat(\n",
    "    list_nbc,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_nbc.attrs = dict(\n",
    "    long_name=\"Number concentration\",\n",
    "    units=\"m^{-3}\",\n",
    ")\n",
    "\n",
    "da_nbc_sem = xr.concat(\n",
    "    list_nbc_sem,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_nbc_sem.attrs = dict(\n",
    "    long_name=\"Standard error of the mean of the number concentration\",\n",
    "    units=\"m^{-3}\",\n",
    ")\n",
    "\n",
    "da_nbc_50um = xr.concat(\n",
    "    list_nbc_50um,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_nbc_50um.attrs = dict(\n",
    "    long_name=\"Number concentration above 50 µm\",\n",
    "    units=\"m^{-3}\",\n",
    ")\n",
    "\n",
    "da_nbc_50um_sem = xr.concat(\n",
    "    list_nbc_50um_sem,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "da_nbc_50um_sem.attrs = dict(\n",
    "    long_name=\"Standard error of the mean of the number concentration above 50 µm\",\n",
    "    units=\"m^{-3}\",\n",
    ")\n",
    "\n",
    "\n",
    "ds_observations = xr.Dataset(\n",
    "    dict(\n",
    "        liquid_water_content=da_lwc,\n",
    "        liquid_water_content_sem=da_lwc_sem,\n",
    "        liquid_water_content_50um=da_lwc_50um,\n",
    "        liquid_water_content_50um_sem=da_lwc_50um_sem,\n",
    "        particle_size_distribution=da_nbc,\n",
    "        particle_size_distribution_sem=da_nbc_sem,\n",
    "        particle_size_distribution_50um=da_nbc_50um,\n",
    "        particle_size_distribution_50um_sem=da_nbc_50um_sem,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define t,w for the fitting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.geomspace(0.1e-6, 3e-3, 100)\n",
    "\n",
    "t_test = xr.DataArray(data=r, coords={\"radius\": r}, dims=[\"radius\"])\n",
    "\n",
    "# compute the weighting for each bin\n",
    "# IMPORTANT: Divide by 2 to have the correct bin_width. Missing this, will result in a bin width of double the size!\n",
    "w_test = (t_test - t_test.shift(radius=2)).shift(radius=-1) / 2\n",
    "w_test = w_test.interpolate_na(\"radius\", method=\"linear\", fill_value=\"extrapolate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit with weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit in number concentration space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 40/154 [00:01<00:05, 20.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:07<00:00, 20.78it/s]\n"
     ]
    }
   ],
   "source": [
    "list_parameters = []\n",
    "fitted_data = []\n",
    "\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"].values):\n",
    "\n",
    "    # extract the cloud composite data for the cloud\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    # use the particle size distribution and the radius\n",
    "    psd = cc[\"particle_size_distribution\"]  # .sel(radius = slice(15e-6, None))\n",
    "    radius = cc[\"radius\"].expand_dims(time=cc[\"time\"])  # .sel(radius = slice(15e-6, None))\n",
    "\n",
    "    # make sure to have the same order of dimensions\n",
    "    psd = psd.transpose(\"time\", \"radius\")\n",
    "    radius = radius.transpose(\"time\", \"radius\")\n",
    "\n",
    "    # fit the double log-normal distribution\n",
    "    double_ln = smodels.CleoDoubleLnNormalFit(\n",
    "        name=\"PSD\",\n",
    "        x0=PSDBounds().x0,\n",
    "        bounds=PSDBounds().bounds,\n",
    "        t_train=radius.mean(\"time\", skipna=True),\n",
    "        y_train=psd.mean(\"time\", skipna=True),\n",
    "        fit_kwargs=dict(\n",
    "            loss=\"linear\",\n",
    "            kwargs=dict(\n",
    "                # parameter_space= 'geometric',\n",
    "                # x_space = 'linear',\n",
    "                # density_scaled = False,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    double_ln.fit(3)\n",
    "\n",
    "    # save the parameters\n",
    "    list_parameters.append(\n",
    "        dict_to_DataArray(\n",
    "            d=double_ln.parameters,\n",
    "            new_coords=dict(\n",
    "                cloud_id=[\n",
    "                    cloud_id,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # predict the number concentration\n",
    "    dimension, prediction = double_ln.predict(t_test)\n",
    "\n",
    "    prediction = prediction.expand_dims(cloud_id=(cloud_id,))\n",
    "\n",
    "    # # we can also use only the radii where we have data:\n",
    "    # radii_measured = cc[\"particle_size_distribution\"].mean(\"time\") > 0\n",
    "    # radius_end = cc[\"radius\"].sel(radius=radii_measured).max().values\n",
    "    # radius_start = cc[\"radius\"].sel(radius=radii_measured).min().values\n",
    "    # radius_end = 1.5 * radius_end\n",
    "    # prediction = prediction.where(dimension >= radius_start, other = np.nan)\n",
    "    # prediction = prediction.where(dimension <= radius_end, other = np.nan)\n",
    "\n",
    "    fitted_data.append(prediction)\n",
    "\n",
    "\n",
    "# create a data array with the fitted number concentration\n",
    "particle_size_distribution_fitted_data = xr.concat(\n",
    "    fitted_data,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "particle_size_distribution_parameters = xr.concat(\n",
    "    list_parameters,\n",
    "    dim=\"cloud_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = \"\"\n",
    "details += \"Parameters of the double log-normal distribution fitted to the particle size distribution.\\n\"\n",
    "details += \"The fitting function was set in the CleoDoubleLnNormalFit class.\\n\"\n",
    "details += \"No weighting used in the cost function.\\n\"\n",
    "details += \"A coarsened input data was used.\\n\"\n",
    "details += \"The loss function was set to 'linear'.\\n\"\n",
    "\n",
    "particle_size_distribution_parameters.attrs = {\n",
    "    \"description\": \"parameters of a double log-normal distribution fitted to the particle size distribution\",\n",
    "    \"details\": details,\n",
    "    \"creation_time\": datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"author\": \"Nils Niebaum\",\n",
    "    \"email\": \"nils-ole.niebaum@mpimet.mpg.de\",\n",
    "    \"institution\": \"Max Planck Institute for Meteorology\",\n",
    "    \"github_repository\": \"https://github.com/nilsnevertree/sdm-eurec4a\",\n",
    "    \"git_commit\": get_git_revision_hash(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Square $^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:07<00:00, 19.79it/s]\n"
     ]
    }
   ],
   "source": [
    "list_parameters = []\n",
    "fitted_data = []\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"].data):\n",
    "\n",
    "    # extract the cloud composite data for the cloud\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    # use the particle size distribution and the radius\n",
    "    psd = cc[\"particle_size_distribution\"]  # .sel(radius = slice(15e-6, None))\n",
    "    radius = cc[\"radius\"].expand_dims(time=cc[\"time\"])  # .sel(radius = slice(15e-6, None))\n",
    "\n",
    "    # make sure to have the same order of dimensions\n",
    "    psd = psd.transpose(\"time\", \"radius\")\n",
    "    radius = radius.transpose(\"time\", \"radius\")\n",
    "\n",
    "    # fit the double log-normal distribution\n",
    "    double_ln = smodels.CleoDoubleLnNormalFit(\n",
    "        name=\"PSD weighted\",\n",
    "        x0=PSDBounds().x0,\n",
    "        bounds=PSDBounds().bounds,\n",
    "        t_train=radius.mean(\"time\", skipna=True),\n",
    "        y_train=psd.mean(\"time\", skipna=True),\n",
    "        fit_kwargs=dict(\n",
    "            loss=\"linear\",\n",
    "            kwargs=dict(\n",
    "                # parameter_space= 'geometric',\n",
    "                # x_space = 'linear',\n",
    "                # density_scaled = False,\n",
    "            ),\n",
    "        ),\n",
    "        t_weight_power=2,\n",
    "    )\n",
    "    double_ln.fit(3)\n",
    "\n",
    "    # save the parameters\n",
    "    list_parameters.append(\n",
    "        dict_to_DataArray(\n",
    "            d=double_ln.parameters,\n",
    "            new_coords=dict(\n",
    "                cloud_id=[\n",
    "                    cloud_id,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # predict the number concentration\n",
    "    dimension, prediction = double_ln.predict(t_test)\n",
    "\n",
    "    prediction = prediction.expand_dims(cloud_id=(cloud_id,))\n",
    "\n",
    "    # # we can also use only the radii where we have data:\n",
    "    # radii_measured = cc[\"particle_size_distribution\"].mean(\"time\") > 0\n",
    "    # radius_end = cc[\"radius\"].sel(radius=radii_measured).max().values\n",
    "    # radius_start = cc[\"radius\"].sel(radius=radii_measured).min().values\n",
    "    # radius_end = 1.5 * radius_end\n",
    "    # prediction = prediction.where(dimension >= radius_start, other = np.nan)\n",
    "    # prediction = prediction.where(dimension <= radius_end, other = np.nan)\n",
    "\n",
    "    fitted_data.append(prediction)\n",
    "\n",
    "\n",
    "# create a data array with the fitted number concentration\n",
    "weighted2_particle_size_distribution_fitted_data = xr.concat(\n",
    "    fitted_data,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "weighted2_particle_size_distribution_parameters = xr.concat(\n",
    "    list_parameters,\n",
    "    dim=\"cloud_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = \"\"\n",
    "details += \"Parameters of the double log-normal distribution fitted to the particle size distribution.\\n\"\n",
    "details += \"The fitting function was set in the CleoDoubleLnNormalFit class.\\n\"\n",
    "details += \"Weighting by the radius to power of 2 (surface) used in the cost function.\\n\"\n",
    "details += \"A coarsened input data was used.\\n\"\n",
    "details += \"The loss function was set to 'linear'.\\n\"\n",
    "\n",
    "weighted2_particle_size_distribution_parameters.attrs = {\n",
    "    \"description\": \"parameters of a double log-normal distribution fitted to the particle size distribution\",\n",
    "    \"details\": details,\n",
    "    \"creation_time\": datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"author\": \"Nils Niebaum\",\n",
    "    \"email\": \"nils-ole.niebaum@mpimet.mpg.de\",\n",
    "    \"institution\": \"Max Planck Institute for Meteorology\",\n",
    "    \"github_repository\": \"https://github.com/nilsnevertree/sdm-eurec4a\",\n",
    "    \"git_commit\": get_git_revision_hash(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cubic $^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:07<00:00, 21.99it/s]\n"
     ]
    }
   ],
   "source": [
    "list_parameters = []\n",
    "fitted_data = []\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"].data):\n",
    "\n",
    "    # extract the cloud composite data for the cloud\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    # use the particle size distribution and the radius\n",
    "    psd = cc[\"particle_size_distribution\"]  # .sel(radius = slice(15e-6, None))\n",
    "    radius = cc[\"radius\"].expand_dims(time=cc[\"time\"])  # .sel(radius = slice(15e-6, None))\n",
    "\n",
    "    # make sure to have the same order of dimensions\n",
    "    psd = psd.transpose(\"time\", \"radius\")\n",
    "    radius = radius.transpose(\"time\", \"radius\")\n",
    "\n",
    "    # fit the double log-normal distribution\n",
    "    double_ln = smodels.CleoDoubleLnNormalFit(\n",
    "        name=\"PSD weighted\",\n",
    "        x0=PSDBounds().x0,\n",
    "        bounds=PSDBounds().bounds,\n",
    "        t_train=radius.mean(\"time\", skipna=True),\n",
    "        y_train=psd.mean(\"time\", skipna=True),\n",
    "        fit_kwargs=dict(\n",
    "            loss=\"linear\",\n",
    "            kwargs=dict(\n",
    "                # parameter_space= 'geometric',\n",
    "                # x_space = 'linear',\n",
    "                # density_scaled = False,\n",
    "            ),\n",
    "        ),\n",
    "        t_weight_power=3,\n",
    "    )\n",
    "    double_ln.fit(3)\n",
    "\n",
    "    # save the parameters\n",
    "    list_parameters.append(\n",
    "        dict_to_DataArray(\n",
    "            d=double_ln.parameters,\n",
    "            new_coords=dict(\n",
    "                cloud_id=[\n",
    "                    cloud_id,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # predict the number concentration\n",
    "    dimension, prediction = double_ln.predict(t_test)\n",
    "\n",
    "    prediction = prediction.expand_dims(cloud_id=(cloud_id,))\n",
    "\n",
    "    # # we can also use only the radii where we have data:\n",
    "    # radii_measured = cc[\"particle_size_distribution\"].mean(\"time\") > 0\n",
    "    # radius_end = cc[\"radius\"].sel(radius=radii_measured).max().values\n",
    "    # radius_start = cc[\"radius\"].sel(radius=radii_measured).min().values\n",
    "    # radius_end = 1.5 * radius_end\n",
    "    # prediction = prediction.where(dimension >= radius_start, other = np.nan)\n",
    "    # prediction = prediction.where(dimension <= radius_end, other = np.nan)\n",
    "\n",
    "    fitted_data.append(prediction)\n",
    "\n",
    "\n",
    "# create a data array with the fitted number concentration\n",
    "weighted3_particle_size_distribution_fitted_data = xr.concat(\n",
    "    fitted_data,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "weighted3_particle_size_distribution_parameters = xr.concat(\n",
    "    list_parameters,\n",
    "    dim=\"cloud_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = \"\"\n",
    "details += \"Parameters of the double log-normal distribution fitted to the particle size distribution.\\n\"\n",
    "details += \"The fitting function was set in the CleoDoubleLnNormalFit class.\\n\"\n",
    "details += \"Weighting by the radius to power of 3 (volume) used in the cost function.\\n\"\n",
    "details += \"A coarsened input data was used.\\n\"\n",
    "details += \"The loss function was set to 'linear'.\\n\"\n",
    "\n",
    "weighted3_particle_size_distribution_parameters.attrs = {\n",
    "    \"description\": \"parameters of a double log-normal distribution fitted to the particle size distribution\",\n",
    "    \"details\": details,\n",
    "    \"creation_time\": datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"author\": \"Nils Niebaum\",\n",
    "    \"email\": \"nils-ole.niebaum@mpimet.mpg.de\",\n",
    "    \"institution\": \"Max Planck Institute for Meteorology\",\n",
    "    \"github_repository\": \"https://github.com/nilsnevertree/sdm-eurec4a\",\n",
    "    \"git_commit\": get_git_revision_hash(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit in mass concentration space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/154 [00:00<00:06, 24.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:09<00:00, 16.57it/s]\n"
     ]
    }
   ],
   "source": [
    "list_parameters = []\n",
    "fitted_data = []\n",
    "\n",
    "for cloud_id in tqdm(identified_clusters[\"cloud_id\"]):\n",
    "\n",
    "    # extract the cloud composite data for the cloud\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "    # cc = cc.expand_dims(cloud_id = [int(cloud_id.values)])\n",
    "\n",
    "    # use the mass size distribution and the radius\n",
    "    msd = cc[\"mass_size_distribution\"]  # .sel(radius = slice(15e-6, None))\n",
    "    radius = cc[\"radius\"].expand_dims(time=cc[\"time\"])  # .sel(radius = slice(15e-6, None))\n",
    "\n",
    "    # make sure to have the same order of dimensions\n",
    "    msd = msd.transpose(\"time\", \"radius\", ...)\n",
    "    radius = radius.transpose(\"time\", \"radius\", ...)\n",
    "\n",
    "    # fit the double log-normal distribution\n",
    "\n",
    "    double_ln = smodels.CleoDoubleLnNormalFit(\n",
    "        name=\"MSD\",\n",
    "        x0=MSDBounds().x0,\n",
    "        bounds=MSDBounds().bounds,\n",
    "        t_train=radius.mean(\"time\", skipna=True),\n",
    "        y_train=msd.mean(\"time\", skipna=True),\n",
    "        fit_kwargs=dict(\n",
    "            loss=\"linear\",\n",
    "            kwargs=dict(\n",
    "                # parameter_space= 'geometric',\n",
    "                # x_space = 'ln',\n",
    "                # density_scaled = False,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    double_ln.fit(3)\n",
    "    double_ln.parameters\n",
    "\n",
    "    # save the parameters\n",
    "    list_parameters.append(\n",
    "        dict_to_DataArray(\n",
    "            d=double_ln.parameters,\n",
    "            new_coords=dict(\n",
    "                cloud_id=[\n",
    "                    cloud_id,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # predict the number concentration\n",
    "    dimension, prediction = double_ln.predict(t_test)\n",
    "\n",
    "    prediction = prediction.expand_dims(cloud_id=(cloud_id,))\n",
    "\n",
    "    # # we can also use only the radii where we have data:\n",
    "    # radii_measured = cc[\"particle_size_distribution\"].mean(\"time\") > 0\n",
    "    # radius_end = cc[\"radius\"].sel(radius=radii_measured).max().values\n",
    "    # radius_start = cc[\"radius\"].sel(radius=radii_measured).min().values\n",
    "    # radius_end = 1.5 * radius_end\n",
    "    # prediction = prediction.where(dimension >= radius_start, other = np.nan)\n",
    "    # prediction = prediction.where(dimension <= radius_end, other = np.nan)\n",
    "\n",
    "    fitted_data.append(prediction)\n",
    "\n",
    "\n",
    "# create a data array with the fitted number concentration\n",
    "mass_size_distribution_fitted_data = xr.concat(\n",
    "    fitted_data,\n",
    "    dim=\"cloud_id\",\n",
    ")\n",
    "mass_size_distribution_parameters = xr.concat(\n",
    "    list_parameters,\n",
    "    dim=\"cloud_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = \"\"\n",
    "details += \"Parameters of the double log-normal distribution fitted to the mass size distribution.\\n\"\n",
    "details += \"The fitting function was set in the CleoDoubleLnNormalFit class.\\n\"\n",
    "details += \"No weighting used in the cost function.\\n\"\n",
    "details += \"A coarsened input data was used.\\n\"\n",
    "details += \"The loss function was set to 'linear'.\\n\"\n",
    "\n",
    "mass_size_distribution_parameters.attrs = {\n",
    "    \"description\": \"parameters of a double log-normal distribution fitted to the particle size distribution\",\n",
    "    \"details\": details,\n",
    "    \"creation_time\": datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"author\": \"Nils Niebaum\",\n",
    "    \"email\": \"nils-ole.niebaum@mpimet.mpg.de\",\n",
    "    \"institution\": \"Max Planck Institute for Meteorology\",\n",
    "    \"github_repository\": \"https://github.com/nilsnevertree/sdm-eurec4a\",\n",
    "    \"git_commit\": get_git_revision_hash(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the data into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the datasets\n",
    "dataset_fitted = xr.Dataset(\n",
    "    dict(\n",
    "        particle_size_distribution=particle_size_distribution_fitted_data,\n",
    "        weighted3_particle_size_distribution=weighted3_particle_size_distribution_fitted_data,\n",
    "        weighted2_particle_size_distribution=weighted2_particle_size_distribution_fitted_data,\n",
    "        mass_size_distribution=mass_size_distribution_fitted_data,\n",
    "    )\n",
    ")\n",
    "dataset_fitted[\"bin_width\"] = w_test\n",
    "dataset_fitted[\"bin_width\"].attrs = dict(\n",
    "    long_name=\"Bin width\",\n",
    "    unit=\"m\",\n",
    ")\n",
    "\n",
    "# Fit to the number concentration\n",
    "dataset_fitted[\"particle_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Number concentration\", unit=\"m^{-3} m^{-1}\", comment=\"Fit to the number concentration\"\n",
    ")\n",
    "\n",
    "dataset_fitted[\"mass_size_distribution_from_nc\"] = (\n",
    "    msd_from_psd_dataarray(\n",
    "        da=dataset_fitted[\"particle_size_distribution\"] * dataset_fitted[\"bin_width\"],\n",
    "    )\n",
    "    / dataset_fitted[\"bin_width\"]\n",
    ")\n",
    "dataset_fitted[\"mass_size_distribution_from_nc\"].attrs = dict(\n",
    "    long_name=\"Mass concentration from number concentration\",\n",
    "    unit=\"kg m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the number concentration\",\n",
    ")\n",
    "\n",
    "# Fit to the number concentration with weight\n",
    "dataset_fitted[\"weighted3_particle_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Weighted number concentration\",\n",
    "    unit=\"m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the number concentration\\nWeighted by the cube of the radius\",\n",
    ")\n",
    "\n",
    "dataset_fitted[\"mass_size_distribution_from_wnc3\"] = (\n",
    "    msd_from_psd_dataarray(\n",
    "        da=dataset_fitted[\"weighted3_particle_size_distribution\"] * dataset_fitted[\"bin_width\"],\n",
    "    )\n",
    "    / dataset_fitted[\"bin_width\"]\n",
    ")\n",
    "dataset_fitted[\"mass_size_distribution_from_wnc3\"].attrs = dict(\n",
    "    long_name=\"Mass concentration from weighted number concentration\",\n",
    "    unit=\"kg m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the number concentration\\nWeighted by the cube of the radius\",\n",
    ")\n",
    "\n",
    "# Fit to the number concentration with weight of radius ^ 2\n",
    "dataset_fitted[\"weighted2_particle_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Weighted number concentration\",\n",
    "    unit=\"m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the number concentration\\nWeighted by the square of the radius\",\n",
    ")\n",
    "\n",
    "dataset_fitted[\"mass_size_distribution_from_wnc2\"] = (\n",
    "    msd_from_psd_dataarray(\n",
    "        da=dataset_fitted[\"weighted2_particle_size_distribution\"] * dataset_fitted[\"bin_width\"],\n",
    "    )\n",
    "    / dataset_fitted[\"bin_width\"]\n",
    ")\n",
    "dataset_fitted[\"mass_size_distribution_from_wnc2\"].attrs = dict(\n",
    "    long_name=\"Mass concentration from weighted number concentration\",\n",
    "    unit=\"kg m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the number concentration\\nWeighted by the square of the radius\",\n",
    ")\n",
    "\n",
    "\n",
    "# Fit to the mass concentration\n",
    "dataset_fitted[\"mass_size_distribution\"].attrs = dict(\n",
    "    long_name=\"Mass concentration\", unit=\"kg m^{-3} m^{-1}\", comment=\"Fit to the mass concentration\"\n",
    ")\n",
    "\n",
    "dataset_fitted[\"particle_size_distribution_from_mc\"] = (\n",
    "    psd_from_msd_dataarray(\n",
    "        da=dataset_fitted[\"mass_size_distribution\"] * dataset_fitted[\"bin_width\"],\n",
    "    )\n",
    "    / dataset_fitted[\"bin_width\"]\n",
    ")\n",
    "dataset_fitted[\"particle_size_distribution_from_mc\"].attrs = dict(\n",
    "    long_name=\"Number concentration from mass concentration\",\n",
    "    unit=\"m^{-3} m^{-1}\",\n",
    "    comment=\"Fit to the mass concentration\",\n",
    ")\n",
    "\n",
    "dataset_fitted[\"radius\"].attrs.update(coarse_composite[\"radius\"].attrs)\n",
    "\n",
    "dataset_fitted[\"radius_micrometer\"] = dataset_fitted[\"radius\"] * 1e6\n",
    "dataset_fitted[\"radius_micrometer\"].attrs = dict(\n",
    "    long_name=\"Radius\",\n",
    "    unit=\"µm\",\n",
    ")\n",
    "\n",
    "colors = dict(\n",
    "    particle_size_distribution=\"orange\",\n",
    "    mass_size_distribution_from_nc=\"orange\",\n",
    "    weighted3_particle_size_distribution=\"blue\",\n",
    "    mass_size_distribution_from_wnc3=\"blue\",\n",
    "    weighted2_particle_size_distribution=\"red\",\n",
    "    mass_size_distribution_from_wnc2=\"red\",\n",
    "    particle_size_distribution_from_mc=\"purple\",\n",
    "    mass_size_distribution=\"purple\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "individual parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(9, 6))\n",
    "\n",
    "for i, ds in enumerate(\n",
    "    [\n",
    "        particle_size_distribution_parameters,\n",
    "        weighted2_particle_size_distribution_parameters,\n",
    "        weighted3_particle_size_distribution_parameters,\n",
    "        mass_size_distribution_parameters,\n",
    "    ]\n",
    "):\n",
    "    axs[i, 0].scatter(\n",
    "        1e6 * ds[\"mu1\"],\n",
    "        1e6 * ds[\"mu2\"],\n",
    "        marker=\".\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[i, 0].set_xlabel(\"mu1\")\n",
    "    axs[i, 0].set_ylabel(\"mu2\")\n",
    "\n",
    "    axs[i, 1].scatter(\n",
    "        1e6 * ds[\"sigma1\"],\n",
    "        1e6 * ds[\"sigma2\"],\n",
    "        marker=\".\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[i, 1].set_xlabel(\"sigma1\")\n",
    "    axs[i, 1].set_ylabel(\"sigma2\")\n",
    "\n",
    "    try:\n",
    "        axs[i, 2].scatter(\n",
    "            ds[\"scale_factor1\"],\n",
    "            ds[\"scale_factor2\"],\n",
    "            marker=\".\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "    except Exception:\n",
    "        axs[i, 2].scatter(\n",
    "            ds[\"scale1\"],\n",
    "            ds[\"scale2\"],\n",
    "            marker=\".\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "    axs[i, 2].set_xlabel(\"scale1\")\n",
    "    axs[i, 2].set_ylabel(\"scale2\")\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distirbution of PSD and MSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(13, 5))\n",
    "\n",
    "\n",
    "# plot the particle size distribution\n",
    "\n",
    "for key in [\n",
    "    \"particle_size_distribution\",\n",
    "    \"particle_size_distribution_from_mc\",\n",
    "    \"weighted3_particle_size_distribution\",\n",
    "    \"weighted2_particle_size_distribution\",\n",
    "]:\n",
    "    axs[0].plot(\n",
    "        dataset_fitted[\"radius\"],\n",
    "        dataset_fitted[key].T,\n",
    "        color=colors[key],\n",
    "        label=key,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "\n",
    "    axs[0].plot(\n",
    "        coarse_composite[\"radius\"],\n",
    "        coarse_composite[\"particle_size_distribution\"].mean(\"time\"),\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        color=\"black\",\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "axs[0].set_xscale(\"log\")\n",
    "# axs[0].set_yscale(\"log\")\n",
    "axs[0].set_yscale(\"symlog\", linthresh=1e-1, linscale=0.1)\n",
    "axs[0].set_yticks((1e0, 1e4, 1e8, 1e12, 1e16))\n",
    "axs[0].set_ylim(0, 1e14)\n",
    "\n",
    "axs[0].set_xlabel(label_from_attrs(dataset_fitted[\"radius_micrometer\"]))\n",
    "axs[0].set_ylabel(label_from_attrs(dataset_fitted[\"particle_size_distribution\"]))\n",
    "\n",
    "\n",
    "# plot the mass size distribution\n",
    "for key in [\n",
    "    \"mass_size_distribution_from_nc\",\n",
    "    \"mass_size_distribution_from_wnc3\",\n",
    "    \"mass_size_distribution_from_wnc2\",\n",
    "    \"mass_size_distribution\",\n",
    "]:\n",
    "    axs[1].plot(\n",
    "        dataset_fitted[\"radius\"],\n",
    "        dataset_fitted[key].T,\n",
    "        color=colors[key],\n",
    "        label=key,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "\n",
    "axs[1].set_xscale(\"log\")\n",
    "# axs[1].set_yscale(\"log\")\n",
    "axs[1].set_yscale(\"symlog\", linthresh=1e-10, linscale=0.1)\n",
    "axs[1].set_yticks((1e-8, 1e-6, 1e-4, 1e-2, 1e0, 1e2, 1e4))\n",
    "axs[1].set_ylim(0, 1e4)\n",
    "\n",
    "\n",
    "axs[1].set_xlabel(label_from_attrs(dataset_fitted[\"radius_micrometer\"]))\n",
    "axs[1].set_ylabel(label_from_attrs(dataset_fitted[\"mass_size_distribution\"]))\n",
    "\n",
    "fig.suptitle(\"Comparison of the three different fits\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(fig_dir / \"fit_comparison.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7ffb589e3140>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(13, 5))\n",
    "\n",
    "\n",
    "im = axs[0].hist2d(\n",
    "    np.ravel(dataset_fitted[\"particle_size_distribution_from_mc\"]),\n",
    "    np.ravel(dataset_fitted[\"weighted3_particle_size_distribution\"]),\n",
    "    bins=np.geomspace(1e-12, 1e14, 100),\n",
    "    cmap=\"plasma\",\n",
    "    norm=mcolors.LogNorm(),\n",
    ")\n",
    "axs[0].set_xscale(\"log\")\n",
    "axs[0].set_yscale(\"log\")\n",
    "fig.colorbar(im[-1], ax=axs[0])\n",
    "\n",
    "im = axs[1].hist2d(\n",
    "    np.ravel(dataset_fitted[\"mass_size_distribution\"]),\n",
    "    np.ravel(dataset_fitted[\"mass_size_distribution_from_wnc3\"]),\n",
    "    bins=np.geomspace(1e-14, 6e1, 100),\n",
    "    cmap=\"plasma\",\n",
    "    norm=mcolors.LogNorm(vmin=1, vmax=300),\n",
    ")\n",
    "axs[1].set_xscale(\"log\")\n",
    "axs[1].set_yscale(\"log\")\n",
    "fig.colorbar(im[-1], ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the LWC from the fits to the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full radius range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, figsize=(15, 4.5), sharex=True, sharey=True)\n",
    "\n",
    "i = 0\n",
    "for key in (\n",
    "    \"mass_size_distribution_from_nc\",\n",
    "    \"mass_size_distribution_from_wnc3\",\n",
    "    \"mass_size_distribution_from_wnc2\",\n",
    "    \"mass_size_distribution\",\n",
    "):\n",
    "\n",
    "    lwc = 1e3 * (dataset_fitted[key] * dataset_fitted[\"bin_width\"]).sum(\"radius\")\n",
    "\n",
    "    axs[i].errorbar(\n",
    "        x=ds_observations[\"liquid_water_content\"],\n",
    "        xerr=ds_observations[\"liquid_water_content_sem\"],\n",
    "        y=lwc,\n",
    "        yerr=0,\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        label=dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=colors[key],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[i].set_title(\n",
    "        dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=adjust_lightness_array(\n",
    "            [\n",
    "                colors[key],\n",
    "            ],\n",
    "            0.75,\n",
    "        )[0],\n",
    "    )\n",
    "\n",
    "    # axs[i].set_title(dataset_fitted[key].attrs['comment'])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "\n",
    "for _ax in axs:\n",
    "    _ax.plot([0, 3], [0, 3], color=\"black\", linestyle=\"--\")\n",
    "    # _ax.set_aspect('equal')\n",
    "    _ax.set_xlabel(\"Observed LWC [g m$^{-3}$]\")\n",
    "    _ax.set_ylabel(\"Fitted LWC [g m$^{-3}$]\")\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed LWC for multiple double Log-Normal fits\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "_ax.set_xlim(0, 5)\n",
    "_ax.set_ylim(0, 5)\n",
    "\n",
    "fig.savefig(fig_dir / \"LWC_fit_comparison.png\", dpi=400)\n",
    "\n",
    "_ax.set_xlim(0, 0.7)\n",
    "_ax.set_ylim(0, 0.7)\n",
    "\n",
    "fig.savefig(fig_dir / \"LWC_fit_comparison_zoom.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Radii larger than 50 µm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, figsize=(15, 4.5), sharex=True, sharey=True)\n",
    "\n",
    "i = 0\n",
    "for key in (\n",
    "    \"mass_size_distribution_from_nc\",\n",
    "    \"mass_size_distribution_from_wnc3\",\n",
    "    \"mass_size_distribution_from_wnc2\",\n",
    "    \"mass_size_distribution\",\n",
    "):\n",
    "\n",
    "    lwc = 1e3 * (dataset_fitted[key] * dataset_fitted[\"bin_width\"]).sel(radius=slice(50e-6, None)).sum(\n",
    "        \"radius\"\n",
    "    )\n",
    "\n",
    "    axs[i].errorbar(\n",
    "        x=1e3 * ds_observations[\"liquid_water_content_50um\"],\n",
    "        xerr=1e3 * ds_observations[\"liquid_water_content_50um_sem\"],\n",
    "        y=lwc,\n",
    "        yerr=0,\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        label=dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=colors[key],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "    axs[i].set_title(\n",
    "        dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=adjust_lightness_array(\n",
    "            [\n",
    "                colors[key],\n",
    "            ],\n",
    "            0.75,\n",
    "        )[0],\n",
    "    )\n",
    "    i += 1\n",
    "\n",
    "for _ax in axs:\n",
    "    _ax.plot([0, 3], [0, 3], color=\"black\", linestyle=\"--\")\n",
    "    # _ax.set_aspect('equal')\n",
    "    _ax.set_xlabel(\"Observed LWC [g m$^{-3}$]\")\n",
    "    _ax.set_ylabel(\"Fitted LWC [g m$^{-3}$]\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Comparison of fitted and observed LWC for multiple double Log-Normal fits\\n Only Radii above 50 µm\"\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "_ax.set_xlim(0, 5)\n",
    "_ax.set_ylim(0, 5)\n",
    "\n",
    "fig.savefig(fig_dir / \"LWC_fit_comparison-50um.png\", dpi=400)\n",
    "\n",
    "_ax.set_xlim(0, 0.7)\n",
    "_ax.set_ylim(0, 0.7)\n",
    "\n",
    "fig.savefig(fig_dir / \"LWC_fit_comparison-50um-zoom.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare NBC from the fits with the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full radius range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, figsize=(15, 4.5), sharex=True, sharey=True)\n",
    "\n",
    "i = 0\n",
    "for key in (\n",
    "    \"particle_size_distribution\",\n",
    "    \"weighted3_particle_size_distribution\",\n",
    "    \"weighted2_particle_size_distribution\",\n",
    "    \"particle_size_distribution_from_mc\",\n",
    "):\n",
    "\n",
    "    nbc = (dataset_fitted[key] * dataset_fitted[\"bin_width\"]).sum(\"radius\")\n",
    "\n",
    "    axs[i].errorbar(\n",
    "        x=ds_observations[\"particle_size_distribution\"],\n",
    "        xerr=ds_observations[\"particle_size_distribution_sem\"],\n",
    "        y=nbc,\n",
    "        yerr=0,\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        label=dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=colors[key],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[i].set_title(\n",
    "        dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=adjust_lightness_array(\n",
    "            [\n",
    "                colors[key],\n",
    "            ],\n",
    "            0.75,\n",
    "        )[0],\n",
    "    )\n",
    "\n",
    "    # axs[i].set_title(dataset_fitted[key].attrs['comment'])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "\n",
    "for _ax in axs:\n",
    "\n",
    "    _ax.plot([0, 8e8], [0, 8e8], color=\"black\", linestyle=\"--\")\n",
    "    # _ax.set_aspect('equal')\n",
    "    _ax.set_xlabel(\"Observed NBC [m$^{-3}$]\")\n",
    "    _ax.set_ylabel(\"Fitted NBC [m$^{-3}$]\")\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed NBC for multiple double Log-Normal fits\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "_ax.set_xlim(0, 8e8)\n",
    "_ax.set_ylim(0, 8e8)\n",
    "\n",
    "fig.savefig(fig_dir / \"NBC_fit_comparison.png\", dpi=400)\n",
    "\n",
    "_ax.set_xlim(0, 2.5e8)\n",
    "_ax.set_ylim(0, 2.5e8)\n",
    "\n",
    "fig.savefig(fig_dir / \"NBC_fit_comparison_zoom.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Radii larger than 50 µm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, figsize=(15, 4.5), sharex=True, sharey=True)\n",
    "\n",
    "i = 0\n",
    "for key in (\n",
    "    \"particle_size_distribution\",\n",
    "    \"weighted3_particle_size_distribution\",\n",
    "    \"weighted2_particle_size_distribution\",\n",
    "    \"particle_size_distribution_from_mc\",\n",
    "):\n",
    "\n",
    "    nbc = (dataset_fitted[key].sel(radius=slice(50e-6, None)) * dataset_fitted[\"bin_width\"]).sum(\n",
    "        \"radius\"\n",
    "    )\n",
    "\n",
    "    axs[i].errorbar(\n",
    "        x=ds_observations[\"particle_size_distribution_50um\"],\n",
    "        xerr=ds_observations[\"particle_size_distribution_50um_sem\"],\n",
    "        y=nbc,\n",
    "        yerr=0,\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        label=dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=colors[key],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[i].set_title(\n",
    "        dataset_fitted[key].attrs[\"comment\"],\n",
    "        color=adjust_lightness_array(\n",
    "            [\n",
    "                colors[key],\n",
    "            ],\n",
    "            0.75,\n",
    "        )[0],\n",
    "    )\n",
    "\n",
    "    # axs[i].set_title(dataset_fitted[key].attrs['comment'])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "\n",
    "for _ax in axs:\n",
    "\n",
    "    _ax.plot([0, 2e5], [0, 2e5], color=\"black\", linestyle=\"--\")\n",
    "    # _ax.set_aspect('equal')\n",
    "    _ax.set_xlabel(\"Observed NBC [m$^{-3}$]\")\n",
    "    _ax.set_ylabel(\"Fitted NBC [m$^{-3}$]\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Comparison of fitted and observed NBC for multiple double Log-Normal fits\\n Only Radii above 50 µm\"\n",
    ")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "_ax.set_xlim(0, 20e3)\n",
    "_ax.set_ylim(0, 20e3)\n",
    "\n",
    "\n",
    "# _ax.set_xscale(\"log\")\n",
    "# _ax.set_yscale(\"log\")\n",
    "fig.savefig(fig_dir / \"NBC_fit_comparison-50um.png\", dpi=400)\n",
    "\n",
    "_ax.set_xlim(0, 5e3)\n",
    "_ax.set_ylim(0, 5e3)\n",
    "\n",
    "fig.savefig(fig_dir / \"NBC_fit_comparison-50um-zoom.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distributions of some random clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cloud_ids = np.random.choice(identified_clusters[\"cloud_id\"], 3, replace=False)\n",
    "\n",
    "large_cloud_ids = identified_clusters[\"cloud_id\"].where(\n",
    "    identified_clusters[\"liquid_water_content\"] / identified_clusters[\"duration\"].dt.seconds > 0.5,\n",
    "    drop=True,\n",
    ")\n",
    "large_cloud_ids = np.random.choice(large_cloud_ids, 3, replace=False)\n",
    "\n",
    "cloud_ids = np.concatenate([small_cloud_ids, large_cloud_ids])\n",
    "\n",
    "ncols_nrows = ncols_nrows_from_N(len(cloud_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Particle size distirbution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10, 7), sharex=True, sharey=True, **ncols_nrows)\n",
    "\n",
    "# plot the PSDs of the selected clouds\n",
    "for i, cloud_id in enumerate(cloud_ids):\n",
    "    _ax = axs.flatten()[i]\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    lwc_mean, lwc_sem = mean_and_stderror_of_mean(cc[\"liquid_water_content\"], dims=(\"time\",))\n",
    "\n",
    "    _ax.set_title(\n",
    "        f\"Cloud ID: {cloud_id:.0f}\\n{lwc_mean.values:.2f} ± {lwc_sem.values:.2f} g\" + \"$m^{-3}$\"\n",
    "    )\n",
    "\n",
    "    observations = cc[\"particle_size_distribution\"]\n",
    "\n",
    "    _ax.plot(\n",
    "        1e6 * observations[\"radius\"],\n",
    "        observations.mean(\"time\"),\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        color=\"black\",\n",
    "    )\n",
    "\n",
    "    for key in (\n",
    "        \"particle_size_distribution\",\n",
    "        \"weighted3_particle_size_distribution\",\n",
    "        \"weighted2_particle_size_distribution\",\n",
    "        \"particle_size_distribution_from_mc\",\n",
    "    ):\n",
    "\n",
    "        fit = dataset_fitted[key].sel(cloud_id=cloud_id)\n",
    "        _ax.plot(\n",
    "            dataset_fitted[\"radius_micrometer\"],\n",
    "            fit,\n",
    "            color=colors[key],\n",
    "            linestyle=\"-\",\n",
    "            label=fit.attrs[\"comment\"],\n",
    "        )\n",
    "\n",
    "    _ax.set_xscale(\"log\")\n",
    "    _ax.set_yscale(\"symlog\", linthresh=1e4, linscale=0.1)\n",
    "    _ax.set_yticks([0, 1e6, 1e9, 1e12])\n",
    "# axs.flatten()[0].legend()\n",
    "_ax.set_ylim(0, None)\n",
    "\n",
    "fig.supxlabel(label_from_attrs(dataset_fitted[\"radius_micrometer\"]))\n",
    "fig.supylabel(label_from_attrs(dataset_fitted[\"particle_size_distribution\"]))\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed PSDs for multiple double Log-Normal fits\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(fig_dir / \"PSD_fit_comparison.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10, 7), sharex=True, sharey=True, **ncols_nrows)\n",
    "\n",
    "# plot the PSDs of the selected clouds\n",
    "for i, cloud_id in enumerate(cloud_ids):\n",
    "    _ax = axs.flatten()[i]\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    lwc_mean, lwc_sem = mean_and_stderror_of_mean(cc[\"liquid_water_content\"], dims=(\"time\",))\n",
    "\n",
    "    _ax.set_title(\n",
    "        f\"Cloud ID: {cloud_id:.0f}\\n{lwc_mean.values:.2f} ± {lwc_sem.values:.2f} g\" + \"$m^{-3}$\"\n",
    "    )\n",
    "\n",
    "    observations = cc[\"mass_size_distribution\"]\n",
    "\n",
    "    _ax.plot(1e6 * cc[\"radius\"], observations.mean(\"time\"), marker=\".\", linestyle=\"None\", color=\"black\")\n",
    "\n",
    "    for key in (\n",
    "        \"mass_size_distribution\",\n",
    "        \"mass_size_distribution_from_wnc3\",\n",
    "        \"mass_size_distribution_from_wnc2\",\n",
    "        \"mass_size_distribution_from_nc\",\n",
    "    ):\n",
    "\n",
    "        fit = dataset_fitted[key].sel(cloud_id=cloud_id)\n",
    "        _ax.plot(\n",
    "            dataset_fitted[\"radius_micrometer\"],\n",
    "            fit,\n",
    "            color=colors[key],\n",
    "            linestyle=\"-\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "    _ax.set_xscale(\"log\")\n",
    "    _ax.set_yscale(\"symlog\", linthresh=1e-6, linscale=0.1)\n",
    "    _ax.set_yticks([0, 1e-3, 1e0, 1e3])\n",
    "    _ax.set_xlim(1, 3e3)\n",
    "_ax.set_ylim(0, None)\n",
    "\n",
    "fig.supxlabel(\"Radius [$µm$]\")\n",
    "fig.supylabel(label_from_attrs(dataset_fitted[\"mass_size_distribution\"]))\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed MSDs for multiple double Log-Normal fits\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(fig_dir / \"MSD_fit_comparison.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear axes - mass size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10, 7), sharex=True, sharey=False, **ncols_nrows)\n",
    "\n",
    "# plot the PSDs of the selected clouds\n",
    "for i, cloud_id in enumerate(cloud_ids):\n",
    "    _ax = axs.flatten()[i]\n",
    "    cc = match_clouds_and_cloudcomposite(\n",
    "        ds_clouds=identified_clusters.sel(cloud_id=cloud_id),\n",
    "        ds_cloudcomposite=coarse_composite,\n",
    "    )\n",
    "\n",
    "    lwc_mean, lwc_sem = mean_and_stderror_of_mean(cc[\"liquid_water_content\"], dims=(\"time\",))\n",
    "\n",
    "    _ax.set_title(\n",
    "        f\"Cloud ID: {cloud_id:.0f}\\n{lwc_mean.values:.2f} ± {lwc_sem.values:.2f} g\" + \"$m^{-3}$\"\n",
    "    )\n",
    "\n",
    "    observations = cc[\"mass_size_distribution\"] * cc[\"bin_width\"]\n",
    "\n",
    "    _ax.plot(\n",
    "        1e3 * observations[\"radius\"],\n",
    "        1e3 * observations.mean(\"time\"),\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "        color=\"black\",\n",
    "    )\n",
    "\n",
    "    for key in (\n",
    "        \"mass_size_distribution_from_nc\",\n",
    "        \"mass_size_distribution_from_wnc3\",\n",
    "        \"mass_size_distribution_from_wnc2\",\n",
    "        \"mass_size_distribution\",\n",
    "    ):\n",
    "\n",
    "        fit = dataset_fitted[key].sel(cloud_id=cloud_id)\n",
    "        _ax.plot(\n",
    "            1e3 * dataset_fitted[\"radius\"],\n",
    "            1e3 * fit * dataset_fitted[\"bin_width\"],\n",
    "            color=colors[key],\n",
    "            linestyle=\"-\",\n",
    "            label=fit.attrs[\"comment\"],\n",
    "            alpha=0.8,\n",
    "        )\n",
    "    _ax.set_xlim([0, 3])\n",
    "\n",
    "    # _ax.set_xscale('log')\n",
    "# _ax.set_yscale('symlog', linthresh = 1e4)\n",
    "for _ax in axs[0]:\n",
    "    _ax.set_ylim(0, 0.008)\n",
    "\n",
    "for _ax in axs[1]:\n",
    "    _ax.set_ylim(0, 0.15)\n",
    "\n",
    "\n",
    "fig.supxlabel(\"Radius [$mm$]\")\n",
    "fig.supylabel(\"Mass concentration [$g m^{-3}$]\")\n",
    "\n",
    "fig.suptitle(\"Comparison of fitted and observed MSDs for multiple double Log-Normal fits\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(fig_dir / \"MSD_fit_comparison_linear.png\", dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide for the correct fits to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think, that the best solution would be the:\n",
    "- <span style=\"color:#8A2BE2\">**Mass Concentration fit**</span>\n",
    "\n",
    "The easiest and still good estimate is the \n",
    "- <span style=\"color:#6495ED\">**Number Concentration fit with weight to $r^3$**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psd_parameters = weighted3_particle_size_distribution_parameters\n",
    "attrs = psd_parameters.attrs\n",
    "params1_linear_space = (\n",
    "    smodels.GeometricMuSigmaScaleLog(\n",
    "        geometric_mu_l=psd_parameters[\"mu1\"],\n",
    "        geometric_std_dev=psd_parameters[\"sigma1\"],\n",
    "        scale_l=psd_parameters[\"scale_factor1\"],\n",
    "    )\n",
    "    .standardize()\n",
    "    .get_geometric_parameters_linear(\n",
    "        dict_keys=(\"geometric_mean1\", \"geometric_std_dev1\", \"scale_factor1\")\n",
    "    )\n",
    ")\n",
    "\n",
    "params2_linear_space = (\n",
    "    smodels.GeometricMuSigmaScaleLog(\n",
    "        geometric_mu_l=psd_parameters[\"mu2\"],\n",
    "        geometric_std_dev=psd_parameters[\"sigma2\"],\n",
    "        scale_l=psd_parameters[\"scale_factor2\"],\n",
    "    )\n",
    "    .standardize()\n",
    "    .get_geometric_parameters_linear(\n",
    "        dict_keys=(\"geometric_mean2\", \"geometric_std_dev2\", \"scale_factor2\")\n",
    "    )\n",
    ")\n",
    "\n",
    "psd_parameters_linear_space = xr.Dataset(dict(**params1_linear_space, **params2_linear_space))\n",
    "\n",
    "mapping = dict(\n",
    "    geometric_mean1=\"geometric_mean1\",\n",
    "    geometric_mean2=\"geometric_mean2\",\n",
    "    geometric_std_dev1=\"geometric_std_dev1\",\n",
    "    geometric_std_dev2=\"geometric_std_dev2\",\n",
    "    scale_factor1=\"scale_factor1\",\n",
    "    scale_factor2=\"scale_factor2\",\n",
    ")\n",
    "\n",
    "psd_parameters_linear_space.attrs.update(attrs)\n",
    "psd_parameters_linear_space.attrs[\"parameter_space\"] = \"geometric\"\n",
    "psd_parameters_linear_space.attrs[\"independent_space\"] = \"linear\"\n",
    "\n",
    "psd_parameters.attrs[\"parameter_space\"] = \"geometric\"\n",
    "psd_parameters.attrs[\"independent_space\"] = \"log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = RepositoryPath(\"levante\").data_dir\n",
    "output_dir = data_dir / Path(\"model/input_v4.2\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "dataset_fitted.to_netcdf(output_dir / Path(\"fitted_distributions.nc\"))\n",
    "\n",
    "mass_size_distribution_parameters.to_netcdf(output_dir / Path(\"mass_size_distribution_parameters.nc\"))\n",
    "\n",
    "psd_parameters.to_netcdf(output_dir / Path(\"particle_size_distribution_parameters.nc\"))\n",
    "\n",
    "psd_parameters_linear_space.to_netcdf(\n",
    "    output_dir / Path(\"particle_size_distribution_parameters_linear_space.nc\")\n",
    ")\n",
    "\n",
    "ds_observations.to_netcdf(output_dir / Path(\"lwc_and_nbc_cloud_composite.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = double_ln.parameters\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    smodels.double_ln_normal_distribution(t_test, *p.values()),\n",
    "    smodels.double_log_normal_distribution_all(\n",
    "        t_test, *p.values(), parameter_space=\"geometric\", x_space=\"ln\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = weighted3_particle_size_distribution_parameters\n",
    "\n",
    "p = dict(\n",
    "    mu1=p[\"mu1\"],\n",
    "    mu2=p[\"mu2\"],\n",
    "    sigma1=p[\"sigma1\"],\n",
    "    sigma2=p[\"sigma2\"],\n",
    "    scale_factor1=p[\"scale_factor1\"],\n",
    "    scale_factor2=p[\"scale_factor2\"],\n",
    ")\n",
    "\n",
    "p_all = dict(\n",
    "    mu1=p[\"mu1\"],\n",
    "    mu2=p[\"mu2\"],\n",
    "    sigma1=p[\"sigma1\"],\n",
    "    sigma2=p[\"sigma2\"],\n",
    "    scale1=p[\"scale_factor1\"],\n",
    "    scale2=p[\"scale_factor2\"],\n",
    ")\n",
    "\n",
    "a = smodels.double_log_normal_distribution_all(\n",
    "    t_test, **p_all, parameter_space=\"geometric\", x_space=\"ln\"\n",
    ")\n",
    "\n",
    "l = smodels.double_ln_normal_distribution(t_test, **p).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdm_pysd_python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
