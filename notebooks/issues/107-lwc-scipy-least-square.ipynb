{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import default_rng\n",
    "from scipy.optimize import Bounds\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict, TypedDict\n",
    "\n",
    "from sdm_eurec4a.reductions import mean_and_stderror_of_mean\n",
    "\n",
    "from sdm_eurec4a.visulization import (\n",
    "    set_custom_rcParams,\n",
    ")\n",
    "from sdm_eurec4a.identifications import match_clouds_and_cloudcomposite, select_individual_cloud_by_id\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "default_colors = set_custom_rcParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_ln_normal_distribution(\n",
    "    t: np.ndarray,\n",
    "    mu1: float,\n",
    "    sigma1: float,\n",
    "    scale_factor1: float,\n",
    "    mu2: float,\n",
    "    sigma2: float,\n",
    "    scale_factor2: float,\n",
    ") -> np.ndarray:\n",
    "\n",
    "    result = np.zeros(t.size)\n",
    "\n",
    "    for mu, sigma, scale_factor in zip(\n",
    "        (mu1, mu2),\n",
    "        (sigma1, sigma2),\n",
    "        (scale_factor1, scale_factor2),\n",
    "    ):\n",
    "        sigtilda = np.log(sigma)\n",
    "        mutilda = np.log(mu)\n",
    "\n",
    "        norm = scale_factor / (np.sqrt(2 * np.pi) * sigtilda)\n",
    "        exponent = -((np.log(t) - mutilda) ** 2) / (2 * sigtilda**2)\n",
    "\n",
    "        dn_dlnr = norm * np.exp(exponent)  # eq.5.8 [lohmann intro 2 clouds]\n",
    "\n",
    "        result += dn_dlnr\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def cost_function(x, t, y, var_scale=0.01, var_minimal=1e-12, var_replace=None):\n",
    "    y_is = double_ln_normal_distribution(t, *x)\n",
    "\n",
    "    # also devide by the variance of the data\n",
    "    var = 1\n",
    "    var = np.abs(var_scale * y)\n",
    "    # # var = np.abs(var_scale * t ** (-0.5))\n",
    "\n",
    "    if var_replace is None:\n",
    "        var_min = np.where(var > var_minimal, var, np.nan)\n",
    "        var_replace = np.nanmin(var_min)\n",
    "    var = np.where(var <= var_minimal, var, var_replace)\n",
    "    return np.ravel((y_is - y) / np.sqrt(var))\n",
    "\n",
    "\n",
    "rng = default_rng()\n",
    "\n",
    "\n",
    "def gen_data(\n",
    "    t: np.ndarray,\n",
    "    mu1: float,\n",
    "    sigma1: float,\n",
    "    scale_factor1: float,\n",
    "    mu2: float,\n",
    "    sigma2: float,\n",
    "    scale_factor2: float,\n",
    "    noise=0.0,\n",
    "    n_outliers=0,\n",
    "    seed=None,\n",
    "):\n",
    "    rng = default_rng(seed)\n",
    "\n",
    "    y = double_ln_normal_distribution(\n",
    "        t=t,\n",
    "        mu1=mu1,\n",
    "        sigma1=sigma1,\n",
    "        scale_factor1=scale_factor1,\n",
    "        mu2=mu2,\n",
    "        sigma2=sigma2,\n",
    "        scale_factor2=scale_factor2,\n",
    "    )\n",
    "    error = noise * rng.standard_normal(t.size)\n",
    "    outliers = rng.integers(0, t.size, n_outliers)\n",
    "    error[outliers] = np.sqrt(t[outliers]) * error[outliers]\n",
    "\n",
    "    return y + error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleLnParams(TypedDict):\n",
    "    mu1: float\n",
    "    sigma1: float\n",
    "    scale_factor1: float\n",
    "    mu2: float\n",
    "    sigma2: float\n",
    "    scale_factor2: float\n",
    "\n",
    "\n",
    "params = DoubleLnParams(\n",
    "    mu1=1e-2,\n",
    "    sigma1=2,\n",
    "    scale_factor1=5,\n",
    "    mu2=0.5e1,\n",
    "    sigma2=3,\n",
    "    scale_factor2=1,\n",
    ")\n",
    "\n",
    "t_min = 0.1\n",
    "t_max = 10\n",
    "n_points = 40\n",
    "n_outliers = 5\n",
    "\n",
    "t_train = np.logspace(-3, 2, n_points)\n",
    "m_train = gen_data(\n",
    "    t=t_train,\n",
    "    noise=0.2,\n",
    "    n_outliers=n_outliers,\n",
    "    **params,\n",
    ")\n",
    "\n",
    "\n",
    "x0 = np.array([1e-1, 2.0, 1.0, 10.0, 2.0, 1.0])\n",
    "\n",
    "\n",
    "bounds = Bounds(\n",
    "    lb=[1e-10, 1e-10, -np.inf, 2e-2, 1e-10, -np.inf],\n",
    "    ub=[5e-1, np.inf, np.inf, np.inf, np.inf, np.inf],\n",
    "    keep_feasible=[True, True, True, False, True, True],\n",
    ")\n",
    "\n",
    "\n",
    "res_lsq = least_squares(cost_function, x0, bounds=bounds, args=(t_train, m_train))\n",
    "\n",
    "res_soft_l1 = least_squares(\n",
    "    cost_function, x0, loss=\"soft_l1\", f_scale=0.1, bounds=bounds, args=(t_train, m_train)\n",
    ")\n",
    "\n",
    "res_log = least_squares(\n",
    "    cost_function, x0, loss=\"cauchy\", f_scale=0.1, bounds=bounds, args=(t_train, m_train)\n",
    ")\n",
    "\n",
    "\n",
    "t_test = np.logspace(-5, 2, n_points * 10)\n",
    "m_true = gen_data(\n",
    "    t=t_test,\n",
    "    **params,\n",
    ")\n",
    "\n",
    "m_lsq = gen_data(t_test, *res_lsq.x)\n",
    "m_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
    "m_log = gen_data(t_test, *res_log.x)\n",
    "\n",
    "plt.plot(t_train, m_train, \"o\")\n",
    "plt.plot(t_test, m_true, \"k\", linewidth=2, label=\"true\", linestyle=\"--\")\n",
    "plt.plot(t_test, m_lsq, label=\"linear loss\", linestyle=\":\")\n",
    "plt.plot(t_test, m_soft_l1, label=\"soft_l1 loss\", linestyle=\"--\")\n",
    "plt.plot(t_test, m_log, label=\"cauchy loss\", linestyle=\"-.\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the ``least_square`` method to the cloud composite dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "cloud_composite = xr.open_dataset(\n",
    "    \"/home/m/m301096/repositories/sdm-eurec4a/data/observation/cloud_composite/processed/cloud_composite_si_units.nc\"\n",
    ")\n",
    "identified_clouds = xr.open_dataset(\n",
    "    \"/home/m/m301096/repositories/sdm-eurec4a/data/observation/cloud_composite/processed/identified_clusters/identified_clusters_rain_mask_5.nc\"\n",
    ")\n",
    "\n",
    "attrs = cloud_composite[\"radius\"].attrs.copy()\n",
    "attrs.update({\"units\": \"µm\"})\n",
    "cloud_composite[\"radius\"] = cloud_composite[\"radius\"]\n",
    "cloud_composite[\"radius_micro\"] = 1e6 * cloud_composite[\"radius\"]\n",
    "cloud_composite[\"radius\"].attrs = attrs\n",
    "\n",
    "# cloud_composite = cloud_composite.sel(radius = slice(10, None))\n",
    "\n",
    "identified_clouds = identified_clouds.where(\n",
    "    (\n",
    "        (identified_clouds.duration.dt.total_seconds() > 5)\n",
    "        & (identified_clouds.alt < 1300)\n",
    "        & (identified_clouds.alt > 500)\n",
    "    ),\n",
    "    drop=True,\n",
    ")\n",
    "print(len(identified_clouds[\"cloud_id\"]))\n",
    "cloud_composite = match_clouds_and_cloudcomposite(identified_clouds, cloud_composite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_composite = cloud_composite.coarsen(radius=3).sum()\n",
    "coarse_composite[\"diameter\"] = 2 * coarse_composite[\"radius\"]\n",
    "# normalize the particle size distirbution\n",
    "attrs = coarse_composite[\"particle_size_distribution\"].attrs.copy()\n",
    "attrs[\"units\"] = \"m^-3 m^-1\"\n",
    "attrs[\"long_name\"] = \"Particle size distribution\"\n",
    "attrs[\"comment\"] = \"Each bin gives the number of droplets per cubic meter of air per meter of radius\"\n",
    "\n",
    "coarse_composite[\"particle_size_distribution\"] = (\n",
    "    coarse_composite[\"particle_size_distribution\"] / coarse_composite[\"bin_width\"] / 2\n",
    ")\n",
    "coarse_composite[\"particle_size_distribution\"].attrs = attrs\n",
    "\n",
    "# normalize the mass size distribution\n",
    "attrs_mass = coarse_composite[\"mass_size_distribution\"].attrs.copy()\n",
    "attrs_mass[\"units\"] = \"kg m^-3 m^-1\"\n",
    "attrs_mass[\"long_name\"] = \"Mass size distribution\"\n",
    "attrs_mass[\"comment\"] = \"Each bin gives the mass of droplets per cubic meter of air per meter of radius\"\n",
    "\n",
    "coarse_composite[\"mass_size_distribution\"] = (\n",
    "    coarse_composite[\"mass_size_distribution\"] / coarse_composite[\"bin_width\"] / 2\n",
    ")\n",
    "coarse_composite[\"mass_size_distribution\"].attrs = attrs_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius_split = 50e-6  # 50 µm\n",
    "\n",
    "\n",
    "coarse_composite = cloud_composite.sel(radius=slice(radius_split, None)).coarsen(radius=3).sum()\n",
    "\n",
    "coarse_composite[\"diameter\"] = 2 * coarse_composite[\"radius\"]\n",
    "\n",
    "coarse_composite = xr.merge(\n",
    "    [\n",
    "        coarse_composite,\n",
    "        cloud_composite.sel(radius=slice(None, radius_split)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# # normalize the particle size distirbution\n",
    "# attrs = coarse_composite['particle_size_distribution'].attrs.copy()\n",
    "# attrs['units'] = 'm^-3 m^-1'\n",
    "# attrs['long_name'] = 'Particle size distribution'\n",
    "# attrs['comment'] = 'Each bin gives the number of droplets per cubic meter of air per meter of radius'\n",
    "\n",
    "# coarse_composite['particle_size_distribution'] = (\n",
    "#     coarse_composite['particle_size_distribution'] / coarse_composite['bin_width'] / 2\n",
    "# )\n",
    "# coarse_composite['particle_size_distribution'].attrs = attrs\n",
    "\n",
    "# # normalize the mass size distribution\n",
    "# attrs_mass = coarse_composite['mass_size_distribution'].attrs.copy()\n",
    "# attrs_mass['units'] = 'kg m^-3 m^-1'\n",
    "# attrs_mass['long_name'] = 'Mass size distribution'\n",
    "# attrs_mass['comment'] = 'Each bin gives the mass of droplets per cubic meter of air per meter of radius'\n",
    "\n",
    "# coarse_composite['mass_size_distribution'] = (\n",
    "#     coarse_composite['mass_size_distribution'] / coarse_composite['bin_width'] / 2\n",
    "# )\n",
    "# coarse_composite['mass_size_distribution'].attrs = attrs_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 7.872833008032434e-05 LWC\n",
      "40 6.697070801544104e-05 LWC\n",
      "194 1.4557802384083504e-05 LWC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mass size distribution')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = cloud_composite\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(8, 6), sharex=True)\n",
    "\n",
    "# plot 5 individual random clouds\n",
    "np.random.seed(42)\n",
    "cloud_ids = rng.choice(identified_clouds[\"cloud_id\"], 3, replace=False)\n",
    "\n",
    "for i, cloud_id in enumerate(cloud_ids):\n",
    "    cloud = select_individual_cloud_by_id(identified_clouds, cloud_id)\n",
    "    ds = match_clouds_and_cloudcomposite(cloud, dataset)\n",
    "    m, v = mean_and_stderror_of_mean(ds[\"particle_size_distribution\"], dims=(\"time\",))\n",
    "    axs[0].errorbar(\n",
    "        x=m[\"radius\"],\n",
    "        xerr=0,\n",
    "        y=m,\n",
    "        yerr=2 * v,\n",
    "        label=f\"cloud {cloud_id}\",\n",
    "        color=default_colors[i],\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "    )\n",
    "    m, v = mean_and_stderror_of_mean(ds[\"mass_size_distribution\"], dims=(\"time\",))\n",
    "    axs[1].errorbar(\n",
    "        x=m[\"radius\"],\n",
    "        xerr=0,\n",
    "        y=m,\n",
    "        yerr=2 * v,\n",
    "        label=f\"cloud {cloud_id}\",\n",
    "        color=default_colors[i],\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "    )\n",
    "    print(f\"{cloud_id} {ds['mass_size_distribution'].sum('radius').mean('time').values} LWC\")\n",
    "\n",
    "\n",
    "m, v = mean_and_stderror_of_mean(dataset[\"particle_size_distribution\"], dims=(\"time\",))\n",
    "axs[0].plot(\n",
    "    m.radius,\n",
    "    m,\n",
    "    label=\"mean\",\n",
    "    color=\"k\",\n",
    "    zorder=10,\n",
    ")\n",
    "axs[0].fill_between(\n",
    "    m.radius,\n",
    "    m - 2 * v,\n",
    "    m + 2 * v,\n",
    "    alpha=0.5,\n",
    "    color=\"k\",\n",
    "    label=\"mean ± std error\",\n",
    "    zorder=10,\n",
    ")\n",
    "\n",
    "m, v = mean_and_stderror_of_mean(dataset[\"mass_size_distribution\"], dims=(\"time\",))\n",
    "axs[1].plot(m.radius, m, label=\"mean\", color=\"k\", zorder=10)\n",
    "axs[1].fill_between(\n",
    "    m.radius,\n",
    "    m - 2 * v,\n",
    "    m + 2 * v,\n",
    "    alpha=0.5,\n",
    "    color=\"k\",\n",
    "    label=\"mean ± std error\",\n",
    "    zorder=10,\n",
    ")\n",
    "\n",
    "axs[0].set_yscale(\"log\")\n",
    "for _ax in axs:\n",
    "    _ax.legend(loc=\"upper center\")\n",
    "    _ax.set_xscale(\"log\")\n",
    "axs[1].set_ylim(0, 1e-5)\n",
    "\n",
    "axs[0].set_ylabel(\"particle size distribution\")\n",
    "axs[1].set_ylabel(\"mass size distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 7.872833008032434e-05 LWC\n",
      "40 6.697070801544103e-05 LWC\n",
      "194 1.4557802384083506e-05 LWC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'mass size distribution')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = coarse_composite\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(8, 6), sharex=True)\n",
    "\n",
    "\n",
    "for i, cloud_id in enumerate(cloud_ids):\n",
    "    cloud = select_individual_cloud_by_id(identified_clouds, cloud_id)\n",
    "    ds = match_clouds_and_cloudcomposite(cloud, dataset)\n",
    "    m, v = mean_and_stderror_of_mean(ds[\"particle_size_distribution\"], dims=(\"time\",))\n",
    "    axs[0].errorbar(\n",
    "        x=m[\"radius\"],\n",
    "        xerr=0,\n",
    "        y=m,\n",
    "        yerr=2 * v,\n",
    "        label=f\"cloud {cloud_id}\",\n",
    "        color=default_colors[i],\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "    )\n",
    "    m, v = mean_and_stderror_of_mean(ds[\"mass_size_distribution\"], dims=(\"time\",))\n",
    "    axs[1].errorbar(\n",
    "        x=m[\"radius\"],\n",
    "        xerr=0,\n",
    "        y=m,\n",
    "        yerr=2 * v,\n",
    "        label=f\"cloud {cloud_id}\",\n",
    "        color=default_colors[i],\n",
    "        marker=\".\",\n",
    "        linestyle=\"None\",\n",
    "    )\n",
    "    print(f\"{cloud_id} {ds['mass_size_distribution'].sum('radius').mean('time').values} LWC\")\n",
    "\n",
    "\n",
    "m, v = mean_and_stderror_of_mean(dataset[\"particle_size_distribution\"], dims=(\"time\",))\n",
    "axs[0].plot(\n",
    "    m.radius,\n",
    "    m,\n",
    "    label=\"mean\",\n",
    "    color=\"k\",\n",
    "    zorder=10,\n",
    ")\n",
    "axs[0].fill_between(\n",
    "    m.radius,\n",
    "    m - 2 * v,\n",
    "    m + 2 * v,\n",
    "    alpha=0.5,\n",
    "    color=\"k\",\n",
    "    label=\"mean ± std error\",\n",
    "    zorder=10,\n",
    ")\n",
    "\n",
    "m, v = mean_and_stderror_of_mean(dataset[\"mass_size_distribution\"], dims=(\"time\",))\n",
    "axs[1].plot(m.radius, m, label=\"mean\", color=\"k\", zorder=10)\n",
    "axs[1].fill_between(\n",
    "    m.radius,\n",
    "    m - 2 * v,\n",
    "    m + 2 * v,\n",
    "    alpha=0.5,\n",
    "    color=\"k\",\n",
    "    label=\"mean ± std error\",\n",
    "    zorder=10,\n",
    ")\n",
    "\n",
    "axs[0].set_yscale(\"log\")\n",
    "for _ax in axs:\n",
    "    _ax.legend(loc=\"upper center\")\n",
    "    _ax.set_xscale(\"log\")\n",
    "# axs[1].set_ylim(0, 1e-5)\n",
    "\n",
    "axs[0].set_ylabel(\"particle size distribution\")\n",
    "axs[1].set_ylabel(\"mass size distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_composite[\"radius2D\"] = coarse_composite[\"radius\"].expand_dims(time=coarse_composite[\"time\"])\n",
    "coarse_composite = coarse_composite.transpose(\"radius\", ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose random time step\n",
    "\n",
    "# np.random.seed(42)\n",
    "train_data = match_clouds_and_cloudcomposite(\n",
    "    identified_clouds.isel(time=np.random.random_integers(0, len(identified_clouds.time) - 1)),\n",
    "    coarse_composite,\n",
    ")\n",
    "\n",
    "t_train = train_data[\"radius2D\"]  # .mean('time')\n",
    "y_train = train_data[\"particle_size_distribution\"]  # .mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_ln_normal_distribution(\n",
    "    t: np.ndarray,\n",
    "    mu1: float,\n",
    "    sigma1: float,\n",
    "    scale_factor1: float,\n",
    "    mu2: float,\n",
    "    sigma2: float,\n",
    "    scale_factor2: float,\n",
    ") -> np.ndarray:\n",
    "\n",
    "    result = np.zeros_like(t, dtype=float)\n",
    "\n",
    "    for mu, sigma, scale_factor in zip(\n",
    "        (mu1, mu2),\n",
    "        (sigma1, sigma2),\n",
    "        (scale_factor1, scale_factor2),\n",
    "    ):\n",
    "        sigtilda = np.log(sigma)\n",
    "        mutilda = np.log(mu)\n",
    "\n",
    "        norm = scale_factor / (np.sqrt(2 * np.pi) * sigtilda)\n",
    "        exponent = -((np.log(t) - mutilda) ** 2) / (2 * sigtilda**2)\n",
    "\n",
    "        dn_dlnr = norm * np.exp(exponent)  # eq.5.8 [lohmann intro 2 clouds]\n",
    "\n",
    "        result += dn_dlnr\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def fun_var(x, t, y):\n",
    "    y_is = double_ln_normal_distribution(t, x[0], x[1], x[2], x[3], x[4], x[5])\n",
    "\n",
    "    # also devide by the variance of the data\n",
    "    var = 0.01 * y\n",
    "    var = np.where(var != 0, var, 1e12)\n",
    "    return (np.abs(y_is - y), 1 / np.sqrt(var))\n",
    "\n",
    "\n",
    "def fun(x, t, y):\n",
    "    y_is = double_ln_normal_distribution(t, x[0], x[1], x[2], x[3], x[4], x[5])\n",
    "\n",
    "    # also devide by the variance of the data\n",
    "    var = 0.01 * y\n",
    "    var = np.where(var != 0, var, 1e30)\n",
    "    return np.ravel((y_is - y) / np.sqrt(var))\n",
    "\n",
    "\n",
    "def fun_no(x, t, y):\n",
    "    y_is = double_ln_normal_distribution(t, x[0], x[1], x[2], x[3], x[4], x[5])\n",
    "\n",
    "    # also devide by the variance of the data\n",
    "    var = 1\n",
    "    # var = np.where(var != 0, var, 1e30)\n",
    "    return np.ravel((y_is - y) / np.sqrt(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, y = t_train.mean(\"time\"), y_train.mean(\"time\")\n",
    "res, var = fun_var(x0, t, y)\n",
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(10, 5), sharex=True)\n",
    "ax0, ax1, ax2 = axs\n",
    "ax0.plot(t, y, marker=\"o\", color=default_colors[0])\n",
    "ax0.plot(t, res, marker=\"o\", color=default_colors[1])\n",
    "ax1.plot(t, var, marker=\"o\", color=default_colors[3])\n",
    "ax2.plot(t, res / y, marker=\"o\", color=default_colors[2])\n",
    "ax0.set_xscale(\"log\")\n",
    "ax0.set_yscale(\"log\")\n",
    "ax1.set_yscale(\"log\")\n",
    "# ax1.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.02\n",
      "6.61\n",
      "2.27 TRUTH\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(42)\n",
    "train_data = match_clouds_and_cloudcomposite(\n",
    "    identified_clouds.isel(time=np.random.random_integers(0, len(identified_clouds.time) - 1)),\n",
    "    coarse_composite,\n",
    ")\n",
    "\n",
    "t_train = train_data[\"radius2D\"]\n",
    "y_train = train_data[\"particle_size_distribution\"]\n",
    "m_train = train_data[\"mass_size_distribution\"]\n",
    "w_train = train_data[\"bin_width\"]\n",
    "\n",
    "x0 = np.array([3e-6, 2, 1e10, 200e-6, 2, 1e6])\n",
    "bounds = Bounds(\n",
    "    lb=[1e-6, 1.1, 1e7, 50e-6, 1.1, 1e0],\n",
    "    ub=[1e-5, 3.0, 1e13, 0.5e-3, 3.0, 1e8],\n",
    "    # keep_feasible = [True, True, True, False, True, True]\n",
    ")\n",
    "res_lsq = least_squares(fun, x0, bounds=bounds, args=(t_train.mean(\"time\"), y_train.mean(\"time\")))\n",
    "res_soft_l1 = least_squares(\n",
    "    fun,\n",
    "    x0,\n",
    "    loss=\"soft_l1\",\n",
    "    f_scale=0.1,\n",
    "    bounds=bounds,\n",
    "    args=(t_train.mean(\"time\"), y_train.mean(\"time\")),\n",
    ")\n",
    "res_log = least_squares(\n",
    "    fun, x0, loss=\"cauchy\", f_scale=0.1, bounds=bounds, args=(t_train.mean(\"time\"), y_train.mean(\"time\"))\n",
    ")\n",
    "t_test = np.logspace(-6, -2.5, 1000)\n",
    "\n",
    "\n",
    "y_lsq = gen_data(t_test, *res_lsq.x)\n",
    "y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
    "y_log = gen_data(t_test, *res_log.x)\n",
    "y_init = gen_data(t_test, *x0)\n",
    "\n",
    "# create mass size distirbution\n",
    "m_lsq = y_lsq * t_test**3\n",
    "m_soft_l1 = y_soft_l1 * t_test**3\n",
    "m_log = y_log * t_test**3\n",
    "m_init = y_init * t_test**3\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(10, 5), sharex=True)\n",
    "ax0, ax1 = axs\n",
    "\n",
    "m_train = y_train * t_train**3\n",
    "\n",
    "ax0.scatter(t_train, y_train, marker=\"o\", color=\"grey\")\n",
    "ax0.scatter(t_train.mean(\"time\"), y_train.mean(\"time\"), marker=\"o\", color=\"k\")\n",
    "ax0.set_xscale(\"log\")\n",
    "ax0.set_yscale(\"log\")\n",
    "\n",
    "ax1.scatter(t_train, m_train, marker=\"o\", color=\"grey\")\n",
    "ax1.scatter(t_train.mean(\"time\"), m_train.mean(\"time\"), marker=\"o\", color=\"k\")\n",
    "ax1.set_xscale(\"log\")\n",
    "# ax1.set_yscale('log')\n",
    "\n",
    "for y, m in zip(\n",
    "    (\n",
    "        y_lsq,\n",
    "        y_soft_l1,\n",
    "    ),\n",
    "    (\n",
    "        m_lsq,\n",
    "        m_soft_l1,\n",
    "    ),\n",
    "):\n",
    "    ax0.plot(t_test, y, label=\"y\")\n",
    "    ax1.plot(t_test, m, label=\"m\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "\n",
    "w_test = t_test * 0\n",
    "w_test[1:] = t_test[1:] - t_test[:-1]\n",
    "\n",
    "# print(f\"{(m_init * w_test).sum():.2e}')\n",
    "print(f\"{1e8 * (m_lsq * w_test).sum():.2f}\")\n",
    "print(f\"{1e8 * (m_soft_l1 * w_test).sum():.2f}\")\n",
    "# print(f1e6 * '{(m_log * w_test).sum():.2f}')\n",
    "print(f\"{1e8 * (m_train * w_train).mean('time').sum(dim = 'radius').values:.2f} TRUTH\")\n",
    "# print(f\"{(m_train * w_train).mean('time').sum().values:.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.09\n",
      "8.28\n",
      "16.38\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(42)\n",
    "train_data = match_clouds_and_cloudcomposite(\n",
    "    identified_clouds.isel(time=np.random.random_integers(0, len(identified_clouds.time) - 1)),\n",
    "    coarse_composite,\n",
    ")\n",
    "\n",
    "t_train = train_data[\"radius2D\"]  # .mean('time')\n",
    "m_train = train_data[\"mass_size_distribution\"]  # .mean('time')\n",
    "w_train = train_data[\"bin_width\"]\n",
    "y_train = m_train * t_train ** (-3)\n",
    "\n",
    "x0 = np.array([3e-6, 2, 1e-1, 300e-6, 2, 1e0])\n",
    "bounds = Bounds(\n",
    "    lb=[1e-6, 1.1, 1e-3, 50e-6, 1.1, 1e-3],\n",
    "    ub=[1e-5, 4.0, 1e2, 0.5e-3, 3.0, 1e1],\n",
    "    # keep_feasible = [True, True, True, False, True, True]\n",
    ")\n",
    "res_lsq = least_squares(fun, x0, bounds=bounds, args=(t_train.mean(\"time\"), m_train.mean(\"time\")))\n",
    "res_soft_l1 = least_squares(\n",
    "    fun,\n",
    "    x0,\n",
    "    loss=\"soft_l1\",\n",
    "    f_scale=0.1,\n",
    "    bounds=bounds,\n",
    "    args=(t_train.mean(\"time\"), m_train.mean(\"time\")),\n",
    ")\n",
    "res_log = least_squares(\n",
    "    fun, x0, loss=\"cauchy\", f_scale=0.1, bounds=bounds, args=(t_train.mean(\"time\"), m_train.mean(\"time\"))\n",
    ")\n",
    "t_test = np.logspace(-6, -2.5, 1000)\n",
    "\n",
    "\n",
    "m_lsq = gen_data(t_test, *res_lsq.x)\n",
    "m_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
    "m_log = gen_data(t_test, *res_log.x)\n",
    "m_init = gen_data(t_test, *x0)\n",
    "\n",
    "# create mass size distirbution\n",
    "y_lsq = m_lsq * t_test ** (-3)\n",
    "y_soft_l1 = m_soft_l1 * t_test ** (-3)\n",
    "y_log = m_log * t_test ** (-3)\n",
    "y_init = m_init * t_test ** (-3)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(10, 5), sharex=True)\n",
    "ax0, ax1 = axs\n",
    "\n",
    "m_train = y_train * t_train**3\n",
    "\n",
    "ax0.scatter(t_train, y_train, marker=\"o\", color=\"grey\")\n",
    "ax0.scatter(t_train.mean(\"time\"), y_train.mean(\"time\"), marker=\"o\", color=\"k\")\n",
    "ax0.set_xscale(\"log\")\n",
    "ax0.set_yscale(\"log\")\n",
    "\n",
    "ax1.scatter(t_train, m_train, marker=\"o\", color=\"grey\")\n",
    "ax1.scatter(t_train.mean(\"time\"), m_train.mean(\"time\"), marker=\"o\", color=\"k\")\n",
    "ax1.set_xscale(\"log\")\n",
    "# ax1.set_yscale('log')\n",
    "\n",
    "for y, m in zip(\n",
    "    (y_lsq, y_soft_l1),\n",
    "    (m_lsq, m_soft_l1),\n",
    "):\n",
    "    ax0.plot(t_test, y, label=\"y\")\n",
    "    ax1.plot(t_test, m, label=\"m\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "\n",
    "w_test = t_test * 0\n",
    "w_test[1:] = t_test[1:] - t_test[:-1]\n",
    "\n",
    "# print(f\"{1e6 * (m_init * w_test).sum():.2f}')\n",
    "print(f\"{1e6 * (m_lsq * w_test).sum():.2f}\")\n",
    "print(f\"{1e6 * (m_soft_l1 * w_test).sum():.2f}\")\n",
    "# print(f\"{1e6 * (m_log * w_test).sum():.2f}')\n",
    "# print(f1e6 * '{(m_train * w_train).sum(dim = 'radius').values:.2e} TRUTf\")\n",
    "print(f\"{1e6 * (m_train * w_train).mean('time').sum().values:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwc_org = []\n",
    "lwc_lsq = []\n",
    "\n",
    "for cloud_id in identified_clouds[\"cloud_id\"]:\n",
    "    # print('cloud_id', cloud_id.values)\n",
    "    ds = select_individual_cloud_by_id(identified_clouds, cloud_id)\n",
    "    train_data = match_clouds_and_cloudcomposite(ds, coarse_composite)\n",
    "\n",
    "    t_train = train_data[\"radius2D\"]  # .mean('time')\n",
    "    m_train = train_data[\"mass_size_distribution\"]  # .mean('time')\n",
    "    w_train = train_data[\"bin_width\"]\n",
    "    y_train = m_train * t_train ** (-3)\n",
    "\n",
    "    x0 = np.array([3e-6, 2, 1e-1, 300e-6, 2, 1e0])\n",
    "    bounds = Bounds(\n",
    "        lb=[1e-6, 1.1, 1e-3, 50e-6, 1.1, 1e-3],\n",
    "        ub=[1e-5, 4.0, 1e2, 0.5e-3, 3.0, 1e1],\n",
    "        # keep_feasible = [True, True, True, False, True, True]\n",
    "    )\n",
    "    res_lsq = least_squares(fun, x0, bounds=bounds, args=(t_train, m_train))\n",
    "\n",
    "    t_test = np.logspace(-6, -2.5, 200)\n",
    "    w_test = t_test * 0\n",
    "    w_test[1:] = t_test[1:] - t_test[:-1]\n",
    "\n",
    "    m_lsq = gen_data(t_test, *res_lsq.x)\n",
    "    m_init = gen_data(t_test, *x0)\n",
    "\n",
    "    # create mass size distirbution\n",
    "    y_lsq = m_lsq * t_test ** (-3)\n",
    "    y_init = m_init * t_test ** (-3)\n",
    "\n",
    "    lwc_org.append(1e6 * (m_train * w_train).mean(\"time\").sum().values)\n",
    "    lwc_lsq.append(1e6 * (m_lsq * w_test).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ffd49ebc2c0>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(f\"{1e6 * (m_init * w_test).sum():.2f}')\n",
    "plt.scatter(lwc_org, lwc_lsq, color=\"k\", marker=\"o\")\n",
    "plt.plot(\n",
    "    [0, 500],\n",
    "    [0, 500],\n",
    ")\n",
    "# print(f\"{1e6 * (m_lsq * w_test).sum():.2f}')\n",
    "# print(f\"{1e6 * (m_train * w_train).mean('time').sum().values:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.71e-05\n",
      "8.06e-05\n"
     ]
    }
   ],
   "source": [
    "res_lsq = least_squares(fun_no, x0, bounds=bounds, args=(t_train, y_train))\n",
    "res_soft_l1 = least_squares(\n",
    "    fun_no,\n",
    "    x0,\n",
    "    loss=\"soft_l1\",\n",
    "    f_scale=0.1,\n",
    "    bounds=bounds,\n",
    "    args=(t_train, y_train),\n",
    ")\n",
    "res_log = least_squares(fun_no, x0, loss=\"cauchy\", f_scale=0.1, bounds=bounds, args=(t_train, y_train))\n",
    "t_test = np.logspace(-6, -2.9, 1000)\n",
    "\n",
    "\n",
    "y_lsq = gen_data(t_test, *res_lsq.x)\n",
    "y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
    "y_log = gen_data(t_test, *res_log.x)\n",
    "y_init = gen_data(t_test, *x0)\n",
    "\n",
    "plt.scatter(t_train, y_train, marker=\"o\", color=\"grey\")\n",
    "plt.scatter(t_train.mean(\"time\"), y_train.mean(\"time\"), marker=\"o\", color=\"k\")\n",
    "plt.plot(t_test, y_lsq, label=\"linear loss\")\n",
    "plt.plot(t_test, y_soft_l1, label=\"soft_l1 loss\")\n",
    "plt.plot(t_test, y_log, label=\"cauchy loss\")\n",
    "plt.plot(t_test, y_init, label=\"innit\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-6, 1e1)\n",
    "\n",
    "w_test = t_test * 0\n",
    "w_test[1:] = t_test[1:] - t_test[:-1]\n",
    "\n",
    "print(f\"{(y_lsq * w_test).sum():.2e}\")\n",
    "print(f\"{(y_train * w_train).mean('time').sum().values:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdm_eurec4a_env312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
