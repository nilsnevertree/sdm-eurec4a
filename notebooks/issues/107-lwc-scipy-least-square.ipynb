{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import default_rng\n",
    "from scipy.optimize import Bounds\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sdm_eurec4a.visulization import (\n",
    "    set_custom_rcParams,\n",
    ")\n",
    "from sdm_eurec4a.identifications import match_clouds_and_cloudcomposite, select_individual_cloud_by_id\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "default_colors = set_custom_rcParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_rosenbrock(x):\n",
    "\n",
    "    return np.array([10 * (x[1] - x[0] ** 2), (1 - x[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         2.9000e+00                                    5.78e+01    \n",
      "       1              2         8.0000e-02      2.82e+00       3.12e-01       8.00e+00    \n",
      "       2              3         0.0000e+00      8.00e-02       4.00e-02       0.00e+00    \n",
      "`gtol` termination condition is satisfied.\n",
      "Function evaluations 3, initial cost 2.9000e+00, final cost 0.0000e+00, first-order optimality 0.00e+00.\n",
      "[1. 1.]\n",
      "0.0\n",
      "0.0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "x0_rosenbrock = np.array([1.2, 1.2])\n",
    "\n",
    "res_1 = least_squares(fun_rosenbrock, x0_rosenbrock, verbose=2)\n",
    "\n",
    "print(res_1.x)\n",
    "print(res_1.cost)\n",
    "print(res_1.optimality)\n",
    "print(res_1.nfev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng()\n",
    "\n",
    "\n",
    "def analytic(t, a, b, c):\n",
    "    return a + b * np.exp(c * t)\n",
    "\n",
    "\n",
    "def gen_data(t, a, b, c, noise=0.0, n_outliers=0, seed=None):\n",
    "    rng = default_rng(seed)\n",
    "\n",
    "    y = analytic(t, a, b, c)\n",
    "\n",
    "    error = noise * rng.standard_normal(t.size)\n",
    "    outliers = rng.integers(0, t.size, n_outliers)\n",
    "    error[outliers] *= 10\n",
    "\n",
    "    return y + error\n",
    "\n",
    "\n",
    "a = 0.5\n",
    "b = 2.0\n",
    "c = -1\n",
    "t_min = 0\n",
    "t_max = 10\n",
    "n_points = 15\n",
    "\n",
    "t_train = np.linspace(t_min, t_max, n_points)\n",
    "y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\n",
    "\n",
    "\n",
    "def fun(x, t, y):\n",
    "    y_is = analytic(t, x[0], x[1], x[2])\n",
    "\n",
    "    return y_is - y\n",
    "\n",
    "\n",
    "x0 = np.array([1.0, 1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_ln_normal_distribution(\n",
    "    t: np.ndarray,\n",
    "    mu1: float,\n",
    "    sigma1: float,\n",
    "    scale_factor1: float,\n",
    "    mu2: float,\n",
    "    sigma2: float,\n",
    "    scale_factor2: float,\n",
    ") -> np.ndarray:\n",
    "\n",
    "    result = np.zeros(t.size)\n",
    "\n",
    "    for mu, sigma, scale_factor in zip(\n",
    "        (mu1, mu2),\n",
    "        (sigma1, sigma2),\n",
    "        (scale_factor1, scale_factor2),\n",
    "    ):\n",
    "        sigtilda = np.log(sigma)\n",
    "        mutilda = np.log(mu)\n",
    "\n",
    "        norm = scale_factor / (np.sqrt(2 * np.pi) * sigtilda)\n",
    "        exponent = -((np.log(t) - mutilda) ** 2) / (2 * sigtilda**2)\n",
    "\n",
    "        dn_dlnr = norm * np.exp(exponent)  # eq.5.8 [lohmann intro 2 clouds]\n",
    "\n",
    "        result += dn_dlnr\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "rng = default_rng()\n",
    "\n",
    "\n",
    "def gen_data(\n",
    "    t: np.ndarray,\n",
    "    mu1: float,\n",
    "    sigma1: float,\n",
    "    scale_factor1: float,\n",
    "    mu2: float,\n",
    "    sigma2: float,\n",
    "    scale_factor2: float,\n",
    "    noise=0.0,\n",
    "    n_outliers=0,\n",
    "    seed=None,\n",
    "):\n",
    "    rng = default_rng(seed)\n",
    "\n",
    "    y = double_ln_normal_distribution(\n",
    "        t=t,\n",
    "        mu1=mu1,\n",
    "        sigma1=sigma1,\n",
    "        scale_factor1=scale_factor1,\n",
    "        mu2=mu2,\n",
    "        sigma2=sigma2,\n",
    "        scale_factor2=scale_factor2,\n",
    "    )\n",
    "    error = noise * rng.standard_normal(t.size)\n",
    "    outliers = rng.integers(0, t.size, n_outliers)\n",
    "    error[outliers] = np.sqrt(t[outliers]) * error[outliers]\n",
    "\n",
    "    return y + error\n",
    "\n",
    "\n",
    "mu1 = 1e-2\n",
    "sigma1 = 2\n",
    "scale_factor1 = 5\n",
    "mu2 = 0.5e1\n",
    "sigma2 = 3\n",
    "scale_factor2 = 1\n",
    "\n",
    "t_min = 0.1\n",
    "t_max = 10\n",
    "n_points = 40\n",
    "n_outliers = 5\n",
    "t_train = np.logspace(-3, 2, n_points)\n",
    "# t_train = np.linspace(t_min, t_max, n_points)\n",
    "y_train = gen_data(\n",
    "    t=t_train,\n",
    "    mu1=mu1,\n",
    "    sigma1=sigma1,\n",
    "    scale_factor1=scale_factor1,\n",
    "    mu2=mu2,\n",
    "    sigma2=sigma2,\n",
    "    scale_factor2=scale_factor2,\n",
    "    noise=0.1,\n",
    "    n_outliers=n_outliers,\n",
    ")\n",
    "\n",
    "\n",
    "def fun(x, t, y):\n",
    "    y_is = double_ln_normal_distribution(t, x[0], x[1], x[2], x[3], x[4], x[5])\n",
    "    return y_is - y\n",
    "\n",
    "\n",
    "x0 = np.array([1e-1, 2.0, 1.0, 10.0, 2.0, 1.0])\n",
    "bounds = Bounds(\n",
    "    lb=[1e-10, 1e-10, -np.inf, 2e-2, 1e-10, -np.inf],\n",
    "    ub=[5e-1, np.inf, np.inf, np.inf, np.inf, np.inf],\n",
    "    keep_feasible=[True, True, True, False, True, True],\n",
    ")\n",
    "res_lsq = least_squares(fun, x0, bounds=bounds, args=(t_train, y_train))\n",
    "res_soft_l1 = least_squares(fun, x0, loss=\"soft_l1\", f_scale=0.1, bounds=bounds, args=(t_train, y_train))\n",
    "res_log = least_squares(fun, x0, loss=\"cauchy\", f_scale=0.1, bounds=bounds, args=(t_train, y_train))\n",
    "t_test = np.logspace(-5, 2, n_points * 10)\n",
    "\n",
    "y_true = gen_data(\n",
    "    t=t_test,\n",
    "    mu1=mu1,\n",
    "    sigma1=sigma1,\n",
    "    scale_factor1=scale_factor1,\n",
    "    mu2=mu2,\n",
    "    sigma2=sigma2,\n",
    "    scale_factor2=scale_factor2,\n",
    ")\n",
    "\n",
    "y_lsq = gen_data(t_test, *res_lsq.x)\n",
    "y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
    "y_log = gen_data(t_test, *res_log.x)\n",
    "\n",
    "plt.plot(t_train, y_train, \"o\")\n",
    "plt.plot(t_test, y_true, \"k\", linewidth=2, label=\"true\")\n",
    "plt.plot(t_test, y_lsq, label=\"linear loss\")\n",
    "plt.plot(t_test, y_soft_l1, label=\"soft_l1 loss\")\n",
    "plt.plot(t_test, y_log, label=\"cauchy loss\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_composite = xr.open_dataset(\n",
    "    \"/home/m/m301096/repositories/sdm-eurec4a/data/observation/cloud_composite/processed/cloud_composite_si_units.nc\"\n",
    ")\n",
    "identified_clouds = xr.open_dataset(\n",
    "    \"/home/m/m301096/repositories/sdm-eurec4a/data/observation/cloud_composite/processed/identified_clusters/identified_clusters_rain_mask_5.nc\"\n",
    ")\n",
    "\n",
    "attrs = cloud_composite[\"radius\"].attrs.copy()\n",
    "attrs.update({\"units\": \"Âµm\"})\n",
    "cloud_composite[\"radius\"] = cloud_composite[\"radius\"]\n",
    "cloud_composite[\"radius_micro\"] = 1e6 * cloud_composite[\"radius\"]\n",
    "cloud_composite[\"radius\"].attrs = attrs\n",
    "\n",
    "# cloud_composite = cloud_composite.sel(radius = slice(10, None))\n",
    "\n",
    "identified_clouds = identified_clouds.where(\n",
    "    (\n",
    "        (identified_clouds.duration.dt.total_seconds() > 50)\n",
    "        & (identified_clouds.alt < 1300)\n",
    "        & (identified_clouds.alt > 500)\n",
    "    ),\n",
    "    drop=True,\n",
    ")\n",
    "\n",
    "cloud_composite = match_clouds_and_cloudcomposite(identified_clouds, cloud_composite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_composite.to_netcdf(\"train_cc.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_composite[\"mass_size_distribution\"].mean(\"time\").plot(x=\"radius\", yscale=\"log\", aspect=2, size=4)\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_composite = cloud_composite.coarsen(radius=2).sum()\n",
    "coarse_composite[\"diameter\"] = 2 * coarse_composite[\"radius\"]\n",
    "# normalize the particle size distirbution\n",
    "attrs = coarse_composite[\"particle_size_distribution\"].attrs.copy()\n",
    "attrs[\"units\"] = \"m^-3 m^-1\"\n",
    "attrs[\"long_name\"] = \"Particle size distribution\"\n",
    "attrs[\"comment\"] = \"Each bin gives the number of droplets per cubic meter of air per meter of radius\"\n",
    "\n",
    "coarse_composite[\"particle_size_distribution\"] = (\n",
    "    coarse_composite[\"particle_size_distribution\"] / coarse_composite[\"bin_width\"] / 2\n",
    ")\n",
    "coarse_composite[\"particle_size_distribution\"].attrs = attrs\n",
    "\n",
    "# normalize the mass size distribution\n",
    "attrs_mass = coarse_composite[\"mass_size_distribution\"].attrs.copy()\n",
    "attrs_mass[\"units\"] = \"kg m^-3 m^-1\"\n",
    "attrs_mass[\"long_name\"] = \"Mass size distribution\"\n",
    "attrs_mass[\"comment\"] = \"Each bin gives the mass of droplets per cubic meter of air per meter of radius\"\n",
    "\n",
    "coarse_composite[\"mass_size_distribution\"] = (\n",
    "    coarse_composite[\"mass_size_distribution\"] / coarse_composite[\"bin_width\"] / 2\n",
    ")\n",
    "coarse_composite[\"mass_size_distribution\"].attrs = attrs_mass\n",
    "\n",
    "coarse_composite[\"mass_size_distribution\"].mean(\"time\").plot(x=\"radius\", yscale=\"log\", aspect=2, size=4)\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0, len(coarse_composite[\"time\"]))\n",
    "# plt.plot(\n",
    "#     t,\n",
    "#     coarse_composite['radius'],\n",
    "#     coarse_composite['mass_size_distribution'],\n",
    "#     shading='auto',\n",
    "#     cmap = 'Blues', vmax = 1e-1, vmin = 1e-3)\n",
    "plt.plot(\n",
    "    coarse_composite[\"radius\"],\n",
    "    coarse_composite[\"particle_size_distribution\"],\n",
    "    color=\"k\",\n",
    "    alpha=0.1,\n",
    ")\n",
    "plt.plot(\n",
    "    coarse_composite[\"radius\"],\n",
    "    coarse_composite[\"particle_size_distribution\"].mean(\"time\"),\n",
    "    color=\"r\",\n",
    "    alpha=1,\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_composite[\"radius2D\"] = coarse_composite[\"radius\"].expand_dims(time=coarse_composite[\"time\"])\n",
    "coarse_composite = coarse_composite.transpose(\"radius\", ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Iteration     Total nfev        Cost      Cost reduction    Step norm     Optimality   \n",
      "       0              1         3.0502e+12                                    3.71e+14    \n",
      "       1              2         2.5881e+12      4.62e+11       3.14e+11       1.81e-57    \n",
      "`gtol` termination condition is satisfied.\n",
      "Function evaluations 2, initial cost 3.0502e+12, final cost 2.5881e+12, first-order optimality 1.81e-57.\n",
      "[1.68558226e-08 1.10502562e+00 9.61009977e+09 4.96913450e-03\n",
      " 1.10450000e+00 3.14378431e+11]\n"
     ]
    }
   ],
   "source": [
    "# chose random time step\n",
    "\n",
    "# np.random.seed(42)\n",
    "train_data = match_clouds_and_cloudcomposite(\n",
    "    identified_clouds.isel(time=np.random.random_integers(0, len(identified_clouds.time) - 1)),\n",
    "    coarse_composite,\n",
    ")\n",
    "\n",
    "t_train = train_data[\"radius2D\"]  # .mean('time')\n",
    "y_train = train_data[\"particle_size_distribution\"]  # .mean('time')\n",
    "\n",
    "\n",
    "def double_ln_normal_distribution(\n",
    "    t: np.ndarray,\n",
    "    mu1: float,\n",
    "    sigma1: float,\n",
    "    scale_factor1: float,\n",
    "    mu2: float,\n",
    "    sigma2: float,\n",
    "    scale_factor2: float,\n",
    ") -> np.ndarray:\n",
    "\n",
    "    result = np.zeros(t.size)\n",
    "\n",
    "    for mu, sigma, scale_factor in zip(\n",
    "        (mu1, mu2),\n",
    "        (sigma1, sigma2),\n",
    "        (scale_factor1, scale_factor2),\n",
    "    ):\n",
    "        sigtilda = np.log(sigma)\n",
    "        mutilda = np.log(mu)\n",
    "\n",
    "        norm = scale_factor / (np.sqrt(2 * np.pi) * sigtilda)\n",
    "        exponent = -((np.log(t) - mutilda) ** 2) / (2 * sigtilda**2)\n",
    "\n",
    "        dn_dlnr = norm * np.exp(exponent)  # eq.5.8 [lohmann intro 2 clouds]\n",
    "\n",
    "        result += dn_dlnr\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def fun(x, t, y):\n",
    "    y_is = double_ln_normal_distribution(t, x[0], x[1], x[2], x[3], x[4], x[5])\n",
    "    return y_is - y\n",
    "\n",
    "\n",
    "x0 = np.array([3e-6, 2, 1e10, 100e-6, 2, 1e6])\n",
    "bounds = Bounds(\n",
    "    lb=[1e-10, 1.1, 1e7, 50e-6, 1.1, 1e0],\n",
    "    ub=[50e-6, 3, 1e13, 5e-3, 3, 1e13],\n",
    "    # keep_feasible = [True, True, True, False, True, True]\n",
    ")\n",
    "# res_lsq = least_squares(\n",
    "#     fun,\n",
    "#     x0,\n",
    "#     bounds= bounds,\n",
    "#     args=(t_train, y_train)\n",
    "# )\n",
    "res_soft_l1 = least_squares(\n",
    "    fun,\n",
    "    x0,\n",
    "    loss=\"soft_l1\",\n",
    "    f_scale=0.5,\n",
    "    bounds=bounds,\n",
    "    verbose=2,\n",
    "    args=(t_train.values.flatten(), y_train.values.flatten()),\n",
    ")\n",
    "# res_log = least_squares(\n",
    "#     fun,\n",
    "#     x0,\n",
    "#     loss='cauchy',\n",
    "#     f_scale=0.5,\n",
    "#     bounds= bounds,\n",
    "#     args=(t_train, y_train)\n",
    "#     )\n",
    "t_test = np.logspace(-6, -2.5, 1000)\n",
    "\n",
    "\n",
    "y_lsq = gen_data(t_test, *res_lsq.x)\n",
    "y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
    "y_log = gen_data(t_test, *res_log.x)\n",
    "\n",
    "plt.scatter(t_train, y_train, marker=\"o\", color=\"grey\")\n",
    "# plt.plot(t_test, y_lsq, label='linear loss')\n",
    "plt.plot(t_test, y_soft_l1, label=\"soft_l1 loss\")\n",
    "# plt.plot(t_test, y_log, label='cauchy loss')\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"symlog\")\n",
    "plt.ylim(0, 1e11)\n",
    "print(res_soft_l1.x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdm_eurec4a_env312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
